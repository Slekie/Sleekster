{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be5171f-f877-4359-9284-00e01803e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.45.1)\n",
      "Requirement already satisfied: MetaTrader5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (5.0.5388)\n",
      "Requirement already satisfied: pandas in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: gymnasium==0.29.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: shimmy==0.2.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from gymnasium==0.29.1) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from gymnasium==0.29.1) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: future in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from ffmpeg-python) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Optional: run this cell only if you need to install missing packages.\n",
    "# Comment out if already installed.\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install MetaTrader5 pandas numpy matplotlib stable-baselines3 gymnasium==0.29.1 shimmy==0.2.1 torch\n",
    "!{sys.executable} -m pip install ffmpeg-python pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e0c6c7-2b7e-467f-b62a-da3cc4d6a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "print(\"MT5 Initialize:\", mt5.initialize())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# ------ CONFIG ------\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")      # normalized CSVs and scalers\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "SCALER_GLOB = os.path.join(DATA_DIR, \"*_scaler.csv\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")   # optional\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "\n",
    "WINDOW = 50\n",
    "TIMEFRAME = \"M1\"   # human-readable timeframe (not MT5 constant)\n",
    "TF_MAP = { \"M1\": mt5.TIMEFRAME_M1, \"M5\": mt5.TIMEFRAME_M5, \"M15\": mt5.TIMEFRAME_M15,\n",
    "           \"M30\": mt5.TIMEFRAME_M30, \"H1\": mt5.TIMEFRAME_H1, \"H4\": mt5.TIMEFRAME_H4,\n",
    "           \"D1\": mt5.TIMEFRAME_D1 }\n",
    "TF_MT5 = TF_MAP[TIMEFRAME.upper()]\n",
    "\n",
    "LOG_FILE = os.path.join(MODEL_DIR, \"live_trade_logs.csv\")\n",
    "DRY_RUN = True   # Set False to actually place orders in MT5 (use with caution)\n",
    "\n",
    "# Safety defaults for order params\n",
    "DEFAULT_RISK_PCT = 0.005\n",
    "MIN_LOT = 0.01\n",
    "MAX_LOT = 1.0\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0be2cd-0b87-4cfc-8ba6-c2d6919040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - utilities\n",
    "\n",
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "def load_scalers(data_dir=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Load per-asset scaler CSVs into dict: {safe: {\"mean\": Series, \"std\": Series}}\n",
    "    \"\"\"\n",
    "    scalers = {}\n",
    "    for path in glob.glob(os.path.join(data_dir, \"*_scaler.csv\")):\n",
    "        safe = os.path.basename(path).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        #df = pd.read_csv(path, index_col=0, squeeze=False)\n",
    "        # Expect columns 'mean' and 'std'\n",
    "        scalers[safe] = {\"mean\": df['mean'], \"std\": df['std']}\n",
    "    return scalers\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Load embeddings (npy) and asset map. Returns embeddings dict {safe: array}\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found:\", embed_file)\n",
    "        return embeddings\n",
    "    emb = np.load(embed_file, allow_pickle=False)\n",
    "    # load asset map\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        #am = pd.read_csv(ASSET_MAP_FILE, index_col=0, squeeze=False)\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, squeeze=False)\n",
    "        # am is a series-like mapping safe->idx perhaps; create mapping\n",
    "        # convert to dict safe->idx\n",
    "        try:\n",
    "            # If CSV is like safe,index\n",
    "            asset_to_idx = {str(k): int(v) for k,v in am.to_dict()[am.columns[0]].items()}\n",
    "        except Exception:\n",
    "            asset_to_idx = {str(k): int(v) for k,v in am.to_dict().items()}\n",
    "        # build dict\n",
    "        for safe, idx in asset_to_idx.items():\n",
    "            if idx < emb.shape[0]:\n",
    "                embeddings[safe] = emb[idx]\n",
    "    else:\n",
    "        # fallback: map based on files in data_dir in sorted order\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i,safe in enumerate(safe_list):\n",
    "                embeddings[safe] = emb[i]\n",
    "        else:\n",
    "            # cannot map reliably\n",
    "            print(\"Warning: embeddings length does not match CSV count and no asset map - skipping embedding loading.\")\n",
    "    return embeddings\n",
    "\n",
    "def load_normalized_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    \"\"\"\n",
    "    Load *_normalized.csv into dict {safe: df} and only keep rows > window\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # required columns or attempt mapping\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if not all(c in df.columns for c in expected):\n",
    "            # try mapping columns lowercase\n",
    "            lower = {c.lower():c for c in df.columns}\n",
    "            if all(x in lower for x in ['open','high','low','close','volume']):\n",
    "                df = df.rename(columns={lower['open']:'open', lower['high']:'high', lower['low']:'low', lower['close']:'close', lower['volume']:'volume'})\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                tmp = tmp.dropna()\n",
    "                df = tmp\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing required columns and cannot auto-convert.\")\n",
    "        else:\n",
    "            df = df[expected].dropna()\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    if not datasets:\n",
    "        raise FileNotFoundError(f\"No usable normalized CSVs found in {data_dir}\")\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3f421f4-4fc5-43d5-aa74-73386dd636e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 4 - load preprocessors and model\u001b[39;00m\n\u001b[32m      2\u001b[39m scalers = load_scalers(DATA_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n\u001b[32m      4\u001b[39m datasets = load_normalized_datasets(DATA_DIR, WINDOW)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# build safe_names mapping {raw_symbol: safe} - we expect raw symbol to match safe for MT5\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# If your MT5 symbol names differ from safe names, create a mapping manually.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mload_embeddings\u001b[39m\u001b[34m(embed_file, data_dir)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# load asset map\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(ASSET_MAP_FILE):\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m#am = pd.read_csv(ASSET_MAP_FILE, index_col=0, squeeze=False)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     am = pd.read_csv(ASSET_MAP_FILE, index_col=\u001b[32m0\u001b[39m, squeeze=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# am is a series-like mapping safe->idx perhaps; create mapping\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# convert to dict safe->idx\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;66;03m# If CSV is like safe,index\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: read_csv() got an unexpected keyword argument 'squeeze'"
     ]
    }
   ],
   "source": [
    "# Cell 4 - load preprocessors and model\n",
    "scalers = load_scalers(DATA_DIR)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "datasets = load_normalized_datasets(DATA_DIR, WINDOW)\n",
    "\n",
    "# build safe_names mapping {raw_symbol: safe} - we expect raw symbol to match safe for MT5\n",
    "# If your MT5 symbol names differ from safe names, create a mapping manually.\n",
    "safe_names = {}\n",
    "for safe in datasets.keys():\n",
    "    # assume MT5 uses same display name as safe BUT with spaces (e.g. \"Volatility 75 Index\")\n",
    "    # If you use the original symbols, you should supply a mapping. We'll attempt to reverse map by replacing underscores.\n",
    "    raw_guess = safe.replace(\"_\", \" \")\n",
    "    safe_names[raw_guess] = safe\n",
    "\n",
    "print(\"Loaded datasets:\", list(datasets.keys()))\n",
    "print(\"Loaded scalers:\", list(scalers.keys())[:10], \" (total:\", len(scalers),\")\")\n",
    "print(\"Loaded embeddings:\", list(embeddings.keys())[:10], \" (total:\", len(embeddings),\")\")\n",
    "\n",
    "# Load model\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Trained model not found at: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded model:\", MODEL_FILE)\n",
    "\n",
    "# Initialize MT5 (for live trading)\n",
    "if not mt5.initialize():\n",
    "    raise RuntimeError(\"MT5 initialization failed. Start MT5 terminal and login to Deriv.\")\n",
    "print(\"MT5 initialized:\", mt5.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb40f09-7333-446a-a243-b80dd078cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - observation builder used in live loop\n",
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names):\n",
    "    \"\"\"\n",
    "    Returns (obs, vol_est, last_price) or (None, None, None) on failure.\n",
    "    - symbol: MT5 symbol string as used by mt5.copy_rates_from_pos(...)\n",
    "    - safe_names: mapping raw_symbol -> safe (the safe key used in scalers/embeddings/datasets)\n",
    "    \"\"\"\n",
    "    safe = safe_names.get(symbol) if symbol in safe_names else make_safe_name(symbol)\n",
    "\n",
    "    # check required objects\n",
    "    if safe not in scalers:\n",
    "        print(f\"❌ Missing scaler for: {safe}\")\n",
    "        return None, None, None\n",
    "    if safe not in embeddings:\n",
    "        print(f\"❌ Missing embedding for: {safe}\")\n",
    "        return None, None, None\n",
    "    if safe not in datasets:\n",
    "        print(f\"❌ No prepared dataset for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    scaler = scalers[safe]\n",
    "    embed_vec = np.array(embeddings[safe], dtype=np.float32)\n",
    "\n",
    "    count = window + 20\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, count)\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        print(f\"Insufficient bars for {symbol}\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    df = df[['open','high','low','close','tick_volume']].rename(columns={'tick_volume':'volume'})\n",
    "\n",
    "    # compute pct changes and take last window\n",
    "    pct = df.pct_change().dropna()\n",
    "    if len(pct) < window:\n",
    "        print(f\"Not enough pct rows for {symbol}\")\n",
    "        return None, None, None\n",
    "    pct = pct.tail(window)\n",
    "\n",
    "    # Normalize using scaler (scaler[\"mean\"] and scaler[\"std\"] are pandas Series)\n",
    "    mean = scaler[\"mean\"]\n",
    "    std = scaler[\"std\"].replace(0, 1.0)\n",
    "    # Ensure indexes align: use pct columns order\n",
    "    cols = [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "    # If scaler mean/std have slightly different index names, align by position\n",
    "    try:\n",
    "        m = mean[cols]\n",
    "        s = std[cols]\n",
    "    except Exception:\n",
    "        # fallback: assume mean/std are in same order as these five columns\n",
    "        m = mean.values[:len(cols)]\n",
    "        s = std.values[:len(cols)]\n",
    "        m = pd.Series(m, index=cols)\n",
    "        s = pd.Series(s, index=cols)\n",
    "\n",
    "    pct_norm = (pct[cols] - m) / s\n",
    "    last_price = float(df['close'].iloc[-1])\n",
    "    vol_est = float(pct['close'].std())\n",
    "\n",
    "    balance_norm = np.full((window,1), 1.0, dtype=np.float32)\n",
    "    # asset id: position of safe among keys -> normalized\n",
    "    safe_list = list(datasets.keys())\n",
    "    try:\n",
    "        asset_id_val = safe_list.index(safe) / max(1,len(safe_list))\n",
    "    except ValueError:\n",
    "        asset_id_val = 0.0\n",
    "    asset_id = np.full((window,1), asset_id_val, dtype=np.float32)\n",
    "\n",
    "    emb_rep = np.tile(embed_vec.reshape(1,-1), (window,1)).astype(np.float32)\n",
    "\n",
    "    obs = np.column_stack([\n",
    "        pct_norm[cols].values.astype(np.float32),\n",
    "        emb_rep,\n",
    "        balance_norm,\n",
    "        asset_id\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4698643c-7a72-4a5a-8342-8c3aec00c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - lot sizing and order placement (DRY_RUN safe)\n",
    "def compute_lot_from_balance(balance, vol, price, risk_pct=DEFAULT_RISK_PCT, min_lot=MIN_LOT, max_lot=MAX_LOT):\n",
    "    \"\"\"\n",
    "    Simple volatility-based lot sizing: risk_pct of balance / (volatility * price_scale)\n",
    "    This is heuristic; customize for your broker's contract specifications.\n",
    "    \"\"\"\n",
    "    risk_amount = balance * risk_pct\n",
    "    vol = max(vol, 1e-8)\n",
    "    # scaling factor adjusts units — change for your instrument contract size\n",
    "    price_scale = 1000.0\n",
    "    lot = risk_amount / (vol * price_scale)\n",
    "    lot = max(min_lot, min(max_lot, round(lot, 2)))\n",
    "    return float(lot)\n",
    "\n",
    "def place_order(symbol, direction, lot, sl, tp, dry_run=True, comment=\"multiasset_live\"):\n",
    "    \"\"\"\n",
    "    Place a market order on MT5 or simulate if dry_run True.\n",
    "    Returns order result object or a simulated dict.\n",
    "    \"\"\"\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"❌ Can't get tick for\", symbol)\n",
    "        return None\n",
    "\n",
    "    price = float(tick.ask if direction == \"BUY\" else tick.bid)\n",
    "    if dry_run:\n",
    "        # simulate a response-like dict\n",
    "        res = {\"retcode\": 10009, \"price\": price, \"comment\": \"DRY_RUN\", \"order\": None}\n",
    "        return res\n",
    "\n",
    "    request = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if direction == \"BUY\" else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl),\n",
    "        \"tp\": float(tp),\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": comment,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(request)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc764dfe-b462-41d8-b70e-0c534d8ea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - single pass: build obs, predict, compute sl/tp, place (or simulate) order and log\n",
    "def run_once_predict_and_place(symbols_list, model, scalers, embeddings, safe_names, window=WINDOW, dry_run=DRY_RUN):\n",
    "    # get account balance or fallback\n",
    "    acct = mt5.account_info()\n",
    "    balance = float(acct.balance) if acct is not None else 10000.0\n",
    "\n",
    "    # ensure log file header\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    for sym in symbols_list:\n",
    "        print(f\"\\n--- Processing: {sym} ---\")\n",
    "        obs, vol, last_price = fetch_and_build_obs(sym, window, scalers, embeddings, safe_names)\n",
    "        if obs is None:\n",
    "            print(f\"Insufficient obs for {sym} - skipping.\")\n",
    "            continue\n",
    "\n",
    "        # model.predict: expects batch dimension\n",
    "        try:\n",
    "            action, _ = model.predict(obs[np.newaxis, ...], deterministic=True)\n",
    "            # action may be array-like\n",
    "            if isinstance(action, (list, tuple, np.ndarray)):\n",
    "                action = int(action[0])\n",
    "            else:\n",
    "                action = int(action)\n",
    "        except Exception as e:\n",
    "            print(\"Model prediction error:\", e)\n",
    "            continue\n",
    "\n",
    "        if action == 0:\n",
    "            print(f\"{sym} -> HOLD\")\n",
    "            continue\n",
    "\n",
    "        direction = \"BUY\" if action == 1 else \"SELL\"\n",
    "        lot = compute_lot_from_balance(balance, vol, last_price)\n",
    "        # heuristic sl/tp distances (multiples of vol * price)\n",
    "        sl_dist = 1.5 * vol * last_price if vol > 0 else 0.01 * last_price\n",
    "        tp_dist = 2.5 * vol * last_price if vol > 0 else 0.02 * last_price\n",
    "        sl = last_price - sl_dist if direction == \"BUY\" else last_price + sl_dist\n",
    "        tp = last_price + tp_dist if direction == \"BUY\" else last_price - tp_dist\n",
    "\n",
    "        res = place_order(sym, direction, lot, sl, tp, dry_run=dry_run)\n",
    "\n",
    "        # Normalize retcode and message\n",
    "        retcode = res.get(\"retcode\") if isinstance(res, dict) else getattr(res, \"retcode\", None)\n",
    "        comment = res.get(\"comment\") if isinstance(res, dict) else getattr(res, \"comment\", \"\")\n",
    "\n",
    "        print(f\"Placed {direction} for {sym} | lot {lot} | price {last_price:.5f} | retcode {retcode}\")\n",
    "\n",
    "        # Log\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"symbol\": sym,\n",
    "            \"direction\": direction,\n",
    "            \"lot\": lot,\n",
    "            \"price\": last_price,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"retcode\": retcode,\n",
    "            \"comment\": comment,\n",
    "            \"dry_run\": dry_run\n",
    "        }\n",
    "        df_entry = pd.DataFrame([entry])\n",
    "        df_entry.to_csv(LOG_FILE, mode=\"a\", index=False, header=header)\n",
    "        header = False\n",
    "\n",
    "    print(\"\\nSingle pass completed; trades logged to\", LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e271669-7eaa-4eb0-9344-5ce2cefff837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - historical simulation evaluation\n",
    "def simulate_trades_on_history(datasets, model, symbols=None, window=WINDOW, horizon=10):\n",
    "    \"\"\"\n",
    "    Simulate trading with model on historical datasets.\n",
    "    For each symbol, slide a window and ask the model to act; if action != 0, simulate an instantaneous market entry\n",
    "    and close after 'horizon' bars; compute PnL based on price change and return aggregated metrics.\n",
    "    This is a simple backtest for evaluation only.\n",
    "    \"\"\"\n",
    "    trades = []\n",
    "    symbols = symbols or list(datasets.keys())\n",
    "    for safe in symbols:\n",
    "        df = datasets[safe]\n",
    "        # reconstruct an example mapping symbol name used in fetch_and_build_obs\n",
    "        # We will build obs from the df directly rather than using MT5\n",
    "        for i in range(window, len(df)-horizon):\n",
    "            window_df = df.iloc[i-window:i]\n",
    "            # build obs: normalized preprocessed already in df (o_pc ...), and need embedding & balance and asset id\n",
    "            # Build obs to match training obs shape\n",
    "            feat = window_df[['o_pc','h_pc','l_pc','c_pc','v_pc']].values.astype(np.float32)\n",
    "            # embedding\n",
    "            emb = embeddings.get(safe, np.zeros((0,)))\n",
    "            emb_rep = np.tile(emb.reshape(1,-1),(window,1)) if emb.size>0 else np.zeros((window,0))\n",
    "            balance_col = np.full((window,1), 1.0, dtype=np.float32)\n",
    "            # asset id\n",
    "            asset_id_val = list(datasets.keys()).index(safe) / max(1,len(datasets))\n",
    "            asset_col = np.full((window,1), asset_id_val, dtype=np.float32)\n",
    "            obs = np.concatenate([feat, emb_rep, balance_col, asset_col], axis=1)\n",
    "            # predict\n",
    "            try:\n",
    "                action, _ = model.predict(obs[np.newaxis,...], deterministic=True)\n",
    "                action = int(action[0]) if isinstance(action,(list,tuple,np.ndarray)) else int(action)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if action == 0:\n",
    "                continue\n",
    "            # Simulate entry at price at i (close of previous bar)\n",
    "            entry_price = float(df['Close_raw'].iat[i-1])\n",
    "            exit_price = float(df['Close_raw'].iat[i+horizon-1])\n",
    "            pos = 1 if action==1 else -1\n",
    "            pnl = (exit_price - entry_price) / entry_price * pos\n",
    "            trades.append({\"symbol\": safe, \"entry_i\": i, \"horizon\": horizon, \"action\":action, \"entry_price\":entry_price, \"exit_price\":exit_price, \"pnl\":pnl})\n",
    "    trades_df = pd.DataFrame(trades)\n",
    "    return trades_df\n",
    "\n",
    "def compute_trade_metrics(trades_df):\n",
    "    if trades_df.empty:\n",
    "        return {}\n",
    "    pnl = trades_df['pnl']\n",
    "    total = pnl.sum()\n",
    "    mean = pnl.mean()\n",
    "    std = pnl.std()\n",
    "    wins = (pnl>0).sum()\n",
    "    losses = (pnl<=0).sum()\n",
    "    win_rate = wins / (wins+losses) if (wins+losses)>0 else 0.0\n",
    "    # approximate daily Sharpe (assuming pnl is per trade; this is illustrative)\n",
    "    sharpe = (mean / std) if std>0 else np.nan\n",
    "    # max drawdown on cumulative\n",
    "    cum = pnl.cumsum()\n",
    "    roll_max = cum.cummax()\n",
    "    dd = (cum - roll_max).min()\n",
    "    metrics = {\n",
    "        \"n_trades\": int(len(pnl)),\n",
    "        \"total_pnl\": float(total),\n",
    "        \"mean_per_trade\": float(mean),\n",
    "        \"std_per_trade\": float(std),\n",
    "        \"win_rate\": float(win_rate),\n",
    "        \"sharpe_approx\": float(sharpe) if not np.isnan(sharpe) else None,\n",
    "        \"max_drawdown\": float(dd)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "470ad9ee-9dbf-4556-9a43-067866aff4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running historical simulation evaluation... (this may take a while)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 9 - run simulation evaluation\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning historical simulation evaluation... (this may take a while)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sim_trades = simulate_trades_on_history(datasets, model, symbols=\u001b[38;5;28mlist\u001b[39m(datasets.keys()), window=WINDOW, horizon=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m sim_trades.to_csv(os.path.join(MODEL_DIR, \u001b[33m\"\u001b[39m\u001b[33msimulated_trades.csv\u001b[39m\u001b[33m\"\u001b[39m), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      5\u001b[39m metrics = compute_trade_metrics(sim_trades)\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 9 - run simulation evaluation\n",
    "print(\"Running historical simulation evaluation... (this may take a while)\")\n",
    "sim_trades = simulate_trades_on_history(datasets, model, symbols=list(datasets.keys()), window=WINDOW, horizon=10)\n",
    "sim_trades.to_csv(os.path.join(MODEL_DIR, \"simulated_trades.csv\"), index=False)\n",
    "metrics = compute_trade_metrics(sim_trades)\n",
    "print(\"Simulation metrics:\", json.dumps(metrics, indent=2))\n",
    "# Quick summary plot of PnL distribution\n",
    "if not sim_trades.empty:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(sim_trades['pnl'], bins=60)\n",
    "    plt.title(\"Distribution of trade returns (simulated)\")\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d33e939-7299-46fe-939c-b2dec418fa83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'safe_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 10 - run single live pass across symbols (DRY_RUN=True recommended)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m symbols_live = \u001b[38;5;28mlist\u001b[39m(safe_names.keys())  \u001b[38;5;66;03m# raw symbol guesses mapped to safe names by safe_names mapping\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSymbols to query (raw -> safe):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m raw,safe \u001b[38;5;129;01min\u001b[39;00m safe_names.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'safe_names' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 10 - run single live pass across symbols (DRY_RUN=True recommended)\n",
    "symbols_live = list(safe_names.keys())  # raw symbol guesses mapped to safe names by safe_names mapping\n",
    "print(\"Symbols to query (raw -> safe):\")\n",
    "for raw,safe in safe_names.items():\n",
    "    print(\" \", raw, \"=>\", safe)\n",
    "\n",
    "# Example: user may want to explicitly set symbols to the MT5 names you use:\n",
    "# symbols_live = [\"Volatility 75 Index\", \"Volatility 10 Index\", \"EURUSD\"]\n",
    "\n",
    "run_once_predict_and_place(symbols_live, model, scalers, embeddings, safe_names, window=WINDOW, dry_run=DRY_RUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671ed6f-68b5-47b4-9e37-97d3cb700599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - analyze live trade log (simple)\n",
    "if os.path.exists(LOG_FILE):\n",
    "    df_log = pd.read_csv(LOG_FILE, parse_dates=['timestamp'])\n",
    "    print(\"Last trades from log:\")\n",
    "    display(df_log.tail(10))\n",
    "else:\n",
    "    print(\"No trade log found at\", LOG_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
