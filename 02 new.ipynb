{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe82337-6074-4692-b46b-fc258781a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — Install libs (run once if needed)\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "!{sys.executable} -m pip install --quiet numpy pandas matplotlib pillow ffmpeg-python\n",
    "!{sys.executable} -m pip install --quiet \"stable-baselines3==2.3.0\" \"gymnasium==0.29.1\" \"shimmy==0.2.1\" torch tensorboard MetaTrader5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4391d3c-8224-4705-91ca-1f0bac23eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — imports\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "try:\n",
    "    from stable_baselines3.common.env_checker import check_env\n",
    "except Exception:\n",
    "    check_env = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474345b0-af1f-4799-ab2e-0a4d994a48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — config\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "WINDOW = 50\n",
    "TRAIN_EPISODES = 50\n",
    "TOTAL_TIMESTEPS = TRAIN_EPISODES * 10_000\n",
    "N_ENVS = 4\n",
    "EMBED_DIM = 8\n",
    "\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "METRICS_FILE = os.path.join(MODEL_DIR, \"eval_metrics.json\")\n",
    "\n",
    "PPO_PARAMS = dict(\n",
    "    policy=\"MlpPolicy\",\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=256,\n",
    "    n_epochs=10,\n",
    "    ent_coef=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538e25e3-b8a6-4a66-9ba5-674a73e56527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — load normalized CSVs\n",
    "def load_normalized_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    csv_files = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No normalized CSVs found in {data_dir}. Run Notebook 01 first.\")\n",
    "    datasets = {}\n",
    "    for p in csv_files:\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        required = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if not all(c in df.columns for c in required):\n",
    "            # try to derive if raw present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = df.copy()\n",
    "                tmp['o_pc'] = tmp['open'].pct_change()\n",
    "                tmp['h_pc'] = tmp['high'].pct_change()\n",
    "                tmp['l_pc'] = tmp['low'].pct_change()\n",
    "                tmp['c_pc'] = tmp['close'].pct_change()\n",
    "                tmp['v_pc'] = tmp['volume'].pct_change()\n",
    "                tmp['Close_raw'] = tmp['close']\n",
    "                tmp = tmp.dropna()\n",
    "                df = tmp[required]\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing required columns and cannot be auto-converted.\")\n",
    "        else:\n",
    "            df = df[required].dropna()\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    if not datasets:\n",
    "        raise ValueError(\"No datasets passed the minimum length requirement.\")\n",
    "    print(f\"Loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "# Quick load\n",
    "# datasets = load_normalized_datasets(DATA_DIR, WINDOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8a0ea5-9ee5-4d11-909d-86c484a94e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — asset map & embeddings (ensures ordering matches datasets)\n",
    "def load_asset_map_and_embeddings(datasets, asset_map_file=ASSET_MAP_FILE, embed_file=EMBED_FILE, embed_dim=EMBED_DIM):\n",
    "    safe_names = list(datasets.keys())\n",
    "    # if file exists and matches keys -> load\n",
    "    if os.path.exists(asset_map_file):\n",
    "        try:\n",
    "            df_map = pd.read_csv(asset_map_file, index_col=0, squeeze=False)\n",
    "            # attempt to get mapping\n",
    "            ser = pd.read_csv(asset_map_file, index_col=0)\n",
    "            loaded = ser.to_dict()[ser.columns[0]]\n",
    "            if set(loaded.keys()) == set(safe_names):\n",
    "                # reorder according to datasets order\n",
    "                asset_to_idx = {s: int(loaded[s]) for s in safe_names}\n",
    "            else:\n",
    "                # overwrite to match datasets order\n",
    "                asset_to_idx = {s:i for i,s in enumerate(safe_names)}\n",
    "                pd.Series(asset_to_idx).to_csv(asset_map_file)\n",
    "        except Exception:\n",
    "            asset_to_idx = {s:i for i,s in enumerate(safe_names)}\n",
    "            pd.Series(asset_to_idx).to_csv(asset_map_file)\n",
    "    else:\n",
    "        asset_to_idx = {s:i for i,s in enumerate(safe_names)}\n",
    "        pd.Series(asset_to_idx).to_csv(asset_map_file)\n",
    "\n",
    "    n_assets = len(asset_to_idx)\n",
    "    if os.path.exists(embed_file):\n",
    "        emb = np.load(embed_file)\n",
    "        if emb.shape[0] != n_assets:\n",
    "            emb = np.random.randn(n_assets, embed_dim).astype(np.float32)\n",
    "            np.save(embed_file, emb)\n",
    "            print(\"Embedding count mismatch -> recreated embeddings.\")\n",
    "    else:\n",
    "        emb = np.random.randn(n_assets, embed_dim).astype(np.float32)\n",
    "        np.save(embed_file, emb)\n",
    "        print(\"Created new embeddings:\", embed_file)\n",
    "\n",
    "    return emb.astype(np.float32), asset_to_idx\n",
    "\n",
    "# Example:\n",
    "# embeddings, asset_map = load_asset_map_and_embeddings(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3100246c-17e1-4ea6-a028-f68fd89125a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — MT5 helpers + lot size calculation (graceful fallback)\n",
    "import MetaTrader5 as mt5\n",
    "\n",
    "def mt5_ensure_init():\n",
    "    try:\n",
    "        return mt5.initialize()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_symbol_info(symbol):\n",
    "    if not mt5_ensure_init():\n",
    "        return None\n",
    "    try:\n",
    "        return mt5.symbol_info(symbol)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def pip_value_per_lot_from_mt5(symbol, entry_price):\n",
    "    info = get_symbol_info(symbol)\n",
    "    if info is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        pip_val = info.trade_tick_value / info.trade_tick_size\n",
    "    except Exception:\n",
    "        contract_size = getattr(info, \"trade_contract_size\", 100000.0)\n",
    "        pip_val = (info.point / entry_price) * contract_size\n",
    "    return float(pip_val), float(info.point)\n",
    "\n",
    "def calculate_lot_size_mt5_fallback(symbol, balance, risk_percent, entry_price, stop_loss_price):\n",
    "    dollar_risk = balance * float(risk_percent)\n",
    "    pip_size = 0.01 if \"JPY\" in symbol else 0.0001\n",
    "    pip_risk = abs(entry_price - stop_loss_price) / pip_size\n",
    "    if pip_risk <= 0:\n",
    "        return 0.0\n",
    "    pip_value_per_lot = (pip_size / entry_price) * 100000.0\n",
    "    lot = dollar_risk / (pip_risk * pip_value_per_lot)\n",
    "    return round(max(lot, 0.0), 2)\n",
    "\n",
    "def calculate_lot_size(symbol, balance, risk_percent, entry_price, stop_loss_price):\n",
    "    pip_val, point = pip_value_per_lot_from_mt5(symbol, entry_price)\n",
    "    pip_risk = None\n",
    "    if point not in (None, 0):\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / point\n",
    "    else:\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / (0.01 if \"JPY\" in symbol else 0.0001)\n",
    "    if pip_risk <= 0:\n",
    "        return 0.0\n",
    "    dollar_risk = balance * float(risk_percent)\n",
    "    if pip_val is not None:\n",
    "        lot = dollar_risk / (pip_risk * pip_val)\n",
    "        info = get_symbol_info(symbol)\n",
    "        if info is not None:\n",
    "            try:\n",
    "                step = float(info.volume_step)\n",
    "                if step > 0:\n",
    "                    lot = round(lot / step) * step\n",
    "            except Exception:\n",
    "                pass\n",
    "        return round(max(lot, 0.0), 2)\n",
    "    return calculate_lot_size_mt5_fallback(symbol, balance, risk_percent, entry_price, stop_loss_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d0bbe6-f672-4b35-89ff-43978936c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — MultiAssetEnv with embeddings + position sizing\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class MultiAssetEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    MultiAssetEnv(datasets, asset_map, embeddings, asset_to_symbol=None, window=50, ...)\n",
    "    - datasets: dict safe_name -> dataframe with columns o_pc,h_pc,l_pc,c_pc,v_pc,Close_raw\n",
    "    - asset_map: dict safe_name -> index (embedding index)\n",
    "    - embeddings: ndarray (n_assets, embed_dim)\n",
    "    - asset_to_symbol: optional mapping safe_name -> MT5 symbol (if different)\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, datasets, asset_map, embeddings, asset_to_symbol=None, window=WINDOW,\n",
    "                 initial_balance=10_000, risk_per_trade=0.01, leverage=100):\n",
    "        super().__init__()\n",
    "        self.datasets = datasets\n",
    "        self.asset_map = asset_map\n",
    "        self.embeddings = embeddings\n",
    "        self.asset_to_symbol = asset_to_symbol or {s: s for s in datasets.keys()}\n",
    "        self.window = int(window)\n",
    "        self.initial_balance = float(initial_balance)\n",
    "        self.risk_per_trade = float(risk_per_trade)\n",
    "        self.leverage = leverage\n",
    "\n",
    "        self.safe_names = list(self.datasets.keys())\n",
    "        self.n_assets = len(self.safe_names)\n",
    "        self.asset_idx = 0\n",
    "        self.current_safe = self.safe_names[self.asset_idx]\n",
    "        self.data = self.datasets[self.current_safe]\n",
    "        self.ptr = self.window\n",
    "        self.balance = float(self.initial_balance)\n",
    "        self.position = 0\n",
    "        self.position_entry_price = None\n",
    "        self.position_lot = 0.0\n",
    "        self.position_sl = None\n",
    "        self.trades = []\n",
    "\n",
    "        self.embed_dim = self.embeddings.shape[1] if self.embeddings is not None else 0\n",
    "        n_features = 5\n",
    "        obs_dim = n_features + 1 + self.embed_dim\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.window, obs_dim), dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        self.np_random = None\n",
    "        self.seed(None)\n",
    "\n",
    "        # trackers for rendering / logging\n",
    "        self.prices = {s: [] for s in self.safe_names}\n",
    "        self.actions = {s: [] for s in self.safe_names}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        # sample a random asset index\n",
    "        self.asset_idx = int(self.np_random.integers(0, self.n_assets))\n",
    "        self.current_safe = self.safe_names[self.asset_idx]\n",
    "        self.data = self.datasets[self.current_safe]\n",
    "        self.ptr = self.window\n",
    "        self.balance = float(self.initial_balance)\n",
    "        self.position = 0\n",
    "        self.position_entry_price = None\n",
    "        self.position_lot = 0.0\n",
    "        self.position_sl = None\n",
    "        self.trades = []\n",
    "\n",
    "        # initialize trackers with initial window close prices\n",
    "        init_prices = list(self.data['Close_raw'].iloc[self.ptr-self.window:self.ptr].values)\n",
    "        self.prices[self.current_safe] = init_prices.copy()\n",
    "        self.actions[self.current_safe] = [0]*len(init_prices)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        window_data = self.data.iloc[self.ptr-self.window:self.ptr]\n",
    "        feat = window_data[['o_pc','h_pc','l_pc','c_pc','v_pc']].values.astype(np.float32)\n",
    "        balance_col = np.full((self.window,1), float(self.balance)/float(self.initial_balance), dtype=np.float32)\n",
    "        if self.embeddings is not None and self.asset_idx < self.embeddings.shape[0]:\n",
    "            emb = np.tile(self.embeddings[self.asset_idx].reshape(1,-1).astype(np.float32),(self.window,1))\n",
    "        else:\n",
    "            emb = np.zeros((self.window, self.embed_dim), dtype=np.float32)\n",
    "        obs = np.concatenate([feat, balance_col, emb], axis=1)\n",
    "        return obs\n",
    "\n",
    "    def _close_position(self, exit_price):\n",
    "        if self.position == 0 or self.position_lot <= 0:\n",
    "            return 0.0\n",
    "        symbol = self.asset_to_symbol.get(self.current_safe, self.current_safe)\n",
    "        pip_val, point = pip_value_per_lot_from_mt5(symbol, exit_price)\n",
    "        if pip_val is None:\n",
    "            pip_size = 0.01 if \"JPY\" in symbol else 0.0001\n",
    "            pip_val = (pip_size / exit_price) * 100000.0\n",
    "            point = pip_size\n",
    "        price_diff = exit_price - self.position_entry_price\n",
    "        pips = price_diff / (point if point not in (None,0) else 0.0001)\n",
    "        pips_signed = pips * (1 if self.position==1 else -1)\n",
    "        pnl_usd = pips_signed * pip_val * self.position_lot\n",
    "        self.balance += pnl_usd\n",
    "        trade = {\n",
    "            \"asset\": self.current_safe,\n",
    "            \"symbol\": symbol,\n",
    "            \"entry\": float(self.position_entry_price),\n",
    "            \"exit\": float(exit_price),\n",
    "            \"position\": int(self.position),\n",
    "            \"lot\": float(self.position_lot),\n",
    "            \"pnl\": float(pnl_usd),\n",
    "            \"balance_after\": float(self.balance),\n",
    "            \"timestamp\": str(self.data.index[self.ptr])\n",
    "        }\n",
    "        self.trades.append(trade)\n",
    "        self.position = 0\n",
    "        self.position_entry_price = None\n",
    "        self.position_lot = 0.0\n",
    "        self.position_sl = None\n",
    "        return float(pnl_usd)\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_close = float(self.data['Close_raw'].iloc[self.ptr-1])\n",
    "        new_close = float(self.data['Close_raw'].iloc[self.ptr])\n",
    "        reward = 0.0\n",
    "\n",
    "        # If opening a new opposite position, close existing first\n",
    "        if action == 0:\n",
    "            # HOLD - no explicit close here\n",
    "            pass\n",
    "        else:\n",
    "            if self.position != 0:\n",
    "                pnl_realized = self._close_position(prev_close)\n",
    "                reward += pnl_realized / max(1.0, self.initial_balance)\n",
    "            direction = 1 if action==1 else -1\n",
    "            recent = self.data['Close_raw'].iloc[self.ptr-self.window:self.ptr]\n",
    "            vol = float(recent.pct_change().std() * recent.iloc[-1]) if recent.pct_change().std() != 0 else 0.0\n",
    "            if vol <= 0 or np.isnan(vol):\n",
    "                sl_dist = max(0.0005, 0.001 * recent.iloc[-1])\n",
    "            else:\n",
    "                sl_dist = max(vol * 1.5, 0.0005 * recent.iloc[-1])\n",
    "            if direction == 1:\n",
    "                stop_loss = new_close - sl_dist\n",
    "            else:\n",
    "                stop_loss = new_close + sl_dist\n",
    "            symbol = self.asset_to_symbol.get(self.current_safe, self.current_safe)\n",
    "            try:\n",
    "                lot = calculate_lot_size(symbol, float(self.balance), float(self.risk_per_trade), float(new_close), float(stop_loss))\n",
    "            except Exception:\n",
    "                lot = calculate_lot_size_mt5_fallback(symbol, float(self.balance), float(self.risk_per_trade), float(new_close), float(stop_loss))\n",
    "            lot = max(0.01, round(min(lot, 100.0), 2))\n",
    "            self.position = direction\n",
    "            self.position_entry_price = new_close\n",
    "            self.position_lot = lot\n",
    "            self.position_sl = stop_loss\n",
    "            open_trade = {\n",
    "                \"asset\": self.current_safe,\n",
    "                \"symbol\": symbol,\n",
    "                \"entry\": float(self.position_entry_price),\n",
    "                \"position\": int(self.position),\n",
    "                \"lot\": float(self.position_lot),\n",
    "                \"stop_loss\": float(self.position_sl),\n",
    "                \"timestamp\": str(self.data.index[self.ptr])\n",
    "            }\n",
    "            self.trades.append(open_trade)\n",
    "\n",
    "        # shaping reward: unrealized PnL scaled by initial balance\n",
    "        reward_shaping = 0.0\n",
    "        if self.position != 0 and self.position_entry_price is not None:\n",
    "            symbol = self.asset_to_symbol.get(self.current_safe, self.current_safe)\n",
    "            pip_val, point = pip_value_per_lot_from_mt5(symbol, new_close)\n",
    "            if pip_val is None:\n",
    "                pip_size = 0.01 if \"JPY\" in symbol else 0.0001\n",
    "                pip_val = (pip_size / new_close) * 100000.0\n",
    "                point = pip_size\n",
    "            price_diff = new_close - self.position_entry_price\n",
    "            pips_signed = (price_diff / point) * (1 if self.position==1 else -1)\n",
    "            unrealized = pips_signed * pip_val * self.position_lot\n",
    "            reward_shaping = unrealized / max(1.0, self.initial_balance)\n",
    "        reward += float(reward_shaping)\n",
    "\n",
    "        # trackers\n",
    "        safe = self.current_safe\n",
    "        self.prices[safe].append(new_close)\n",
    "        self.actions[safe].append(int(action))\n",
    "\n",
    "        self.ptr += 1\n",
    "        done = self.ptr >= len(self.data)\n",
    "        info = {\"balance\": float(self.balance), \"asset\": self.current_safe}\n",
    "        return self._get_obs(), float(reward), bool(done), False, info\n",
    "\n",
    "    def render(self, mode='human', animate=False, interval=200, save_path=None, fps=10):\n",
    "        import matplotlib.animation as animation\n",
    "        assets_with_data = [s for s in self.safe_names if len(self.prices.get(s,[]))>0]\n",
    "        if not assets_with_data:\n",
    "            print(\"⚠️ Nothing recorded to render yet.\")\n",
    "            return\n",
    "        n = len(assets_with_data)\n",
    "        fig, axes = plt.subplots(n,1, figsize=(10,4*n), sharex=True)\n",
    "        if n==1:\n",
    "            axes=[axes]\n",
    "        if not animate:\n",
    "            for i,s in enumerate(assets_with_data):\n",
    "                prices = np.array(self.prices[s])\n",
    "                acts = np.array(self.actions[s])\n",
    "                steps = np.arange(len(prices))\n",
    "                axes[i].plot(steps, prices, label=f\"{s} price\")\n",
    "                axes[i].scatter(steps[acts==1], prices[acts==1], marker='^', color='green', label='Buy')\n",
    "                axes[i].scatter(steps[acts==2], prices[acts==2], marker='v', color='red', label='Sell')\n",
    "                axes[i].legend(); axes[i].grid(True)\n",
    "            plt.tight_layout()\n",
    "            if save_path and save_path.lower().endswith(('.png','.jpg','.pdf')):\n",
    "                plt.savefig(save_path, dpi=200); print(\"Saved static render to\", save_path)\n",
    "            plt.show()\n",
    "            return\n",
    "        # animated\n",
    "        lines, buys, sells = [], [], []\n",
    "        for ax, s in zip(axes, assets_with_data):\n",
    "            line, = ax.plot([],[],lw=2)\n",
    "            buy_sc = ax.scatter([],[], marker='^', color='green')\n",
    "            sell_sc = ax.scatter([],[], marker='v', color='red')\n",
    "            lines.append(line); buys.append(buy_sc); sells.append(sell_sc)\n",
    "            arr = np.array(self.prices[s])\n",
    "            ax.set_xlim(0, max(1,len(arr)))\n",
    "            ax.set_ylim(np.min(arr)*0.98, np.max(arr)*1.02)\n",
    "            ax.set_title(s)\n",
    "            ax.grid(True)\n",
    "        def update(frame):\n",
    "            artists=[]\n",
    "            for i,s in enumerate(assets_with_data):\n",
    "                pr = np.array(self.prices[s]); ac = np.array(self.actions[s])\n",
    "                f = frame if frame<=len(pr) else len(pr)\n",
    "                x = np.arange(f); y = pr[:f]\n",
    "                lines[i].set_data(x,y); artists.append(lines[i])\n",
    "                buys_idx = x[ac[:f]==1] if f>0 else []\n",
    "                sells_idx = x[ac[:f]==2] if f>0 else []\n",
    "                if len(buys_idx)>0:\n",
    "                    buys[i].set_offsets(np.c_[buys_idx, pr[:f][ac[:f]==1]])\n",
    "                else:\n",
    "                    buys[i].set_offsets([])\n",
    "                if len(sells_idx)>0:\n",
    "                    sells[i].set_offsets(np.c_[sells_idx, pr[:f][ac[:f]==2]])\n",
    "                else:\n",
    "                    sells[i].set_offsets([])\n",
    "                artists += [buys[i], sells[i]]\n",
    "            return artists\n",
    "        frames = len(self.prices[assets_with_data[0]])\n",
    "        ani = animation.FuncAnimation(fig, update, frames=frames, interval=interval, blit=True, repeat=False)\n",
    "        if save_path:\n",
    "            ext = os.path.splitext(save_path)[1].lower()\n",
    "            if ext=='.mp4':\n",
    "                ani.save(save_path, writer='ffmpeg', fps=fps)\n",
    "            elif ext=='.gif':\n",
    "                ani.save(save_path, writer='pillow', fps=fps)\n",
    "            print(\"Saved animation to\", save_path)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c06a06-2ff8-441c-a926-ee19f38c45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — env factory for DummyVecEnv\n",
    "def make_env_factory(datasets, asset_map, embeddings, asset_to_symbol, window):\n",
    "    def _init():\n",
    "        return MultiAssetEnv(datasets, asset_map, embeddings, asset_to_symbol=asset_to_symbol, window=window)\n",
    "    return _init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac90751c-ce08-4ffe-8044-0c8ae20e0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — train & save\n",
    "def train_and_save(datasets, asset_map, embeddings, asset_to_symbol=None, total_timesteps=TOTAL_TIMESTEPS, n_envs=N_ENVS):\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    env_fns = [make_env_factory(datasets, asset_map, embeddings, asset_to_symbol or {s:s for s in datasets.keys()}, WINDOW) for _ in range(n_envs)]\n",
    "    vec_env = DummyVecEnv(env_fns)\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    model = PPO(**PPO_PARAMS, env=vec_env, tensorboard_log=os.path.join(MODEL_DIR, \"tensorboard\"))\n",
    "    print(\"Starting training:\", total_timesteps, \"timesteps\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    model.save(MODEL_FILE)\n",
    "    vec_env.save(VEC_FILE)\n",
    "    print(\"Saved model and VecNormalize wrapper.\")\n",
    "    return model, vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4bd615d-a0d9-43d9-8660-5859402636b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — safe action extraction (use in evaluation)\n",
    "def extract_action_scalar(action):\n",
    "    if isinstance(action, (int, np.integer)):\n",
    "        return int(action)\n",
    "    a = np.array(action)\n",
    "    if a.size == 1:\n",
    "        return int(a.flatten()[0])\n",
    "    return int(a.flatten()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868fd159-d9f6-4124-a786-e3238acd8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — evaluation (deterministic)\n",
    "def evaluate_model(model_path, datasets, embeddings, n_episodes=10, window=WINDOW):\n",
    "    model = PPO.load(model_path)\n",
    "    emb_dummy = embeddings if embeddings is not None else np.zeros((len(datasets), EMBED_DIM), dtype=np.float32)\n",
    "    asset_map = {s:i for i,s in enumerate(datasets.keys())}\n",
    "    env = MultiAssetEnv(datasets, asset_map, emb_dummy, window=window)\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_r = 0.0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            act = extract_action_scalar(action)\n",
    "            obs, reward, done, truncated, info = env.step(act)\n",
    "            ep_r += float(reward)\n",
    "        rewards.append(ep_r)\n",
    "    metrics = {\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"mean_reward\": float(np.mean(rewards)),\n",
    "        \"std_reward\": float(np.std(rewards)),\n",
    "        \"total_reward\": float(np.sum(rewards)),\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "    with open(METRICS_FILE, \"w\") as fh:\n",
    "        json.dump(metrics, fh, indent=2)\n",
    "    print(\"Evaluation metrics:\", metrics)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76d1a-d0e8-404c-88b4-da557a58fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Starting training: 500000 timesteps\n",
      "Logging to models\\multiasset\\tensorboard\\PPO_5\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — run pipeline\n",
    "datasets = load_normalized_datasets(DATA_DIR, WINDOW)\n",
    "embeddings, asset_map = load_asset_map_and_embeddings(datasets)\n",
    "# If needed, asset_to_symbol mapping (safe->MT5 symbol). Default uses safe==symbol\n",
    "asset_to_symbol = {s: s for s in datasets.keys()}\n",
    "\n",
    "# Train\n",
    "model, vec_env = train_and_save(datasets, asset_map, embeddings, asset_to_symbol, TOTAL_TIMESTEPS, N_ENVS)\n",
    "\n",
    "# Evaluate\n",
    "metrics = evaluate_model(MODEL_FILE, datasets, embeddings, n_episodes=10, window=WINDOW)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100379af-0bc3-4a94-8454-c280a6a79e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend & Stop-hunt utilities\n",
    "# Copy this whole cell into your notebook.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "from collections import deque, Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Local extrema / swing points\n",
    "# -----------------------------\n",
    "def find_swing_points(prices: pd.Series, order:int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect local highs and lows (simple approach).\n",
    "    Args:\n",
    "        prices: pd.Series of close prices indexed by datetime\n",
    "        order: number of bars on each side to use for local extremum detection\n",
    "    Returns:\n",
    "        DataFrame with columns ['is_high','is_low'] boolean mask aligned with prices.\n",
    "    \"\"\"\n",
    "    n = len(prices)\n",
    "    is_high = np.zeros(n, dtype=bool)\n",
    "    is_low = np.zeros(n, dtype=bool)\n",
    "    vals = prices.values\n",
    "    for i in range(order, n-order):\n",
    "        left = vals[i-order:i]\n",
    "        right = vals[i+1:i+1+order]\n",
    "        center = vals[i]\n",
    "        if (center > left).all() and (center > right).all():\n",
    "            is_high[i] = True\n",
    "        if (center < left).all() and (center < right).all():\n",
    "            is_low[i] = True\n",
    "    return pd.DataFrame({'is_high': is_high, 'is_low': is_low}, index=prices.index)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Structure-based trend (HH/HL and LH/LL)\n",
    "# -----------------------------\n",
    "def trend_by_structure(prices: pd.Series,\n",
    "                       order:int = 5,\n",
    "                       lookback_swings:int = 6) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Determine trend using recent swing structure: HH/HL -> uptrend, LH/LL -> downtrend, else 0 (sideways).\n",
    "    Args:\n",
    "        prices: close price series\n",
    "        order: window for swing point detection\n",
    "        lookback_swings: number of recent swings to evaluate pattern\n",
    "    Returns:\n",
    "        pd.Series of {-1,0,1} per timestamp (trend label)\n",
    "    \"\"\"\n",
    "    swings = find_swing_points(prices, order=order)\n",
    "    # extract actual swing points (value + index)\n",
    "    highs_idx = list(prices[swings['is_high']].index)\n",
    "    lows_idx = list(prices[swings['is_low']].index)\n",
    "\n",
    "    # Create combined sorted list of swings (time, type, price)\n",
    "    combined = []\n",
    "    for idx in highs_idx:\n",
    "        combined.append((idx, 'H', prices.loc[idx]))\n",
    "    for idx in lows_idx:\n",
    "        combined.append((idx, 'L', prices.loc[idx]))\n",
    "    combined.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Build swing sequence (type, price)\n",
    "    seq = [(t, typ, p) for t,typ,p in combined]\n",
    "\n",
    "    # For each timestamp, infer trend from last lookback_swings swings\n",
    "    trend = pd.Series(0, index=prices.index)\n",
    "    # create sliding list of swing types/prices\n",
    "    for i in range(len(seq)):\n",
    "        end = i+1\n",
    "        start = max(0, end - lookback_swings)\n",
    "        window = seq[start:end]\n",
    "        types = [t[1] for t in window]\n",
    "        prices_window = [t[2] for t in window]\n",
    "        # We need at least 3 swings to infer a structure\n",
    "        if len(window) < 3:\n",
    "            continue\n",
    "        # Determine monotonic patterns:\n",
    "        # Uptrend if highs increasing and lows increasing (both monotonic increasing)\n",
    "        highs = [p for (ti, tp, p) in window if tp=='H']\n",
    "        lows  = [p for (ti, tp, p) in window if tp=='L']\n",
    "        def is_strictly_increasing(arr):\n",
    "            return all(x2 > x1 for x1,x2 in zip(arr, arr[1:])) if len(arr)>=2 else False\n",
    "        def is_strictly_decreasing(arr):\n",
    "            return all(x2 < x1 for x1,x2 in zip(arr, arr[1:])) if len(arr)>=2 else False\n",
    "        idx_time = window[-1][0]  # assign label at last swing time\n",
    "        if is_strictly_increasing(highs) and is_strictly_increasing(lows):\n",
    "            trend.loc[idx_time] = 1\n",
    "        elif is_strictly_decreasing(highs) and is_strictly_decreasing(lows):\n",
    "            trend.loc[idx_time] = -1\n",
    "        else:\n",
    "            trend.loc[idx_time] = 0\n",
    "\n",
    "    # propagate last known swing label forward to all timestamps until next swing\n",
    "    last_val = 0\n",
    "    for t in trend.index:\n",
    "        if trend.loc[t] != 0:\n",
    "            last_val = trend.loc[t]\n",
    "        trend.loc[t] = last_val\n",
    "    return trend.fillna(0).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Regression slope label (future-return style)\n",
    "# -----------------------------\n",
    "def trend_by_regression(prices: pd.Series,\n",
    "                        future_window:int = 50,\n",
    "                        up_thresh:float = 0.005,\n",
    "                        down_thresh:float = -0.005,\n",
    "                        vol_window:int = 50) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Label trend by fitting a linear regression to FUTURE closes and thresholding normalized slope.\n",
    "    Args:\n",
    "        prices: pd.Series closes\n",
    "        future_window: horizon to compute slope over future bars\n",
    "        up_thresh/down_thresh: thresholds on normalized slope for label decision\n",
    "        vol_window: window for recent volatility normalization\n",
    "    Returns:\n",
    "        pd.Series of {-1,0,1} shape len(prices), with NaN near end where future not available (filled 0)\n",
    "    \"\"\"\n",
    "    n = len(prices)\n",
    "    labels = np.zeros(n, dtype=int)\n",
    "    close_vals = prices.values\n",
    "    for t in range(n - future_window):\n",
    "        y = close_vals[t+1:t+1+future_window]\n",
    "        x = np.arange(len(y))\n",
    "        # linear fit slope\n",
    "        a = np.polyfit(x, y, 1)[0]\n",
    "        # normalize slope by price and recent volatility\n",
    "        recent_diff = np.diff(prices.values[max(0, t-vol_window):t+1]) if t>0 else np.array([0.0])\n",
    "        recent_vol = np.std(recent_diff) if recent_diff.size>0 else 1.0\n",
    "        if recent_vol <= 0:\n",
    "            recent_vol = 1e-8\n",
    "        norm_slope = a / (prices.values[t] * recent_vol)\n",
    "        if norm_slope > up_thresh:\n",
    "            labels[t] = 1\n",
    "        elif norm_slope < down_thresh:\n",
    "            labels[t] = -1\n",
    "        else:\n",
    "            labels[t] = 0\n",
    "    return pd.Series(labels, index=prices.index)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Combined/ensemble trend label (for auxiliary task)\n",
    "# -----------------------------\n",
    "def combined_trend_label(prices: pd.Series,\n",
    "                         structure_order:int=5, structure_lookback:int=6,\n",
    "                         future_window:int=50, up_thresh:float=0.005, down_thresh:float=-0.005,\n",
    "                         voting:bool=True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Combine structure-based and regression-based labels using voting/priority:\n",
    "      - If structure and regression agree -> that label\n",
    "      - If disagree -> optionally prioritize regression or return 0\n",
    "      - voting=True uses majority (if more signals available you can extend)\n",
    "    Returns pd.Series {-1,0,1}\n",
    "    \"\"\"\n",
    "    struct = trend_by_structure(prices, order=structure_order, lookback_swings=structure_lookback)\n",
    "    reg = trend_by_regression(prices, future_window=future_window, up_thresh=up_thresh, down_thresh=down_thresh)\n",
    "    combined = pd.Series(0, index=prices.index)\n",
    "    for t in prices.index:\n",
    "        s = int(struct.get(t, 0))\n",
    "        r = int(reg.get(t, 0))\n",
    "        if s == r:\n",
    "            combined.loc[t] = s\n",
    "        else:\n",
    "            # if one indicates strong trend and the other 0, take the non-zero\n",
    "            if s != 0 and r == 0:\n",
    "                combined.loc[t] = s\n",
    "            elif r != 0 and s == 0:\n",
    "                combined.loc[t] = r\n",
    "            else:\n",
    "                # conflict: prefer regression (short-term future), but you can change to s or 0\n",
    "                combined.loc[t] = r\n",
    "    return combined\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Detect stop-loss hunt spikes\n",
    "# -----------------------------\n",
    "def detect_stop_loss_hunt(prices: pd.Series,\n",
    "                          intrabar_returns: pd.Series=None,\n",
    "                          spike_multiplier:float=4.0,\n",
    "                          short_lived_window:int=5,\n",
    "                          min_spike_size:float=None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Detect potential stop-loss hunt events:\n",
    "      - Find spikes in returns that are large relative to recent distribution (z-score)\n",
    "      - Mark spikes that reverse within `short_lived_window` bars (short-lived extremes)\n",
    "    Args:\n",
    "        prices: close price series\n",
    "        intrabar_returns: optional series of bar returns (if you have tick intrabar data use that)\n",
    "        spike_multiplier: multiple of rolling std to mark spike\n",
    "        short_lived_window: bars to check for reversal\n",
    "        min_spike_size: absolute minimum return magnitude to qualify as spike (optional)\n",
    "    Returns:\n",
    "        pd.Series boolean indexed by prices indicating suspected stop-hunt bar (True)\n",
    "    \"\"\"\n",
    "    # compute returns if not supplied\n",
    "    if intrabar_returns is None:\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "    else:\n",
    "        returns = intrabar_returns.reindex(prices.index).fillna(0)\n",
    "    rolling_std = returns.rolling(window=50, min_periods=5).std().fillna(0.0)\n",
    "    z = returns / (rolling_std.replace(0, np.nan))\n",
    "    # candidate spikes\n",
    "    is_spike = (z.abs() > spike_multiplier)\n",
    "    if min_spike_size is not None:\n",
    "        is_spike &= (returns.abs() >= min_spike_size)\n",
    "    is_spike = is_spike.fillna(False)\n",
    "    # mark short-lived reversal: spike followed by reversal of sign within short_lived_window\n",
    "    n = len(returns)\n",
    "    idxs = prices.index\n",
    "    hunt_flag = pd.Series(False, index=prices.index)\n",
    "    for i in range(n):\n",
    "        if not is_spike.iloc[i]:\n",
    "            continue\n",
    "        sign = np.sign(returns.iloc[i])\n",
    "        # check next short_lived_window bars for return of opposite sign with magnitude > 50% spike\n",
    "        window_end = min(n, i+1+short_lived_window)\n",
    "        reversed_found = False\n",
    "        for j in range(i+1, window_end):\n",
    "            if np.sign(returns.iloc[j]) == -sign and abs(returns.iloc[j]) > 0.5 * abs(returns.iloc[i]):\n",
    "                reversed_found = True\n",
    "                break\n",
    "        if reversed_found:\n",
    "            hunt_flag.iloc[i] = True\n",
    "    return hunt_flag\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Simulate stop-hunt noise (inject spikes) for robust training\n",
    "# -----------------------------\n",
    "def inject_stop_hunt_noise(prices: pd.Series,\n",
    "                           prob:float = 0.002,\n",
    "                           spike_multiplier:float=0.02,\n",
    "                           rng_seed:int = None) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Produce a new price series with occasional injected spikes (up or down) that reverse quickly.\n",
    "    Args:\n",
    "        prices: original close price series (pd.Series)\n",
    "        prob: probability of injecting a spike at any bar\n",
    "        spike_multiplier: relative price change magnitude for spike (fraction of price)\n",
    "        rng_seed: seed for reproducibility\n",
    "    Returns:\n",
    "        pd.Series with same index and modified prices\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    p = prices.copy().astype(float)\n",
    "    n = len(p)\n",
    "    for i in range(2, n-3):\n",
    "        if rng.random() < prob:\n",
    "            sign = rng.choice([1, -1])\n",
    "            spike_size = (1.0 + sign * spike_multiplier)\n",
    "            # apply instantaneous spike at bar i and immediate partial reversal next bar\n",
    "            p.iloc[i] = p.iloc[i] * spike_size\n",
    "            # partial reversal next bar:\n",
    "            p.iloc[i+1] = p.iloc[i] * (1.0 - 0.8 * sign * spike_multiplier)\n",
    "            # ensure subsequent next bars adjust smoothly (simple linear interpolation)\n",
    "            # note: this is simple; you can adapt for ticks intrabar injection\n",
    "    return p\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Confusion metrics vs true trend / compare to baseline\n",
    "# -----------------------------\n",
    "def trend_confusion_report(y_true: pd.Series, y_pred: pd.Series, labels=[-1,0,1]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute confusion matrix and classification metrics for trend labels.\n",
    "    Returns dict with confusion matrix, accuracy, and sklearn classification report.\n",
    "    \"\"\"\n",
    "    # align\n",
    "    y_true, y_pred = y_true.align(y_pred, join='inner', fill_value=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)\n",
    "    cls_report = classification_report(y_true, y_pred, labels=labels, zero_division=0, output_dict=True)\n",
    "    return {\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'labels': labels,\n",
    "        'accuracy': float(acc),\n",
    "        'precision_per_label': prec.tolist(),\n",
    "        'recall_per_label': rec.tolist(),\n",
    "        'f1_per_label': f1.tolist(),\n",
    "        'classification_report': cls_report\n",
    "    }\n",
    "\n",
    "def compare_with_baseline(y_true: pd.Series, y_pred: pd.Series, baseline_preds: Dict[str, pd.Series]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare model predictions against one or more baseline prediction series.\n",
    "    baseline_preds: dict name->pd.Series\n",
    "    Returns metrics dict keyed by model/baseline name.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results['model'] = trend_confusion_report(y_true, y_pred)\n",
    "    for name, bs in baseline_preds.items():\n",
    "        results[name] = trend_confusion_report(y_true, bs)\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Normalization + seeds + cross-validation utilities\n",
    "# -----------------------------\n",
    "def normalize_per_asset(df: pd.DataFrame, feature_cols:List[str]=['o_pc','h_pc','l_pc','c_pc','v_pc']) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Z-score normalize selected feature columns and return (normalized_df, means, stds)\n",
    "    \"\"\"\n",
    "    means = df[feature_cols].mean()\n",
    "    stds = df[feature_cols].std().replace(0,1.0)\n",
    "    norm = df.copy()\n",
    "    norm[feature_cols] = (df[feature_cols] - means) / stds\n",
    "    return norm, means, stds\n",
    "\n",
    "def set_global_seed(seed:int):\n",
    "    \"\"\"Set python/numpy/random torch seeds for reproducibility (call at top of training/eval).\"\"\"\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Example baseline predictors\n",
    "# -----------------------------\n",
    "def simple_baselines(prices: pd.Series, short_window:int=5, long_window:int=20) -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Compute some simple baseline trend predictors (no external indicators other than moving averages of price)\n",
    "    Returns dict of name->pd.Series of {-1,0,1}\n",
    "    \"\"\"\n",
    "    close = prices\n",
    "    ma_short = close.rolling(short_window).mean()\n",
    "    ma_long = close.rolling(long_window).mean()\n",
    "    buy = (ma_short > ma_long).astype(int)\n",
    "    # Convert to -1/0/1: if short > long -> 1 else -1 (aggressive) or 0 for neutral thresholding\n",
    "    baseline = pd.Series(0, index=prices.index)\n",
    "    baseline[ma_short > ma_long] = 1\n",
    "    baseline[ma_short < ma_long] = -1\n",
    "    return {'ma_crossover': baseline.fillna(0).astype(int)}\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Usage examples (pseudo)\n",
    "# -----------------------------\n",
    "# Example pseudo-code to produce labels and evaluate:\n",
    "# raw_df: a DataFrame for a single asset with 'close' or 'Close_raw' column.\n",
    "# prices = raw_df['Close_raw'] if 'Close_raw' in raw_df.columns else raw_df['close']\n",
    "# labels_aux = combined_trend_label(prices)                    # supervisory trend labels\n",
    "# stophunt_flags = detect_stop_loss_hunt(prices)              # boolean series of suspected hunts\n",
    "# baseline_preds = simple_baselines(prices)\n",
    "# results = compare_with_baseline(labels_aux, model_preds_series, baseline_preds)\n",
    "# confusion = trend_confusion_report(labels_aux, model_preds_series)\n",
    "#\n",
    "# To make model robust:\n",
    "# p_noisy = inject_stop_hunt_noise(prices, prob=0.002, spike_multiplier=0.02, rng_seed=42)\n",
    "# use p_noisy in some episodes during RL training (data augmentation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
