{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e280ea-2cc6-4386-be04-e7e188e66ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.45.1)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Cell 1 — Install dependencies (run once if needed)\n",
    "# ==================================================\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install --quiet numpy pandas matplotlib seaborn MetaTrader5 stable-baselines3 gymnasium==0.29.1 torch ffmpeg-python pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8043273-6afd-41fd-b0c7-89d0aa2845ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 2 — Imports & configuration\n",
    "# ==================================================\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import MetaTrader5 as mt5\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33f2c4b-c91d-48f7-ba40-ed106104f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 3 — Paths (adjust if different)\n",
    "# ==================================================\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "SCALER_GLOB = os.path.join(DATA_DIR, \"*_scaler.csv\")  # per-symbol scalers\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0572971-647a-4988-9baa-6cebeded81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 4 — Live configuration\n",
    "# ==================================================\n",
    "TIMEFRAME = \"M15\"        # timeframe to request from MT5\n",
    "WINDOW = 50\n",
    "RISK_PER_TRADE = 0.02   # default risk per trade (1%)\n",
    "MIN_LOT = 0.01\n",
    "MAX_LOT = 10.0\n",
    "DEVIATION = 20\n",
    "MAGIC = 2025001\n",
    "DRY_RUN_DEFAULT = False  # safety default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8898925-d1ed-42a1-b138-55655824c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 5 — Helpers: safe action extraction, load datasets, scalers, embeddings\n",
    "# ==================================================\n",
    "def extract_action_scalar(action):\n",
    "    import numpy as _np\n",
    "    if isinstance(action, (int, _np.integer)):\n",
    "        return int(action)\n",
    "    a = _np.array(action)\n",
    "    if a.size == 1:\n",
    "        return int(a.flatten()[0])\n",
    "    return int(a.flatten()[0])\n",
    "\n",
    "def load_scalers(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in glob.glob(os.path.join(data_dir, \"*_scaler.csv\")):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        try:\n",
    "            df = pd.read_csv(p, index_col=0)\n",
    "            scalers[safe] = {\"mean\": df['mean'].astype(float), \"std\": df['std'].astype(float)}\n",
    "        except Exception:\n",
    "            scalers[safe] = None\n",
    "    return scalers\n",
    "\n",
    "def load_assets_and_embeddings(data_dir=DATA_DIR, embed_file=EMBED_FILE):\n",
    "    # load asset order from asset_to_idx.csv if present\n",
    "    asset_map = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        try:\n",
    "            s = pd.read_csv(ASSET_MAP_FILE, index_col=0, squeeze=True)\n",
    "            asset_map = (s.to_dict() if hasattr(s, \"to_dict\") else dict(s))\n",
    "            # s might be safe->idx or vice-versa; normalize\n",
    "            # produce ordered list by idx\n",
    "            try:\n",
    "                ordered = sorted(asset_map.items(), key=lambda x:int(x[1]))\n",
    "                safes = [k for k,_ in ordered]\n",
    "            except Exception:\n",
    "                safes = list(asset_map.keys())\n",
    "        except Exception:\n",
    "            safes = []\n",
    "    else:\n",
    "        # fallback: read normalized csv names\n",
    "        safes = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in sorted(glob.glob(os.path.join(DATA_DIR,\"*_normalized.csv\")))]\n",
    "    embeddings = np.load(embed_file) if os.path.exists(embed_file) else np.zeros((len(safes), 1), dtype=np.float32)\n",
    "    # build canonical list\n",
    "    safes = safes if len(safes)>0 else list({os.path.basename(p).replace(\"_normalized.csv\",\"\"):None for p in glob.glob(os.path.join(DATA_DIR,\"*_normalized.csv\"))}.keys())\n",
    "    return safes, embeddings, load_scalers(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cf371a5-fc37-48b6-ac79-9a5bc36194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 6 — MT5 init helpers (safe)\n",
    "# ==================================================\n",
    "def mt5_init_if_needed():\n",
    "    try:\n",
    "        if not mt5.initialize():\n",
    "            # sometimes initialize() returns False but terminal still ok; give another try\n",
    "            return mt5.initialize()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def mt5_shutdown_if_needed():\n",
    "    try:\n",
    "        mt5.shutdown()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5feddcf2-967a-4697-b076-14d9e3ac6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 7 — symbol_published (symbol)\n",
    "# ==================================================\n",
    "def ensure_symbol_published(symbol):\n",
    "    \"\"\"Ensure symbol is visible in Market Watch; attempt to add if not.\"\"\"\n",
    "    try:\n",
    "        sinfo = mt5.symbol_info(symbol)\n",
    "        if sinfo is None:\n",
    "            # try to enable the symbol\n",
    "            mt5.symbol_select(symbol, True)\n",
    "            sinfo = mt5.symbol_info(symbol)\n",
    "        return sinfo is not None\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5db0d78-94c9-4314-b2dd-4845bcabe77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 8 — Time frame mapping\n",
    "# ==================================================\n",
    "# Cell 5 — Build observation for a symbol in the same format used in training\n",
    "def timeframe_to_mt5(tf_str):\n",
    "    TF_MAP = {\n",
    "        \"M1\": mt5.TIMEFRAME_M1, \"M5\": mt5.TIMEFRAME_M5, \"M15\": mt5.TIMEFRAME_M15,\n",
    "        \"M30\": mt5.TIMEFRAME_M30, \"H1\": mt5.TIMEFRAME_H1, \"H4\": mt5.TIMEFRAME_H4,\n",
    "        \"D1\": mt5.TIMEFRAME_D1\n",
    "    }\n",
    "    return TF_MAP.get(tf_str.upper(), mt5.TIMEFRAME_M15)\n",
    "\n",
    "TF_MT5 = timeframe_to_mt5(TIMEFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18ad9856-e2ef-423d-9da1-a65351610473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 8 — \n",
    "# ==================================================\n",
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names, fallback_embedding_dim=None):\n",
    "    \"\"\"\n",
    "    Build observation window for a symbol using:\n",
    "    - normalized percent-change features\n",
    "    - saved scaler (mean/std)\n",
    "    - saved embedding vector\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): raw symbol, e.g. \"EURUSD\"\n",
    "        window (int): sliding window length\n",
    "        scalers (dict): {safe: {\"mean\": Series, \"std\": Series}}\n",
    "        embeddings (dict): {safe: np.ndarray}\n",
    "        safe_names (dict): {raw_symbol: safe_symbol}\n",
    "        fallback_embedding_dim (int or None): optional zero-vector if embedding missing\n",
    "\n",
    "    Returns:\n",
    "        obs_window (np.ndarray)\n",
    "        scaler_info (dict)\n",
    "        embedding (np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Convert symbol → safe-name\n",
    "    # -----------------------------------------------------------\n",
    "    if symbol not in safe_names:\n",
    "        print(f\"❌ No safe-name found for symbol: {symbol}\")\n",
    "        return None, None, None\n",
    "\n",
    "    safe = safe_names[symbol]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Check required preprocessing objects\n",
    "    # -----------------------------------------------------------\n",
    "    if safe not in scalers:\n",
    "        print(f\"❌ Missing scaler for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if safe not in embeddings:\n",
    "        if fallback_embedding_dim is None:\n",
    "            print(f\"❌ Missing embedding for: {safe}\")\n",
    "            return None, None, None\n",
    "        else:\n",
    "            print(f\"⚠️ Missing embedding → using zero vector for {safe}\")\n",
    "            embedding = np.zeros(fallback_embedding_dim)\n",
    "    else:\n",
    "        embedding = embeddings[safe]\n",
    "\n",
    "    scaler_info = scalers[safe]\n",
    "\n",
    "    # Extract mean / std\n",
    "    mean = scaler_info[\"mean\"]\n",
    "    std = scaler_info[\"std\"].replace(0, 1.0)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Load raw normalized dataset for this symbol\n",
    "    # -----------------------------------------------------------\n",
    "    if safe not in datasets:\n",
    "        print(f\"❌ No dataset loaded for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = datasets[safe].copy()\n",
    "\n",
    "    # Must contain the required columns\n",
    "    required_columns = [\"o_pc\", \"h_pc\", \"l_pc\", \"c_pc\", \"v_pc\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"❌ Missing required column '{col}' in dataset for {safe}\")\n",
    "            return None, None, None\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) Apply normalization using stored scaler\n",
    "    # -----------------------------------------------------------\n",
    "    df_norm = (df[required_columns] - mean[required_columns]) / std[required_columns]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5) Extract last window\n",
    "    # -----------------------------------------------------------\n",
    "    if len(df_norm) < window:\n",
    "        print(f\"❌ Not enough data for window={window} | {safe} has {len(df_norm)} rows\")\n",
    "        return None, None, None\n",
    "\n",
    "    obs_window = df_norm.tail(window).values.astype(np.float32)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 6) Append embedding at each timestep (optional architecture)\n",
    "    # -----------------------------------------------------------\n",
    "    embedding_repeated = np.tile(embedding, (window, 1))\n",
    "    obs_window = np.concatenate([obs_window, embedding_repeated], axis=1)\n",
    "\n",
    "    return obs_window, scaler_info, embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebe98a-dd7f-4cb0-a7f2-be9d4f364361",
   "metadata": {},
   "source": [
    "# ==================================================\n",
    "# Cell 9 — \n",
    "# ==================================================\n",
    "def fetch_and_build_obs(symbol, window=WINDOW, scalers=None, embeddings=None, safe_names=None):\n",
    "    \"\"\"\n",
    "    Returns: obs (shape (window, feat_dim)), vol_est (std of c_pc), last_price\n",
    "    obs columns: o_pc,h_pc,l_pc,c_pc,v_pc, balance_col omitted (live uses balance separately), embedding appended as columns\n",
    "    \"\"\"\n",
    "    # fetch extra slack bars\n",
    "    count = window + 10\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, count)\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        return None, None, None\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    df = df[['open','high','low','close','tick_volume']].rename(columns={'tick_volume':'volume'})\n",
    "    pct = pd.DataFrame(index=df.index)\n",
    "    pct['o_pc'] = df['open'].pct_change()\n",
    "    pct['h_pc'] = df['high'].pct_change()\n",
    "    pct['l_pc'] = df['low'].pct_change()\n",
    "    pct['c_pc'] = df['close'].pct_change()\n",
    "    pct['v_pc'] = df['volume'].pct_change()\n",
    "    pct.dropna(inplace=True)\n",
    "    if len(pct) < window:\n",
    "        return None, None, None\n",
    "    recent = pct.iloc[-window:]\n",
    "    # normalization using scaler if available\n",
    "    safe = symbol.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\",\"\")\n",
    "    if scalers and safe in scalers and scalers[safe] is not None:\n",
    "        s = scalers[safe]\n",
    "        mean = s['mean']\n",
    "        std = s['std'].replace(0,1.0)\n",
    "        # ensure correct order of columns\n",
    "        mean = mean.reindex(recent.columns).fillna(0.0)\n",
    "        std = std.reindex(recent.columns).replace(0,1.0).fillna(1.0)\n",
    "        norm = (recent - mean) / std\n",
    "    else:\n",
    "        mean = recent.mean()\n",
    "        std = recent.std().replace(0,1.0)\n",
    "        norm = (recent - mean) / std\n",
    "    # features\n",
    "    feats = norm[['o_pc','h_pc','l_pc','c_pc','v_pc']].values.astype(np.float32)\n",
    "    vol_est = float(norm['c_pc'].std())\n",
    "    last_price = float(df['close'].iloc[-1])\n",
    "    # embeddings: find index for symbol in safe_names (if provided)\n",
    "    if embeddings is not None and safe_names is not None and safe in safe_names:\n",
    "        idx = safe_names.index(safe)\n",
    "        emb = np.tile(embeddings[idx].reshape(1,-1).astype(np.float32),(window,1))\n",
    "        obs = np.concatenate([feats, emb], axis=1)\n",
    "    else:\n",
    "        obs = feats\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bc189-93c2-4e74-ab34-e9c6c8c64552",
   "metadata": {},
   "source": [
    "# ==================================================\n",
    "# Cell 10 — \n",
    "# ==================================================\n",
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names):\n",
    "    safe = symbol.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "    if safe not in scalers or safe not in embeddings:\n",
    "        return None, None, None\n",
    "\n",
    "    scaler = scalers[safe]\n",
    "    embed_vec = embeddings[safe]      # shape MUST match training (e.g., 7 dims)\n",
    "\n",
    "    # fetch extra slack bars\n",
    "    count = window + 10\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, count)\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        return None, None, None\n",
    "\n",
    "    print(\"bars: \",bars)\n",
    "    \n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    df = df[['open','high','low','close','tick_volume']].rename(columns={'tick_volume':'volume'})\n",
    "    pct = pd.DataFrame(index=df.index)\n",
    "    pct['o_pc'] = df['open'].pct_change()\n",
    "    pct['h_pc'] = df['high'].pct_change()\n",
    "    pct['l_pc'] = df['low'].pct_change()\n",
    "    pct['c_pc'] = df['close'].pct_change()\n",
    "    pct['v_pc'] = df['volume'].pct_change()\n",
    "    pct.dropna(inplace=True)\n",
    "    \n",
    "    #df = fetch_recent_raw(symbol, window + 1)   # you must have this helper\n",
    "    if df is None or len(df) < window + 1:\n",
    "        return None, None, None\n",
    "\n",
    "    pct = df.pct_change().dropna()\n",
    "    pct = pct.tail(window)\n",
    "\n",
    "    # normalize with per-asset scaler\n",
    "    pct_norm = (pct - scaler[\"mean\"]) / scaler[\"std\"]\n",
    "\n",
    "    # last raw close (for entry price)\n",
    "    last_price = df[\"close\"].iloc[-1]\n",
    "\n",
    "    # volatility estimate (std of returns)\n",
    "    vol_est = pct[\"close\"].std()\n",
    "\n",
    "    # balance normalized like training (fixed 10000 for live)\n",
    "    balance_norm = np.full((window, 1), 1.0)\n",
    "\n",
    "    # asset_id normalized\n",
    "    asset_id_norm = safe_names.index(safe) / len(safe_names)\n",
    "    asset_id_norm = np.full((window, 1), asset_id_norm)\n",
    "\n",
    "    # repeat embedding per row\n",
    "    emb = np.tile(embed_vec, (window, 1))\n",
    "\n",
    "    # BUILD FINAL OBSERVATION — EXACT MATCH WITH TRAINING SHAPE\n",
    "    obs = np.column_stack([\n",
    "        pct_norm[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].values,\n",
    "        emb,\n",
    "        balance_norm,\n",
    "        asset_id_norm\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba0f2d35-a5b1-4179-926a-a91afc837078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 11 — \n",
    "# ==================================================\n",
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names):# The correct one\n",
    "    safe = symbol.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "    # Check if preprocessing objects exist\n",
    "    if safe not in scalers:\n",
    "        print(f\"❌ Missing scaler for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if safe not in embeddings:\n",
    "        print(f\"❌ Missing embedding for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "    scaler = scalers[safe]\n",
    "    embed_vec = embeddings[safe]  # Expecting shape (embedding_dim,)\n",
    "\n",
    "    # Fetch raw bars\n",
    "    count = window + 10   # fetch surplus for pct-change\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, count)\n",
    "\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        return None, None, None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    df = df[['open','high','low','close','tick_volume']].rename(columns={'tick_volume':'volume'})\n",
    "\n",
    "    # Ensure enough rows\n",
    "    if len(df) < window + 1:\n",
    "        return None, None, None\n",
    "\n",
    "    # Compute pct changes\n",
    "    pct = df.pct_change().dropna()\n",
    "    pct = pct.tail(window)\n",
    "\n",
    "    if pct.shape[0] < window:\n",
    "        return None, None, None\n",
    "\n",
    "    # Normalize using the saved scaler\n",
    "    pct_norm = (pct - scaler[\"mean\"]) / scaler[\"std\"]\n",
    "\n",
    "    # Extract last price\n",
    "    last_price = df[\"close\"].iloc[-1]\n",
    "\n",
    "    # Volatility estimate\n",
    "    vol_est = pct[\"close\"].std()\n",
    "\n",
    "    # balance normalized (fixed 1.0 for live)\n",
    "    balance_norm = np.full((window, 1), 1.0, dtype=np.float32)\n",
    "\n",
    "    # asset-id normalized\n",
    "    asset_id_norm_val = safe_names.index(safe) / len(safe_names)\n",
    "    asset_id_norm = np.full((window, 1), asset_id_norm_val, dtype=np.float32)\n",
    "\n",
    "    # repeat embedding per timestep\n",
    "    emb = np.tile(embed_vec, (window, 1)).astype(np.float32)\n",
    "\n",
    "    # final obs = [normalized OHLCV + embedding + balance + asset_id]\n",
    "    obs = np.column_stack([\n",
    "        pct_norm[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values.astype(np.float32),\n",
    "        emb,\n",
    "        balance_norm,\n",
    "        asset_id_norm\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    return obs, float(vol_est), float(last_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6f5b345-a5d0-42a1-a410-4a7a52c0bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 12 — \n",
    "# ==================================================\n",
    "def make_safe(symbol: str) -> str:\n",
    "    return (\n",
    "        symbol.replace(\" \", \"_\")\n",
    "              .replace(\"/\", \"_\")\n",
    "              .replace(\"(\", \"\")\n",
    "              .replace(\")\", \"\")\n",
    "              .replace(\".\", \"_\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63d38441-4b46-4d74-ba61-17fefed0d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names):\n",
    "    # -------------------------------------------------------\n",
    "    # 1) Always generate safe-name the SAME way\n",
    "    # -------------------------------------------------------\n",
    "    safe = make_safe(symbol)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) Verify preprocessing data\n",
    "    # -------------------------------------------------------\n",
    "    if safe not in scalers:\n",
    "        print(f\"❌ Missing scaler for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if safe not in embeddings:\n",
    "        print(f\"❌ Missing embedding for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    scaler = scalers[safe]\n",
    "    embed_vec = embeddings[safe]\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3) MT5 price fetch\n",
    "    # -------------------------------------------------------\n",
    "    count = window + 10\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, count)\n",
    "\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        print(f\"❌ Insufficient data for {symbol}\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    df = df[['open', 'high', 'low', 'close', 'tick_volume']].rename(columns={'tick_volume': 'volume'})\n",
    "\n",
    "    if len(df) < window + 1:\n",
    "        return None, None, None\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 4) Percent change window\n",
    "    # -------------------------------------------------------\n",
    "    pct = df.pct_change().dropna().tail(window)\n",
    "\n",
    "    if pct.shape[0] < window:\n",
    "        return None, None, None\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 5) Normalize using the saved scaler\n",
    "    # -------------------------------------------------------\n",
    "    pct_norm = (pct - scaler[\"mean\"]) / scaler[\"std\"]\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 6) Build obs components\n",
    "    # -------------------------------------------------------\n",
    "    last_price = df[\"close\"].iloc[-1]\n",
    "    vol_est = pct[\"close\"].std()\n",
    "\n",
    "    balance_norm = np.full((window, 1), 1.0, dtype=np.float32)\n",
    "\n",
    "    # ---- safe_names MUST be a dict: {raw_symbol: safe} ----\n",
    "    safe_list = list(safe_names.values())\n",
    "    asset_id_norm_val = safe_list.index(safe) / len(safe_list)\n",
    "    asset_id_norm = np.full((window, 1), asset_id_norm_val, dtype=np.float32)\n",
    "\n",
    "    emb = np.tile(embed_vec, (window, 1)).astype(np.float32)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 7) Final Observation\n",
    "    # -------------------------------------------------------\n",
    "    obs = np.column_stack([\n",
    "        pct_norm[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].values.astype(np.float32),\n",
    "        emb,\n",
    "        balance_norm,\n",
    "        asset_id_norm\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    return obs, float(vol_est), float(last_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e468fe04-62d1-40e4-82f9-e5f8f2d3d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Lot-size helpers (MT5-aware)\n",
    "def pip_value_per_lot_from_mt5_live(symbol, entry_price):\n",
    "    info = None\n",
    "    try:\n",
    "        info = mt5.symbol_info(symbol)\n",
    "    except Exception:\n",
    "        info = None\n",
    "    if info is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        pip_val = info.trade_tick_value / info.trade_tick_size\n",
    "    except Exception:\n",
    "        contract_size = getattr(info, \"trade_contract_size\", 100000.0)\n",
    "        pip_val = (info.point / entry_price) * contract_size\n",
    "    return float(pip_val), float(info.point)\n",
    "\n",
    "def calculate_lot_size_live(symbol, balance, risk_percent, entry_price, stop_loss_price):\n",
    "    pip_val, point = pip_value_per_lot_from_mt5_live(symbol, entry_price)\n",
    "    if point and point not in (0,None):\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / point\n",
    "    else:\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / (0.01 if \"JPY\" in symbol else 0.0001)\n",
    "    if pip_risk <= 0:\n",
    "        return MIN_LOT\n",
    "    dollar_risk = balance * float(risk_percent)\n",
    "    if pip_val is not None:\n",
    "        lot = dollar_risk / (pip_risk * pip_val)\n",
    "        info = mt5.symbol_info(symbol)\n",
    "        if info is not None:\n",
    "            try:\n",
    "                step = float(info.volume_step)\n",
    "                if step>0:\n",
    "                    lot = round(lot/step)*step\n",
    "            except Exception:\n",
    "                pass\n",
    "        lot = max(MIN_LOT, min(MAX_LOT, round(lot,2)))\n",
    "        return lot\n",
    "    # fallback\n",
    "    pip_size = 0.01 if \"JPY\" in symbol else 0.0001\n",
    "    pip_value_per_lot = (pip_size / entry_price) * 100000.0\n",
    "    lot = dollar_risk / (pip_risk * pip_value_per_lot)\n",
    "    lot = max(MIN_LOT, min(MAX_LOT, round(lot,2)))\n",
    "    return lot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f3fad29-aa10-4421-acc7-9dfbd25bc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Place order wrapper (dry_run default True)\n",
    "def place_order_on_mt5(symbol, direction, lot, sl, tp=None, dry_run=DRY_RUN_DEFAULT, deviation=DEVIATION, magic=MAGIC, comment=\"rl_live\"):\n",
    "    \"\"\"\n",
    "    direction: \"BUY\" or \"SELL\"\n",
    "    Returns MT5 response or a dict if dry_run.\n",
    "    \"\"\"\n",
    "    if dry_run:\n",
    "        req = {\n",
    "            \"action\": \"DEAL (dry_run)\",\n",
    "            \"symbol\": symbol,\n",
    "            \"volume\": lot,\n",
    "            \"type\": direction,\n",
    "            \"price\": None,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"deviation\": deviation,\n",
    "            \"magic\": magic,\n",
    "            \"comment\": comment\n",
    "        }\n",
    "        print(\"DRY RUN order:\", req)\n",
    "        return {\"dry_run\": True, \"request\": req}\n",
    "\n",
    "    if not mt5_init_if_needed():\n",
    "        raise RuntimeError(\"MT5 not initialized. Open terminal and login.\")\n",
    "    if not ensure_symbol_published(symbol):\n",
    "        raise RuntimeError(f\"Symbol not available in MT5: {symbol}\")\n",
    "\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        raise RuntimeError(\"No tick info for symbol: \"+symbol)\n",
    "\n",
    "    price = tick.ask if direction==\"BUY\" else tick.bid\n",
    "    order_type = mt5.ORDER_TYPE_BUY if direction==\"BUY\" else mt5.ORDER_TYPE_SELL\n",
    "\n",
    "    request = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": order_type,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl),\n",
    "        \"tp\": float(tp) if tp is not None else 0.0,\n",
    "        \"deviation\": int(deviation),\n",
    "        \"magic\": int(magic),\n",
    "        \"comment\": comment,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(request)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0279704-7e82-453c-9ada-deb0392cb260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: models\\multiasset\\ppo_multiasset.zip\n",
      "Assets: ['EURUSD', 'Jump_100_Index', 'Jump_10_Index', 'Jump_25_Index', 'Jump_50_Index', 'Jump_75_Index', 'Volatility_100_1s_Index', 'Volatility_100_Index', 'Volatility_10_1s_Index', 'Volatility_10_Index', 'Volatility_25_1s_Index', 'Volatility_25_Index', 'Volatility_50_1s_Index', 'Volatility_50_Index', 'Volatility_75_1s_Index', 'Volatility_75_Index']\n",
      "Embeddings shape: (16, 8)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Load trained model and environment assets\n",
    "# Model\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Trained model not found at: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded model:\", MODEL_FILE)\n",
    "\n",
    "# Assets, embeddings, scalers\n",
    "safe_names, embeddings, scalers = load_assets_and_embeddings(DATA_DIR, EMBED_FILE)\n",
    "print(\"Assets:\", safe_names)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "612a53c4-e8bd-49a7-aaa9-c39aada176fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EURUSD ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m records\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Example single-run (dry-run): change dry_run=False to execute live (be cautious!)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m records = run_once_live(dry_run=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_once_live\u001b[39m\u001b[34m(dry_run, risk_per_trade)\u001b[39m\n\u001b[32m     13\u001b[39m symbol = sym_safe\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[33m\"\u001b[39m, symbol, \u001b[33m\"\u001b[39m\u001b[33m---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m obs, vol_est, last_price = fetch_and_build_obs(symbol, window=WINDOW, scalers=scalers, embeddings=embeddings, safe_names=safe_names)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInsufficient data for\u001b[39m\u001b[33m\"\u001b[39m, symbol, \u001b[33m\"\u001b[39m\u001b[33m- skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mfetch_and_build_obs\u001b[39m\u001b[34m(symbol, window, scalers, embeddings, safe_names, fallback_embedding_dim)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❌ No safe-name found for symbol: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m safe = safe_names[symbol]\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 2) Check required preprocessing objects\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m scalers:\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Single-pass prediction and optional execution across all assets.\n",
    "# This performs one sweep and returns a log of attempted/executed orders.\n",
    "\n",
    "def run_once_live(dry_run=DRY_RUN_DEFAULT, risk_per_trade=RISK_PER_TRADE):\n",
    "    if not mt5_init_if_needed():\n",
    "        print(\"⚠️ Warning: MT5 initialization failed or MT5 not running. Running in offline/dry mode.\")\n",
    "    acct = mt5.account_info() if mt5_init_if_needed() else None\n",
    "    balance = float(acct.balance) if acct is not None else 10000.0\n",
    "\n",
    "    records = []\n",
    "    for sym_safe in safe_names:\n",
    "        # Try to reconstruct broker symbol name: assume safe==symbol; if your safe names differ, update asset_to_symbol mapping\n",
    "        symbol = sym_safe\n",
    "        print(\"\\n---\", symbol, \"---\")\n",
    "        obs, vol_est, last_price = fetch_and_build_obs(symbol, window=WINDOW, scalers=scalers, embeddings=embeddings, safe_names=safe_names)\n",
    "        if obs is None:\n",
    "            print(\"Insufficient data for\", symbol, \"- skipping.\")\n",
    "            continue\n",
    "\n",
    "        # model.predict expects shape like env obs. Ensure dims align:\n",
    "        try:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        except Exception as e:\n",
    "            # try flatten\n",
    "            try:\n",
    "                action, _ = model.predict(obs.flatten(), deterministic=True)\n",
    "            except Exception as e2:\n",
    "                print(\"Model predict failed for\", symbol, \":\", e, e2)\n",
    "                continue\n",
    "\n",
    "        act = extract_action_scalar(action)\n",
    "        if act == 0:\n",
    "            print(symbol, \"→ HOLD\")\n",
    "            records.append({\"symbol\":symbol, \"action\":\"HOLD\"})\n",
    "            continue\n",
    "\n",
    "        direction = \"BUY\" if act==1 else \"SELL\"\n",
    "        # compute stoploss/TP heuristics (based on vol_est)\n",
    "        if vol_est is None or vol_est<=0:\n",
    "            sl_dist = 0.001 * last_price\n",
    "        else:\n",
    "            sl_dist = max(1.5 * vol_est * last_price, 0.0005 * last_price)\n",
    "        sl = last_price - sl_dist if direction==\"BUY\" else last_price + sl_dist\n",
    "        tp = last_price + (2.5 * vol_est * last_price) if direction==\"BUY\" else last_price - (2.5 * vol_est * last_price)\n",
    "\n",
    "        lot = calculate_lot_size_live(symbol, balance, risk_per_trade, last_price, sl)\n",
    "        lot = max(MIN_LOT, min(MAX_LOT, round(lot, 2)))\n",
    "\n",
    "        # place order\n",
    "        res = place_order_on_mt5(symbol, direction, lot, sl, tp, dry_run=dry_run)\n",
    "        rec = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"symbol\": symbol,\n",
    "            \"action\": direction,\n",
    "            \"lot\": lot,\n",
    "            \"price\": last_price,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"result\": str(getattr(res, \"retcode\", res))\n",
    "        }\n",
    "        records.append(rec)\n",
    "        print(\"Planned trade:\", rec)\n",
    "    # save records\n",
    "    out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    pd.DataFrame(records).to_csv(out_csv, index=False)\n",
    "    print(\"Saved run log to\", out_csv)\n",
    "    return records\n",
    "\n",
    "# Example single-run (dry-run): change dry_run=False to execute live (be cautious!)\n",
    "records = run_once_live(dry_run=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b31f61-85cc-441d-8330-85270dd70dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting continuous live loop — dry_run = True\n",
      "\n",
      "=== Iteration 1 @ 2025-11-16T08:26:34.948204 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_082634.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 2 @ 2025-11-16T08:27:34.976006 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_082749.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 3 @ 2025-11-16T08:28:49.037744 ===\n",
      "⚠️ Warning: MT5 initialization failed or MT5 not running. Running in offline/dry mode.\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083244.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 4 @ 2025-11-16T08:33:44.028995 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083344.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 5 @ 2025-11-16T08:34:44.058892 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083444.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 6 @ 2025-11-16T08:35:44.084677 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083544.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 7 @ 2025-11-16T08:36:44.108793 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083644.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 8 @ 2025-11-16T08:37:44.139535 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083744.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 9 @ 2025-11-16T08:38:44.167804 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083844.csv\n",
      "\n",
      "=== Iteration 10 @ 2025-11-16T08:39:44.194836 ===\n",
      "\n",
      "--- EURUSD ---\n",
      "Insufficient data for EURUSD - skipping.\n",
      "\n",
      "--- Jump_100_Index ---\n",
      "Insufficient data for Jump_100_Index - skipping.\n",
      "\n",
      "--- Jump_10_Index ---\n",
      "Insufficient data for Jump_10_Index - skipping.\n",
      "\n",
      "--- Jump_25_Index ---\n",
      "Insufficient data for Jump_25_Index - skipping.\n",
      "\n",
      "--- Jump_50_Index ---\n",
      "Insufficient data for Jump_50_Index - skipping.\n",
      "\n",
      "--- Jump_75_Index ---\n",
      "Insufficient data for Jump_75_Index - skipping.\n",
      "\n",
      "--- Volatility_100_1s_Index ---\n",
      "Insufficient data for Volatility_100_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_100_Index ---\n",
      "Insufficient data for Volatility_100_Index - skipping.\n",
      "\n",
      "--- Volatility_10_1s_Index ---\n",
      "Insufficient data for Volatility_10_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_10_Index ---\n",
      "Insufficient data for Volatility_10_Index - skipping.\n",
      "\n",
      "--- Volatility_25_1s_Index ---\n",
      "Insufficient data for Volatility_25_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_25_Index ---\n",
      "Insufficient data for Volatility_25_Index - skipping.\n",
      "\n",
      "--- Volatility_50_1s_Index ---\n",
      "Insufficient data for Volatility_50_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_50_Index ---\n",
      "Insufficient data for Volatility_50_Index - skipping.\n",
      "\n",
      "--- Volatility_75_1s_Index ---\n",
      "Insufficient data for Volatility_75_1s_Index - skipping.\n",
      "\n",
      "--- Volatility_75_Index ---\n",
      "Insufficient data for Volatility_75_Index - skipping.\n",
      "Saved run log to models\\multiasset\\live_logs\\live_run_20251116_083944.csv\n",
      "Max iterations reached; stopping.\n",
      "MT5 connection closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3984932639.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11856\\3652491068.py:64: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 — Continuous loop for live trading (uncomment to enable). KEEP dry_run=True until fully tested.\n",
    "# WARNING: Live trading will place real orders if dry_run is False. Test on demo account first.\n",
    "\n",
    "def run_continuous(interval_s=60, dry_run=DRY_RUN_DEFAULT, risk_per_trade=RISK_PER_TRADE, max_iterations=None):\n",
    "    print(\"Starting continuous live loop — dry_run =\", dry_run)\n",
    "    iteration = 0\n",
    "    try:\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
    "            run_once_live(dry_run=dry_run, risk_per_trade=risk_per_trade)\n",
    "            if max_iterations is not None and iteration >= max_iterations:\n",
    "                print(\"Max iterations reached; stopping.\")\n",
    "                break\n",
    "            time.sleep(interval_s)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped by user (KeyboardInterrupt).\")\n",
    "    except Exception as e:\n",
    "        print(\"Loop error:\", e)\n",
    "    finally:\n",
    "        mt5_shutdown_if_needed()\n",
    "        print(\"MT5 connection closed.\")\n",
    "\n",
    "# Example usage (commented): run_continuous(interval_s=60, dry_run=True, max_iterations=10)\n",
    "run_continuous(interval_s=60, dry_run=True, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb5bfbd-dafe-4253-95ed-d5a19aa798f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 — Visualize logs and compute simple metrics\n",
    "def summarize_run_log(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total_trades = len(df[df['action']!=\"HOLD\"])\n",
    "    if total_trades==0:\n",
    "        print(\"No trades in log.\")\n",
    "    print(\"Run summary:\", csv_path)\n",
    "    print(\"Total rows:\", len(df), \"Trades executed/planned:\", total_trades)\n",
    "    return df\n",
    "\n",
    "def plot_price_with_signals(symbol, run_records_csv):\n",
    "    df_log = pd.read_csv(run_records_csv)\n",
    "    df_sym = df_log[df_log['symbol']==symbol]\n",
    "    if df_sym.empty:\n",
    "        print(\"No records for\", symbol)\n",
    "        return\n",
    "    # fetch historical price for plotting (recent window)\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, WINDOW*5)\n",
    "    if bars is None:\n",
    "        print(\"No bars to plot for\", symbol)\n",
    "        return\n",
    "    d = pd.DataFrame(bars)\n",
    "    d['time'] = pd.to_datetime(d['time'], unit='s')\n",
    "    d.set_index('time', inplace=True)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(d['close'], label='close')\n",
    "    for _, row in df_sym.iterrows():\n",
    "        # mark entry price\n",
    "        plt.axhline(row['price'], linestyle='--', alpha=0.6)\n",
    "    plt.title(f\"{symbol} recent closes and signals\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: df = summarize_run_log(\"models/multiasset/live_logs/live_run_YYYY...csv\")\n",
    "# plot_price_with_signals(\"Volatility_75_Index\", \"path-to-log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f8718f-ed4a-4c8e-917f-179b8a4293c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run a single dry-run pass: records = run_once_live(dry_run=True)\n",
      "To run continuous (demo): run_continuous(interval_s=60, dry_run=True, max_iterations=10)\n",
      "To execute live for real orders set dry_run=False in run_once_live() or run_continuous(). DO NOT DO THIS UNTIL FULLY TESTED ON DEMO ACCOUNT.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 — Safe shutdown and final notes\n",
    "print(\"To run a single dry-run pass: records = run_once_live(dry_run=True)\")\n",
    "print(\"To run continuous (demo): run_continuous(interval_s=60, dry_run=True, max_iterations=10)\")\n",
    "print(\"To execute live for real orders set dry_run=False in run_once_live() or run_continuous(). DO NOT DO THIS UNTIL FULLY TESTED ON DEMO ACCOUNT.\")\n",
    "# Do not auto-init MT5 here — user will call run_* functions to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a66394-b9e9-419f-b11f-84e2e6b25a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Cell 1 — Install dependencies (run once if needed)\n",
    "# ==================================================\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install --quiet numpy pandas matplotlib seaborn MetaTrader5 stable-baselines3 gymnasium==0.29.1 torch ffmpeg-python pillow\n",
    "\n",
    "# ==================================================\n",
    "# Cell 2 — Imports & configuration\n",
    "# ==================================================\n",
    "import os, glob, json, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import MetaTrader5 as mt5\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# ==================================================\n",
    "# Cell 3 — Paths (adjust if different)\n",
    "# ==================================================\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "SCALER_GLOB = os.path.join(DATA_DIR, \"*_scaler.csv\")  # per-symbol scalers\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# ==================================================\n",
    "# Cell 4 — Live configuration\n",
    "# ==================================================\n",
    "TIMEFRAME = \"M15\"        # timeframe to request from MT5\n",
    "WINDOW = 50\n",
    "RISK_PER_TRADE = 0.02   # default risk per trade (1%)\n",
    "MIN_LOT = 0.01\n",
    "MAX_LOT = 10.0\n",
    "DEVIATION = 20\n",
    "MAGIC = 2025001\n",
    "DRY_RUN_DEFAULT = False  # safety default\n",
    "\n",
    "# ==================================================\n",
    "# Cell 5 — Helpers: safe action extraction, load datasets, scalers, embeddings\n",
    "# ==================================================\n",
    "def extract_action_scalar(action):\n",
    "    import numpy as _np\n",
    "    if isinstance(action, (int, _np.integer)):\n",
    "        return int(action)\n",
    "    a = _np.array(action)\n",
    "    if a.size == 1:\n",
    "        return int(a.flatten()[0])\n",
    "    return int(a.flatten()[0])\n",
    "\n",
    "def load_scalers(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in glob.glob(os.path.join(data_dir, \"*_scaler.csv\")):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        try:\n",
    "            df = pd.read_csv(p, index_col=0)\n",
    "            scalers[safe] = {\"mean\": df['mean'].astype(float), \"std\": df['std'].astype(float)}\n",
    "        except Exception:\n",
    "            scalers[safe] = None\n",
    "    return scalers\n",
    "\n",
    "def load_assets_and_embeddings(data_dir=DATA_DIR, embed_file=EMBED_FILE):\n",
    "    # load asset order from asset_to_idx.csv if present\n",
    "    asset_map = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        try:\n",
    "            s = pd.read_csv(ASSET_MAP_FILE, index_col=0, squeeze=True)\n",
    "            asset_map = (s.to_dict() if hasattr(s, \"to_dict\") else dict(s))\n",
    "            # s might be safe->idx or vice-versa; normalize\n",
    "            # produce ordered list by idx\n",
    "            try:\n",
    "                ordered = sorted(asset_map.items(), key=lambda x:int(x[1]))\n",
    "                safes = [k for k,_ in ordered]\n",
    "            except Exception:\n",
    "                safes = list(asset_map.keys())\n",
    "        except Exception:\n",
    "            safes = []\n",
    "    else:\n",
    "        # fallback: read normalized csv names\n",
    "        safes = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in sorted(glob.glob(os.path.join(DATA_DIR,\"*_normalized.csv\")))]\n",
    "    embeddings = np.load(embed_file) if os.path.exists(embed_file) else np.zeros((len(safes), 1), dtype=np.float32)\n",
    "    # build canonical list\n",
    "    safes = safes if len(safes)>0 else list({os.path.basename(p).replace(\"_normalized.csv\",\"\"):None for p in glob.glob(os.path.join(DATA_DIR,\"*_normalized.csv\"))}.keys())\n",
    "    return safes, embeddings, load_scalers(data_dir)\n",
    "\n",
    "# ==================================================\n",
    "# Cell 6 — MT5 init helpers (safe)\n",
    "# ==================================================\n",
    "def mt5_init_if_needed():\n",
    "    try:\n",
    "        if not mt5.initialize():\n",
    "            # sometimes initialize() returns False but terminal still ok; give another try\n",
    "            return mt5.initialize()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def mt5_shutdown_if_needed():\n",
    "    try:\n",
    "        mt5.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# Cell 7 — symbol_published (symbol)\n",
    "# ==================================================\n",
    "def ensure_symbol_published(symbol):\n",
    "    \"\"\"Ensure symbol is visible in Market Watch; attempt to add if not.\"\"\"\n",
    "    try:\n",
    "        sinfo = mt5.symbol_info(symbol)\n",
    "        if sinfo is None:\n",
    "            # try to enable the symbol\n",
    "            mt5.symbol_select(symbol, True)\n",
    "            sinfo = mt5.symbol_info(symbol)\n",
    "        return sinfo is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ==================================================\n",
    "# Cell 8 — Time frame mapping\n",
    "# ==================================================\n",
    "# Cell 5 — Build observation for a symbol in the same format used in training\n",
    "def timeframe_to_mt5(tf_str):\n",
    "    TF_MAP = {\n",
    "        \"M1\": mt5.TIMEFRAME_M1, \"M5\": mt5.TIMEFRAME_M5, \"M15\": mt5.TIMEFRAME_M15,\n",
    "        \"M30\": mt5.TIMEFRAME_M30, \"H1\": mt5.TIMEFRAME_H1, \"H4\": mt5.TIMEFRAME_H4,\n",
    "        \"D1\": mt5.TIMEFRAME_D1\n",
    "    }\n",
    "    return TF_MAP.get(tf_str.upper(), mt5.TIMEFRAME_M15)\n",
    "\n",
    "TF_MT5 = timeframe_to_mt5(TIMEFRAME)\n",
    "\n",
    "# ==================================================\n",
    "# Cell 8 — \n",
    "# ==================================================\n",
    "def fetch_and_build_obs(symbol, window, scalers, embeddings, safe_names, fallback_embedding_dim=None):\n",
    "    \"\"\"\n",
    "    Build observation window for a symbol using:\n",
    "    - normalized percent-change features\n",
    "    - saved scaler (mean/std)\n",
    "    - saved embedding vector\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): raw symbol, e.g. \"EURUSD\"\n",
    "        window (int): sliding window length\n",
    "        scalers (dict): {safe: {\"mean\": Series, \"std\": Series}}\n",
    "        embeddings (dict): {safe: np.ndarray}\n",
    "        safe_names (dict): {raw_symbol: safe_symbol}\n",
    "        fallback_embedding_dim (int or None): optional zero-vector if embedding missing\n",
    "\n",
    "    Returns:\n",
    "        obs_window (np.ndarray)\n",
    "        scaler_info (dict)\n",
    "        embedding (np.ndarray)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Convert symbol → safe-name\n",
    "    # -----------------------------------------------------------\n",
    "    if symbol not in safe_names:\n",
    "        print(f\"❌ No safe-name found for symbol: {symbol}\")\n",
    "        return None, None, None\n",
    "\n",
    "    safe = safe_names[symbol]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Check required preprocessing objects\n",
    "    # -----------------------------------------------------------\n",
    "    if safe not in scalers:\n",
    "        print(f\"❌ Missing scaler for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if safe not in embeddings:\n",
    "        if fallback_embedding_dim is None:\n",
    "            print(f\"❌ Missing embedding for: {safe}\")\n",
    "            return None, None, None\n",
    "        else:\n",
    "            print(f\"⚠️ Missing embedding → using zero vector for {safe}\")\n",
    "            embedding = np.zeros(fallback_embedding_dim)\n",
    "    else:\n",
    "        embedding = embeddings[safe]\n",
    "\n",
    "    scaler_info = scalers[safe]\n",
    "\n",
    "    # Extract mean / std\n",
    "    mean = scaler_info[\"mean\"]\n",
    "    std = scaler_info[\"std\"].replace(0, 1.0)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Load raw normalized dataset for this symbol\n",
    "    # -----------------------------------------------------------\n",
    "    if safe not in datasets:\n",
    "        print(f\"❌ No dataset loaded for: {safe}\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = datasets[safe].copy()\n",
    "\n",
    "    # Must contain the required columns\n",
    "    required_columns = [\"o_pc\", \"h_pc\", \"l_pc\", \"c_pc\", \"v_pc\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"❌ Missing required column '{col}' in dataset for {safe}\")\n",
    "            return None, None, None\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) Apply normalization using stored scaler\n",
    "    # -----------------------------------------------------------\n",
    "    df_norm = (df[required_columns] - mean[required_columns]) / std[required_columns]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 5) Extract last window\n",
    "    # -----------------------------------------------------------\n",
    "    if len(df_norm) < window:\n",
    "        print(f\"❌ Not enough data for window={window} | {safe} has {len(df_norm)} rows\")\n",
    "        return None, None, None\n",
    "\n",
    "    obs_window = df_norm.tail(window).values.astype(np.float32)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 6) Append embedding at each timestep (optional architecture)\n",
    "    # -----------------------------------------------------------\n",
    "    embedding_repeated = np.tile(embedding, (window, 1))\n",
    "    obs_window = np.concatenate([obs_window, embedding_repeated], axis=1)\n",
    "\n",
    "    return obs_window, scaler_info, embedding\n",
    "\n",
    "# Cell 6 — Lot-size helpers (MT5-aware)\n",
    "def pip_value_per_lot_from_mt5_live(symbol, entry_price):\n",
    "    info = None\n",
    "    try:\n",
    "        info = mt5.symbol_info(symbol)\n",
    "    except Exception:\n",
    "        info = None\n",
    "    if info is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        pip_val = info.trade_tick_value / info.trade_tick_size\n",
    "    except Exception:\n",
    "        contract_size = getattr(info, \"trade_contract_size\", 100000.0)\n",
    "        pip_val = (info.point / entry_price) * contract_size\n",
    "    return float(pip_val), float(info.point)\n",
    "\n",
    "def calculate_lot_size_live(symbol, balance, risk_percent, entry_price, stop_loss_price):\n",
    "    pip_val, point = pip_value_per_lot_from_mt5_live(symbol, entry_price)\n",
    "    if point and point not in (0,None):\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / point\n",
    "    else:\n",
    "        pip_risk = abs(entry_price - stop_loss_price) / (0.01 if \"JPY\" in symbol else 0.0001)\n",
    "    if pip_risk <= 0:\n",
    "        return MIN_LOT\n",
    "    dollar_risk = balance * float(risk_percent)\n",
    "    if pip_val is not None:\n",
    "        lot = dollar_risk / (pip_risk * pip_val)\n",
    "        info = mt5.symbol_info(symbol)\n",
    "        if info is not None:\n",
    "            try:\n",
    "                step = float(info.volume_step)\n",
    "                if step>0:\n",
    "                    lot = round(lot/step)*step\n",
    "            except Exception:\n",
    "                pass\n",
    "        lot = max(MIN_LOT, min(MAX_LOT, round(lot,2)))\n",
    "        return lot\n",
    "    # fallback\n",
    "    pip_size = 0.01 if \"JPY\" in symbol else 0.0001\n",
    "    pip_value_per_lot = (pip_size / entry_price) * 100000.0\n",
    "    lot = dollar_risk / (pip_risk * pip_value_per_lot)\n",
    "    lot = max(MIN_LOT, min(MAX_LOT, round(lot,2)))\n",
    "    return lot\n",
    "\n",
    "# Cell 7 — Place order wrapper (dry_run default True)\n",
    "def place_order_on_mt5(symbol, direction, lot, sl, tp=None, dry_run=DRY_RUN_DEFAULT, deviation=DEVIATION, magic=MAGIC, comment=\"rl_live\"):\n",
    "    \"\"\"\n",
    "    direction: \"BUY\" or \"SELL\"\n",
    "    Returns MT5 response or a dict if dry_run.\n",
    "    \"\"\"\n",
    "    if dry_run:\n",
    "        req = {\n",
    "            \"action\": \"DEAL (dry_run)\",\n",
    "            \"symbol\": symbol,\n",
    "            \"volume\": lot,\n",
    "            \"type\": direction,\n",
    "            \"price\": None,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"deviation\": deviation,\n",
    "            \"magic\": magic,\n",
    "            \"comment\": comment\n",
    "        }\n",
    "        print(\"DRY RUN order:\", req)\n",
    "        return {\"dry_run\": True, \"request\": req}\n",
    "\n",
    "    if not mt5_init_if_needed():\n",
    "        raise RuntimeError(\"MT5 not initialized. Open terminal and login.\")\n",
    "    if not ensure_symbol_published(symbol):\n",
    "        raise RuntimeError(f\"Symbol not available in MT5: {symbol}\")\n",
    "\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        raise RuntimeError(\"No tick info for symbol: \"+symbol)\n",
    "\n",
    "    price = tick.ask if direction==\"BUY\" else tick.bid\n",
    "    order_type = mt5.ORDER_TYPE_BUY if direction==\"BUY\" else mt5.ORDER_TYPE_SELL\n",
    "\n",
    "    request = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": order_type,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl),\n",
    "        \"tp\": float(tp) if tp is not None else 0.0,\n",
    "        \"deviation\": int(deviation),\n",
    "        \"magic\": int(magic),\n",
    "        \"comment\": comment,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(request)\n",
    "    return res\n",
    "\n",
    "# Cell 8 — Load trained model and environment assets\n",
    "# Model\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Trained model not found at: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded model:\", MODEL_FILE)\n",
    "\n",
    "# Assets, embeddings, scalers\n",
    "safe_names, embeddings, scalers = load_assets_and_embeddings(DATA_DIR, EMBED_FILE)\n",
    "print(\"Assets:\", safe_names)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# Cell 9 — Single-pass prediction and optional execution across all assets.\n",
    "# This performs one sweep and returns a log of attempted/executed orders.\n",
    "\n",
    "def run_once_live(dry_run=DRY_RUN_DEFAULT, risk_per_trade=RISK_PER_TRADE):\n",
    "    if not mt5_init_if_needed():\n",
    "        print(\"⚠️ Warning: MT5 initialization failed or MT5 not running. Running in offline/dry mode.\")\n",
    "    acct = mt5.account_info() if mt5_init_if_needed() else None\n",
    "    balance = float(acct.balance) if acct is not None else 10000.0\n",
    "\n",
    "    records = []\n",
    "    for sym_safe in safe_names:\n",
    "        # Try to reconstruct broker symbol name: assume safe==symbol; if your safe names differ, update asset_to_symbol mapping\n",
    "        symbol = sym_safe\n",
    "        print(\"\\n---\", symbol, \"---\")\n",
    "        obs, vol_est, last_price = fetch_and_build_obs(symbol, window=WINDOW, scalers=scalers, embeddings=embeddings, safe_names=safe_names)\n",
    "        if obs is None:\n",
    "            print(\"Insufficient data for\", symbol, \"- skipping.\")\n",
    "            continue\n",
    "\n",
    "        # model.predict expects shape like env obs. Ensure dims align:\n",
    "        try:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "        except Exception as e:\n",
    "            # try flatten\n",
    "            try:\n",
    "                action, _ = model.predict(obs.flatten(), deterministic=True)\n",
    "            except Exception as e2:\n",
    "                print(\"Model predict failed for\", symbol, \":\", e, e2)\n",
    "                continue\n",
    "\n",
    "        act = extract_action_scalar(action)\n",
    "        if act == 0:\n",
    "            print(symbol, \"→ HOLD\")\n",
    "            records.append({\"symbol\":symbol, \"action\":\"HOLD\"})\n",
    "            continue\n",
    "\n",
    "        direction = \"BUY\" if act==1 else \"SELL\"\n",
    "        # compute stoploss/TP heuristics (based on vol_est)\n",
    "        if vol_est is None or vol_est<=0:\n",
    "            sl_dist = 0.001 * last_price\n",
    "        else:\n",
    "            sl_dist = max(1.5 * vol_est * last_price, 0.0005 * last_price)\n",
    "        sl = last_price - sl_dist if direction==\"BUY\" else last_price + sl_dist\n",
    "        tp = last_price + (2.5 * vol_est * last_price) if direction==\"BUY\" else last_price - (2.5 * vol_est * last_price)\n",
    "\n",
    "        lot = calculate_lot_size_live(symbol, balance, risk_per_trade, last_price, sl)\n",
    "        lot = max(MIN_LOT, min(MAX_LOT, round(lot, 2)))\n",
    "\n",
    "        # place order\n",
    "        res = place_order_on_mt5(symbol, direction, lot, sl, tp, dry_run=dry_run)\n",
    "        rec = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"symbol\": symbol,\n",
    "            \"action\": direction,\n",
    "            \"lot\": lot,\n",
    "            \"price\": last_price,\n",
    "            \"sl\": sl,\n",
    "            \"tp\": tp,\n",
    "            \"result\": str(getattr(res, \"retcode\", res))\n",
    "        }\n",
    "        records.append(rec)\n",
    "        print(\"Planned trade:\", rec)\n",
    "    # save records\n",
    "    out_csv = os.path.join(LOG_DIR, f\"live_run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    pd.DataFrame(records).to_csv(out_csv, index=False)\n",
    "    print(\"Saved run log to\", out_csv)\n",
    "    return records\n",
    "\n",
    "# Example single-run (dry-run): change dry_run=False to execute live (be cautious!)\n",
    "records = run_once_live(dry_run=False)\n",
    "\n",
    "# Cell 10 — Continuous loop for live trading (uncomment to enable). KEEP dry_run=True until fully tested.\n",
    "# WARNING: Live trading will place real orders if dry_run is False. Test on demo account first.\n",
    "\n",
    "def run_continuous(interval_s=60, dry_run=DRY_RUN_DEFAULT, risk_per_trade=RISK_PER_TRADE, max_iterations=None):\n",
    "    print(\"Starting continuous live loop — dry_run =\", dry_run)\n",
    "    iteration = 0\n",
    "    try:\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"\\n=== Iteration {iteration} @ {datetime.utcnow().isoformat()} ===\")\n",
    "            run_once_live(dry_run=dry_run, risk_per_trade=risk_per_trade)\n",
    "            if max_iterations is not None and iteration >= max_iterations:\n",
    "                print(\"Max iterations reached; stopping.\")\n",
    "                break\n",
    "            time.sleep(interval_s)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped by user (KeyboardInterrupt).\")\n",
    "    except Exception as e:\n",
    "        print(\"Loop error:\", e)\n",
    "    finally:\n",
    "        mt5_shutdown_if_needed()\n",
    "        print(\"MT5 connection closed.\")\n",
    "\n",
    "# Example usage (commented): run_continuous(interval_s=60, dry_run=True, max_iterations=10)\n",
    "run_continuous(interval_s=60, dry_run=True, max_iterations=10)\n",
    "\n",
    "# Cell 11 — Visualize logs and compute simple metrics\n",
    "def summarize_run_log(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    total_trades = len(df[df['action']!=\"HOLD\"])\n",
    "    if total_trades==0:\n",
    "        print(\"No trades in log.\")\n",
    "    print(\"Run summary:\", csv_path)\n",
    "    print(\"Total rows:\", len(df), \"Trades executed/planned:\", total_trades)\n",
    "    return df\n",
    "\n",
    "def plot_price_with_signals(symbol, run_records_csv):\n",
    "    df_log = pd.read_csv(run_records_csv)\n",
    "    df_sym = df_log[df_log['symbol']==symbol]\n",
    "    if df_sym.empty:\n",
    "        print(\"No records for\", symbol)\n",
    "        return\n",
    "    # fetch historical price for plotting (recent window)\n",
    "    bars = mt5.copy_rates_from_pos(symbol, TF_MT5, 0, WINDOW*5)\n",
    "    if bars is None:\n",
    "        print(\"No bars to plot for\", symbol)\n",
    "        return\n",
    "    d = pd.DataFrame(bars)\n",
    "    d['time'] = pd.to_datetime(d['time'], unit='s')\n",
    "    d.set_index('time', inplace=True)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(d['close'], label='close')\n",
    "    for _, row in df_sym.iterrows():\n",
    "        # mark entry price\n",
    "        plt.axhline(row['price'], linestyle='--', alpha=0.6)\n",
    "    plt.title(f\"{symbol} recent closes and signals\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: df = summarize_run_log(\"models/multiasset/live_logs/live_run_YYYY...csv\")\n",
    "# plot_price_with_signals(\"Volatility_75_Index\", \"path-to-log.csv\")\n",
    "\n",
    "# Cell 12 — Safe shutdown and final notes\n",
    "print(\"To run a single dry-run pass: records = run_once_live(dry_run=True)\")\n",
    "print(\"To run continuous (demo): run_continuous(interval_s=60, dry_run=True, max_iterations=10)\")\n",
    "print(\"To execute live for real orders set dry_run=False in run_once_live() or run_continuous(). DO NOT DO THIS UNTIL FULLY TESTED ON DEMO ACCOUNT.\")\n",
    "# Do not auto-init MT5 here — user will call run_* functions to start.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
