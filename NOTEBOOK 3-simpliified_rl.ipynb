{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de151741-dbd4-418c-9b0d-2f340cd8b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MetaTrader5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (5.0.5388)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (2.9.0+cpu)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
      "Requirement already satisfied: pygame in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (2.20.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (14.2.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (0.11.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from stable-baselines3[extra]) (11.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.19.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (6.33.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\slick\\desktop\\ml\\env\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Optional: install if missing (uncomment to run)\n",
    "import sys\n",
    "!{sys.executable} -m pip install MetaTrader5 stable-baselines3[extra] gymnasium numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456b96e0-66b7-4eeb-a332-1ca0a1927c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "import os, glob, time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c2eb54-58ba-4074-9dba-1d9a666a1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATHS (from your config) ===\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "METRICS_FILE = os.path.join(MODEL_DIR, \"eval_metrics.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafcee32-5041-4e5e-992e-2ef705650a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Timeframe mapping\n",
    "# --------------------------------------\n",
    "TIMEFRAME = \"M15\"\n",
    "TF_MAP = {\n",
    "    \"M1\": mt5.TIMEFRAME_M1,\n",
    "    \"M5\": mt5.TIMEFRAME_M5,\n",
    "    \"M15\": mt5.TIMEFRAME_M15,\n",
    "    \"M30\": mt5.TIMEFRAME_M30,\n",
    "    \"H1\": mt5.TIMEFRAME_H1,\n",
    "    \"H4\": mt5.TIMEFRAME_H4,\n",
    "    \"D1\": mt5.TIMEFRAME_D1\n",
    "}\n",
    "tf = TF_MAP[TIMEFRAME.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3affc552-0677-4f91-90ed-4d8e723de521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime config\n",
    "WINDOW = 50\n",
    "TF_MT5 = mt5.TIMEFRAME_M15   # M1 as used earlier\n",
    "DRY_RUN = False              # KEEP True while testing\n",
    "DEFAULT_LOT = 0.1           # as requested\n",
    "MAX_POS_PER_SYMBOL = 2\n",
    "\n",
    "# SL/TP settings: SL computed from volatility (function below); TP = 3 * SL\n",
    "DEFAULT_SL_PIPS_FALLBACK = 20\n",
    "TP_MULT = 3\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"live_trade_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb679acc-15a7-4394-9654-6886c2595b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "#def raw_from_safe(safe: str) -> str:\n",
    " #   return safe.replace(\"_\", \" \")\n",
    "\n",
    "def safe_from_raw(raw: str) -> str:\n",
    "    return make_safe_name(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6461be2-0a4e-45b7-bbb4-2a188a8f6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è Cell 4 - Symbols Selection\n",
    "# ================================================================\n",
    "SYMBOLS = [\n",
    "    # --- Volatility Indices (Standard) ---\n",
    "    \"Volatility 10 Index\",\n",
    "    \"Volatility 25 Index\",\n",
    "    \"Volatility 50 Index\",\n",
    "    \"Volatility 75 Index\",\n",
    "    \"Volatility 100 Index\",\n",
    "\n",
    "    # --- Volatility 1s Indices ---\n",
    "    \"Volatility 10 (1s) Index\",\n",
    "    \"Volatility 25 (1s) Index\",\n",
    "    \"Volatility 50 (1s) Index\",\n",
    "    \"Volatility 75 (1s) Index\",\n",
    "    \"Volatility 100 (1s) Index\",\n",
    "\n",
    "    # --- Volatility 10s Indices ---\n",
    "    \"Volatility 10 (10s) Index\",\n",
    "    \"Volatility 25 (10s) Index\",\n",
    "    \"Volatility 50 (10s) Index\",\n",
    "    \"Volatility 75 (10s) Index\",\n",
    "    \"Volatility 100 (10s) Index\",\n",
    "\n",
    "    # --- Jump Indices ---\n",
    "    \"Jump 10 Index\",\n",
    "    \"Jump 25 Index\",\n",
    "    \"Jump 50 Index\",\n",
    "    \"Jump 75 Index\",\n",
    "    \"Jump 100 Index\",\n",
    "\n",
    "    # --- Step Indices ---\n",
    "    \"Step Index 25\",\n",
    "    \"Step Index 50\",\n",
    "    \"Step Index 75\",\n",
    "    \"Step Index 100\",\n",
    "\n",
    "    # --- Forex Reference ---\n",
    "    \"EURUSD\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28f1db43-906a-41e7-abc3-cc789f4788ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_for_symbol(safe_name, window: int = WINDOW):\n",
    "    \"\"\"\n",
    "    Fetch recent bars from MT5 for the broker symbol mapped from safe_name.\n",
    "    Returns DataFrame with columns open,high,low,close,volume,Close_raw\n",
    "    \"\"\"\n",
    "    mt5.initialize()\n",
    "    raw_symbol = SYMBOLS\n",
    "    info = mt5.symbol_info(raw_symbol)\n",
    "    if info is None:\n",
    "        print(f\"‚ùå Symbol not found in MT5: {raw_symbol}\")\n",
    "        return None\n",
    "    # ensure visible\n",
    "    if not info.visible:\n",
    "        try:\n",
    "            mt5.symbol_select(raw_symbol, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    count = window + 60  # a buffer\n",
    "    bars = mt5.copy_rates_from_pos(raw_symbol, TF_MT5, 0, count)\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        print(f\"‚ùå Not enough bars for {raw_symbol} (got {0 if bars is None else len(bars)})\")\n",
    "        return None\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    if 'tick_volume' in df.columns:\n",
    "        df = df.rename(columns={'tick_volume': 'volume'})\n",
    "    df = df[['open','high','low','close','volume']].copy()\n",
    "    df['Close_raw'] = df['close']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300440d7-e942-4fa6-a696-ce99820f65d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Fetching live data for: Volatility 10 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 10 Index\n",
      "üì° Fetching live data for: Volatility 25 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 25 Index\n",
      "üì° Fetching live data for: Volatility 50 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 50 Index\n",
      "üì° Fetching live data for: Volatility 75 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 75 Index\n",
      "üì° Fetching live data for: Volatility 100 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 100 Index\n",
      "üì° Fetching live data for: Volatility 10 (1s) Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 10 (1s) Index\n",
      "üì° Fetching live data for: Volatility 25 (1s) Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 25 (1s) Index\n",
      "üì° Fetching live data for: Volatility 50 (1s) Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 50 (1s) Index\n",
      "üì° Fetching live data for: Volatility 75 (1s) Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 75 (1s) Index\n",
      "üì° Fetching live data for: Volatility 100 (1s) Index\n",
      "‚úîÔ∏è Loaded 110 bars for Volatility 100 (1s) Index\n",
      "üì° Fetching live data for: Volatility 10 (10s) Index\n",
      "‚ùå Symbol not found in MT5: Volatility 10 (10s) Index\n",
      "‚ö†Ô∏è Failed to load data for Volatility 10 (10s) Index\n",
      "üì° Fetching live data for: Volatility 25 (10s) Index\n",
      "‚ùå Symbol not found in MT5: Volatility 25 (10s) Index\n",
      "‚ö†Ô∏è Failed to load data for Volatility 25 (10s) Index\n",
      "üì° Fetching live data for: Volatility 50 (10s) Index\n",
      "‚ùå Symbol not found in MT5: Volatility 50 (10s) Index\n",
      "‚ö†Ô∏è Failed to load data for Volatility 50 (10s) Index\n",
      "üì° Fetching live data for: Volatility 75 (10s) Index\n",
      "‚ùå Symbol not found in MT5: Volatility 75 (10s) Index\n",
      "‚ö†Ô∏è Failed to load data for Volatility 75 (10s) Index\n",
      "üì° Fetching live data for: Volatility 100 (10s) Index\n",
      "‚ùå Symbol not found in MT5: Volatility 100 (10s) Index\n",
      "‚ö†Ô∏è Failed to load data for Volatility 100 (10s) Index\n",
      "üì° Fetching live data for: Jump 10 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Jump 10 Index\n",
      "üì° Fetching live data for: Jump 25 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Jump 25 Index\n",
      "üì° Fetching live data for: Jump 50 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Jump 50 Index\n",
      "üì° Fetching live data for: Jump 75 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Jump 75 Index\n",
      "üì° Fetching live data for: Jump 100 Index\n",
      "‚úîÔ∏è Loaded 110 bars for Jump 100 Index\n",
      "üì° Fetching live data for: Step Index 25\n",
      "‚ùå Symbol not found in MT5: Step Index 25\n",
      "‚ö†Ô∏è Failed to load data for Step Index 25\n",
      "üì° Fetching live data for: Step Index 50\n",
      "‚ùå Symbol not found in MT5: Step Index 50\n",
      "‚ö†Ô∏è Failed to load data for Step Index 50\n",
      "üì° Fetching live data for: Step Index 75\n",
      "‚ùå Symbol not found in MT5: Step Index 75\n",
      "‚ö†Ô∏è Failed to load data for Step Index 75\n",
      "üì° Fetching live data for: Step Index 100\n",
      "‚ùå Symbol not found in MT5: Step Index 100\n",
      "‚ö†Ô∏è Failed to load data for Step Index 100\n",
      "üì° Fetching live data for: EURUSD\n",
      "‚úîÔ∏è Loaded 110 bars for EURUSD\n",
      "\n",
      "‚úÖ Completed fetching live data for 16 symbols.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è Cell 4 - Auto-Fetch Live Bars for All Symbols\n",
    "# ================================================================\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch parameters\n",
    "# --------------------------------------\n",
    "WINDOW = 50\n",
    "BUFFER = 60\n",
    "COUNT = WINDOW + BUFFER   # extra bars for safety\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch bars for each symbol individually\n",
    "# --------------------------------------\n",
    "def fetch_symbol_bars(symbol):\n",
    "    \"\"\"Fetch bars safely for one symbol.\"\"\"\n",
    "    # Ensure MT5 knows this symbol\n",
    "\n",
    "    mt5.initialize()\n",
    "    \n",
    "    info = mt5.symbol_info(symbol)\n",
    "    if info is None:\n",
    "        print(f\"‚ùå Symbol not found in MT5: {symbol}\")\n",
    "        return None\n",
    "\n",
    "    if not info.visible:\n",
    "        mt5.symbol_select(symbol, True)\n",
    "\n",
    "    bars = mt5.copy_rates_from_pos(symbol, tf, 0, COUNT)\n",
    "\n",
    "    if bars is None or len(bars) < WINDOW + 2:\n",
    "        print(f\"‚ùå Not enough bars for {symbol} (got {0 if bars is None else len(bars)})\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    mt5.shutdown()\n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch all symbols into a dictionary\n",
    "# --------------------------------------\n",
    "symbol_data = {}\n",
    "\n",
    "for sym in SYMBOLS:\n",
    "    print(f\"üì° Fetching live data for: {sym}\")\n",
    "    df = fetch_symbol_bars(sym)\n",
    "    if df is not None:\n",
    "        symbol_data[sym] = df\n",
    "        print(f\"‚úîÔ∏è Loaded {len(df)} bars for {sym}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Failed to load data for {sym}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Completed fetching live data for {len(symbol_data)} symbols.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b058bc-9efd-4062-9bd2-d80e0925777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets: 16 scalers: 16 embeddings: 17\n",
      "Assets: ['EURUSD', 'Jump_100_Index', 'Jump_10_Index', 'Jump_25_Index', 'Jump_50_Index', 'Jump_75_Index', 'Volatility_100_1s_Index', 'Volatility_100_Index', 'Volatility_10_1s_Index', 'Volatility_10_Index', 'Volatility_25_1s_Index', 'Volatility_25_Index', 'Volatility_50_1s_Index', 'Volatility_50_Index', 'Volatility_75_1s_Index', 'Volatility_75_Index']\n"
     ]
    }
   ],
   "source": [
    "# scalers: read per-asset CSVs created earlier (mean/std)\n",
    "def load_scalers_from_csv(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                print(\"Skipping\", p, \"- unexpected format\")\n",
    "                continue\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found:\", embed_file)\n",
    "        return {}\n",
    "    emb = np.load(embed_file, allow_pickle=True)\n",
    "    emb_dict = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "        for safe, idx in am.items():\n",
    "            idx = int(idx)\n",
    "            if idx < emb.shape[0]:\n",
    "                emb_dict[safe] = np.array(emb[idx], dtype=np.float32)\n",
    "    else:\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i, safe in enumerate(safe_list):\n",
    "                emb_dict[safe] = np.array(emb[i], dtype=np.float32)\n",
    "        else:\n",
    "            print(\"Warning: embedding count and CSV count mismatch\")\n",
    "    return emb_dict\n",
    "\n",
    "# Load them\n",
    "scalers = load_scalers_from_csv(DATA_DIR)\n",
    "datasets = load_datasets(DATA_DIR, WINDOW)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "\n",
    "print(\"Loaded datasets:\", len(datasets), \"scalers:\", len(scalers), \"embeddings:\", len(embeddings))\n",
    "safe_list = sorted(datasets.keys())\n",
    "print(\"Assets:\", safe_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72ca0c0-3c31-4766-a93e-1a3472f77633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO model: models\\multiasset\\ppo_multiasset.zip\n",
      "Warning: VecNormalize load failed: VecNormalize.load() missing 1 required positional argument: 'venv'\n"
     ]
    }
   ],
   "source": [
    "# Load PPO and VecNormalize\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Model file missing: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded PPO model:\", MODEL_FILE)\n",
    "\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE)\n",
    "        print(\"Loaded VecNormalize:\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: VecNormalize load failed:\", e)\n",
    "        vecnorm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690cade6-579c-4d4c-884d-f260125e20b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VecNormalize from models\\multiasset\\vec_normalize.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell: Safe VecNormalize loader using an inline dummy env (no external envs module required)\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# parameters that must match training\n",
    "WINDOW = 50                   # your window used in training\n",
    "N_PRICE_FEATURES = 5          # o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "EMBED_DIM = next(iter(embeddings.values())).shape[0] if len(embeddings)>0 else 8\n",
    "OBS_DIM = N_PRICE_FEATURES + 1 + EMBED_DIM   # matches: 5 + 1 + embed_dim\n",
    "\n",
    "class _DummyTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Minimal dummy env with the correct obs/action spaces for VecNormalize.load().\n",
    "    Used only to provide a 'venv' for VecNormalize.load(path, venv).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # observation: shape=(WINDOW, OBS_DIM)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(WINDOW, OBS_DIM), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(3)  # hold / buy / sell\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # return an observation of correct shape and empty info dict\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # return obs, reward, done, truncated, info in gymnasium style\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        reward = 0.0\n",
    "        done = True   # immediate done is fine for dummy\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "# Create vectorized dummy env with 1 env (can be >1)\n",
    "venv = DummyVecEnv([lambda: _DummyTradingEnv()])\n",
    "\n",
    "# Try loading VecNormalize with the dummy venv\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv)\n",
    "        # ensure it's in inference mode\n",
    "        vecnorm.training = False\n",
    "        vecnorm.norm_reward = False\n",
    "        print(\"Loaded VecNormalize from\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: VecNormalize.load failed:\", e)\n",
    "        vecnorm = None\n",
    "else:\n",
    "    print(\"No VecNormalize file found at\", VEC_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d35bcd-4fdd-4262-9ea3-f0cfd572be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_build_obs(safe_name, window, scalers, embeddings, datasets, safe_list):\n",
    "    \"\"\"\n",
    "    Build observation consistent with training:\n",
    "    obs shape = (window, 5 + 1 + embed_dim)\n",
    "    where 5 = o_pc,h_pc,l_pc,c_pc,v_pc ; 1 = balance (fixed 1.0) ; embed_dim from embeddings.\n",
    "    \"\"\"\n",
    "    # prefer live bars, but fallback to preloaded dataset if fetch fails\n",
    "    df_live = fetch_data_for_symbol(safe_name, window)\n",
    "    if df_live is None:\n",
    "        # fallback to prepared dataset\n",
    "        if safe_name not in datasets:\n",
    "            print(\"No data available for\", safe_name)\n",
    "            return None, None, None\n",
    "        df = datasets[safe_name].iloc[-(window+1):].copy()\n",
    "        # reconstruct OHLCV in price-space from Close_raw if needed is not possible; but we expect dataset to contain pct-change already\n",
    "        # We'll use stored pct columns if present:\n",
    "        df_pct = df[['o_pc','h_pc','l_pc','c_pc','v_pc']].copy()\n",
    "        last_price = float(df['Close_raw'].iloc[-1])\n",
    "    else:\n",
    "        # compute pct-change from live OHLCV\n",
    "        df = df_live\n",
    "        df_pct = df[['open','high','low','close','volume']].pct_change().dropna()\n",
    "        if len(df_pct) < window:\n",
    "            print(\"Not enough pct rows for\", safe_name)\n",
    "            return None, None, None\n",
    "        df_pct = df_pct.tail(window)\n",
    "        last_price = float(df['Close_raw'].iloc[-1])\n",
    "\n",
    "    # when using infile dataset, df_pct above already length==window\n",
    "    if df_pct.shape[0] < window:\n",
    "        print(\"Window mismatch for\", safe_name)\n",
    "        return None, None, None\n",
    "\n",
    "    # get scaler\n",
    "    if safe_name not in scalers:\n",
    "        print(\"No scaler for\", safe_name)\n",
    "        return None, None, None\n",
    "    s = scalers[safe_name]\n",
    "    mean = np.array(s['mean'].values if hasattr(s['mean'], 'values') else s['mean'], dtype=np.float32)\n",
    "    std  = np.array(s['std'].values if hasattr(s['std'], 'values') else s['std'], dtype=np.float32)\n",
    "    std = np.where(std == 0, 1e-8, std)\n",
    "\n",
    "    # Ensure df_pct column order matches mean/std indices: scalers were saved with index names o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "    cols = ['o_pc','h_pc','l_pc','c_pc','v_pc']\n",
    "    # If df_pct currently has original OHLCV names, convert order\n",
    "    if list(df_pct.columns) != cols:\n",
    "        # attempt to rename if current names are open,high,...\n",
    "        if all(c in df_pct.columns for c in ['open','high','low','close','volume']):\n",
    "            df_pct = df_pct[['open','high','low','close','volume']]\n",
    "        else:\n",
    "            # assume df_pct already has o_pc,h_pc...\n",
    "            pass\n",
    "\n",
    "    # get features array of shape (window,5)\n",
    "    features = df_pct[cols].values.astype(np.float32) if all(c in df_pct.columns for c in cols) else df_pct.values.astype(np.float32)\n",
    "    features_scaled = (features - mean) / std\n",
    "\n",
    "    # embedding\n",
    "    if safe_name in embeddings:\n",
    "        emb_vec = np.array(embeddings[safe_name], dtype=np.float32)\n",
    "    else:\n",
    "        emb_dim = next(iter(embeddings.values())).shape[0] if len(embeddings)>0 else 8\n",
    "        emb_vec = np.zeros(emb_dim, dtype=np.float32)\n",
    "    emb_block = np.repeat(emb_vec.reshape(1,-1), window, axis=0)\n",
    "\n",
    "    # balance column (used in training) - keep as ones\n",
    "    balance_col = np.ones((window,1), dtype=np.float32)\n",
    "\n",
    "    obs = np.concatenate([features_scaled, balance_col, emb_block], axis=1).astype(np.float32)\n",
    "\n",
    "    # If VecNormalize exists and matches expected flattened size, attempt to apply (vecnorm expects flattened obs in older SB3 versions)\n",
    "    if vecnorm is not None:\n",
    "        try:\n",
    "            flat = obs.reshape(1, -1)\n",
    "            mean_rms = getattr(vecnorm.obs_rms, \"mean\", None)\n",
    "            var_rms = getattr(vecnorm.obs_rms, \"var\", None)\n",
    "            if mean_rms is not None and var_rms is not None and len(mean_rms) == flat.shape[1]:\n",
    "                flat_norm = (flat - mean_rms) / np.sqrt(var_rms + 1e-8)\n",
    "                obs = flat_norm.reshape(obs.shape).astype(np.float32)\n",
    "            else:\n",
    "                # some SB3 versions provide normalize_obs\n",
    "                try:\n",
    "                    obs = vecnorm.normalize_obs(obs, False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(\"VecNormalize apply error:\", e)\n",
    "\n",
    "    # volatility estimate for SL/lot sizing\n",
    "    vol_est = float(df_pct['c_pc'].std()) if 'c_pc' in df_pct.columns else float(np.std(features[:,3]))\n",
    "\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc942503-0b47-41ab-b24b-67fc8ba0eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip_value(symbol):\n",
    "    return 0.01 if \"JPY\" in symbol.upper() else 0.0001\n",
    "\n",
    "def estimate_sl_pips_from_vol(raw_symbol, last_price, vol_est, min_pips=5, max_pips=200):\n",
    "    if \"JPY\" in raw_symbol.upper():\n",
    "        pip = 0.01\n",
    "    else:\n",
    "        pip = 0.0001\n",
    "    abs_move = vol_est * last_price\n",
    "    if abs_move <= 0:\n",
    "        sl_pips = DEFAULT_SL_PIPS_FALLBACK\n",
    "    else:\n",
    "        sl_raw = abs_move / pip\n",
    "        sl_pips = float(np.clip(np.round(sl_raw * 1.5), min_pips, max_pips))\n",
    "    return int(sl_pips)\n",
    "\n",
    "def compute_lot_from_balance(balance, vol, price, risk_pct=0.005, min_lot=0.01, max_lot=1.0):\n",
    "    risk_amount = balance * risk_pct\n",
    "    vol = max(vol, 1e-8)\n",
    "    price_scale = 1000.0\n",
    "    lot = risk_amount / (vol * price_scale)\n",
    "    return float(max(min_lot, min(max_lot, round(lot, 2))))\n",
    "\n",
    "def compute_sl_tp_by_pips(symbol, price, direction, sl_pips, tp_pips):\n",
    "    pip = pip_value(symbol)\n",
    "    if direction == \"BUY\":\n",
    "        sl = price - sl_pips * pip\n",
    "        tp = price + tp_pips * pip\n",
    "    else:\n",
    "        sl = price + sl_pips * pip\n",
    "        tp = price - tp_pips * pip\n",
    "    return float(sl), float(tp)\n",
    "\n",
    "def get_positions_for_symbol(symbol):\n",
    "    pos = mt5.positions_get(symbol=symbol)\n",
    "    return [] if pos is None else list(pos)\n",
    "\n",
    "def place_market_order(symbol, direction, lot, sl_price=None, tp_price=None):\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"‚ùå Cannot get tick for\", symbol)\n",
    "        return {\"retcode\": None, \"comment\":\"NO_TICK\"}\n",
    "    price = float(tick.ask if direction==\"BUY\" else tick.bid)\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\": 10009, \"price\": price, \"comment\":\"DRY_RUN\", \"direction\":direction}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if direction==\"BUY\" else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl_price) if sl_price is not None else 0.0,\n",
    "        \"tp\": float(tp_price) if tp_price is not None else 0.0,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"ppo_multiasset_live\",\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(req)\n",
    "    return res\n",
    "\n",
    "def close_position_by_ticket(ticket):\n",
    "    pos_list = mt5.positions_get(ticket=ticket)\n",
    "    if not pos_list:\n",
    "        return None\n",
    "    p = pos_list[0]\n",
    "    symbol = p.symbol\n",
    "    if getattr(p, \"type\", None) == 0:  # BUY => close with SELL\n",
    "        order_type = mt5.ORDER_TYPE_SELL\n",
    "        price = mt5.symbol_info_tick(symbol).bid\n",
    "    else:\n",
    "        order_type = mt5.ORDER_TYPE_BUY\n",
    "        price = mt5.symbol_info_tick(symbol).ask\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\":10009, \"comment\":\"DRY_RUN_CLOSE\", \"ticket\": ticket}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(p.volume),\n",
    "        \"type\": order_type,\n",
    "        \"position\": int(ticket),\n",
    "        \"price\": price,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"auto_close\",\n",
    "    }\n",
    "    return mt5.order_send(req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "067469f2-5f07-4449-a0e8-688752df235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets):\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    acct = mt5.account_info()\n",
    "    balance = float(acct.balance) if acct else 10000.0\n",
    "\n",
    "    for safe_name in safe_list:\n",
    "        raw_symbol = get_mt5_symbol(safe_name)\n",
    "        print(f\"\\n--- {raw_symbol} ({safe_name}) ---\")\n",
    "        obs, vol, last_price = fetch_and_build_obs(safe_name, WINDOW, scalers, embeddings, datasets, safe_list)\n",
    "        if obs is None:\n",
    "            print(\"Skip\", safe_name)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            action, _ = model.predict(obs[np.newaxis,...], deterministic=True)\n",
    "            a = int(action[0]) if isinstance(action,(list,tuple,np.ndarray)) else int(action)\n",
    "            print(\"Signal:\", a)\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "            continue\n",
    "\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        print(\"Existing positions:\", len(positions))\n",
    "\n",
    "        # auto-close opposing\n",
    "        if a == 1:\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 1:\n",
    "                    print(\"Closing opposing SELL ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "        elif a == 2:\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 0:\n",
    "                    print(\"Closing opposing BUY ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "\n",
    "        # refresh positions\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        if len(positions) >= MAX_POS_PER_SYMBOL:\n",
    "            print(\"Max positions reached, skipping open\")\n",
    "        else:\n",
    "            if a == 0:\n",
    "                print(\"HOLD\")\n",
    "            else:\n",
    "                direction = \"BUY\" if a==1 else \"SELL\"\n",
    "                lot = compute_lot_from_balance(balance, vol, last_price)\n",
    "                sl_pips = estimate_sl_pips_from_vol(raw_symbol, last_price, vol)\n",
    "                tp_pips = int(sl_pips * TP_MULT)\n",
    "                sl_price, tp_price = compute_sl_tp_by_pips(raw_symbol, last_price, direction, sl_pips, tp_pips)\n",
    "                res = place_market_order(raw_symbol, direction, lot, sl_price, tp_price)\n",
    "\n",
    "                if isinstance(res, dict):\n",
    "                    retcode = res.get(\"retcode\")\n",
    "                    comment = res.get(\"comment\")\n",
    "                    exec_price = res.get(\"price\", last_price)\n",
    "                else:\n",
    "                    retcode = getattr(res,\"retcode\", None)\n",
    "                    comment = getattr(res,\"comment\", \"\")\n",
    "                    exec_price = last_price\n",
    "\n",
    "                entry = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"safe\": safe_name,\n",
    "                    \"symbol\": raw_symbol,\n",
    "                    \"action\": direction,\n",
    "                    \"lot\": lot,\n",
    "                    \"exec_price\": exec_price,\n",
    "                    \"sl_price\": sl_price,\n",
    "                    \"tp_price\": tp_price,\n",
    "                    \"sl_pips\": sl_pips,\n",
    "                    \"tp_pips\": tp_pips,\n",
    "                    \"retcode\": retcode,\n",
    "                    \"comment\": comment,\n",
    "                    \"dry_run\": DRY_RUN\n",
    "                }\n",
    "                pd.DataFrame([entry]).to_csv(LOG_FILE, mode=\"a\", index=False, header=header)\n",
    "                header = False\n",
    "                print(\"Placed\", direction, \"lot\", lot, \"retcode\", retcode)\n",
    "\n",
    "        # trailing\n",
    "        for p in get_positions_for_symbol(raw_symbol):\n",
    "            # recompute vol-based trail\n",
    "            trail_pips = max(5, int(estimate_sl_pips_from_vol(raw_symbol, last_price, vol) // 2))\n",
    "            pip = pip_value(raw_symbol)\n",
    "            if getattr(p,\"type\",None) == 0:  # BUY\n",
    "                cur = mt5.symbol_info_tick(raw_symbol).bid\n",
    "                new_sl = float(cur - trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw_symbol, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "            else:  # SELL\n",
    "                cur = mt5.symbol_info_tick(raw_symbol).ask\n",
    "                new_sl = float(cur + trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw_symbol, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "\n",
    "    print(\"\\nSingle pass complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1a34b80-ccd3-4ab3-a94a-ea74327f3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # ensure expected columns exist; otherwise try conversion as earlier notebooks did\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            # attempt auto-convert if original OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing expected columns and cannot convert\")\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf706d1-ed7a-4d64-bd5f-948d79e8b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_datasets(data_dir=DATA_DIR, window=WINDOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea83b763-24a9-4dfb-9bc8-25ca13706dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets: ['EURUSD', 'Jump_100_Index', 'Jump_10_Index', 'Jump_25_Index', 'Jump_50_Index', 'Jump_75_Index', 'Volatility_100_1s_Index', 'Volatility_100_Index', 'Volatility_10_1s_Index', 'Volatility_10_Index', 'Volatility_25_1s_Index', 'Volatility_25_Index', 'Volatility_50_1s_Index', 'Volatility_50_Index', 'Volatility_75_1s_Index', 'Volatility_75_Index']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_mt5_symbol' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m safe_list = \u001b[38;5;28msorted\u001b[39m(datasets.keys())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssets:\u001b[39m\u001b[33m\"\u001b[39m, safe_list)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mrun_once_predict_and_manage\u001b[39m\u001b[34m(model, safe_list, scalers, embeddings, datasets)\u001b[39m\n\u001b[32m      4\u001b[39m balance = \u001b[38;5;28mfloat\u001b[39m(acct.balance) \u001b[38;5;28;01mif\u001b[39;00m acct \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m10000.0\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m safe_name \u001b[38;5;129;01min\u001b[39;00m safe_list:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     raw_symbol = get_mt5_symbol(safe_name)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     obs, vol, last_price = fetch_and_build_obs(safe_name, WINDOW, scalers, embeddings, datasets, safe_list)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_mt5_symbol' is not defined"
     ]
    }
   ],
   "source": [
    "safe_list = sorted(datasets.keys())\n",
    "print(\"Assets:\", safe_list)\n",
    "run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8500bb-f6e0-4999-a3fd-d7da8f6dd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with caution. Set DRY_RUN=False only after testing thoroughly.\n",
    "# try:\n",
    "#     while True:\n",
    "#         run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n",
    "#         time.sleep(60)  # wait one minute (for M1)\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopped by user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275283fa-dfbe-4f6d-9d26-085ed3397096",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(LOG_FILE):\n",
    "    df = pd.read_csv(LOG_FILE)\n",
    "    display(df.tail(30))\n",
    "else:\n",
    "    print(\"No logs yet at\", LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc64ef-2b49-4935-a4d6-8e371b2861b8",
   "metadata": {},
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52de425e-638d-4f1e-ad43-d94984013299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded assets: 16\n",
      "Loaded scalers: 16\n",
      "Loaded embeddings: 17\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "# Load per-asset scalers saved as CSVs: {safe}_scaler.csv with columns mean/std\n",
    "def load_scalers_from_csv(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        # ensure ordering of mean/std columns matches our feature order: o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # ensure expected columns exist; otherwise try conversion as earlier notebooks did\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            # attempt auto-convert if original OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing expected columns and cannot convert\")\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found at\", embed_file)\n",
    "        return {}\n",
    "    emb = np.load(embed_file, allow_pickle=True)\n",
    "    # emb likely is ndarray shape (n_assets, embed_dim)\n",
    "    # Map using asset_to_idx.csv if present\n",
    "    emb_dict = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "        # am is mapping safe->idx\n",
    "        for safe, idx in am.items():\n",
    "            idx = int(idx)\n",
    "            if idx < emb.shape[0]:\n",
    "                emb_dict[safe] = np.array(emb[idx], dtype=np.float32)\n",
    "    else:\n",
    "        # fallback: map by order of normalized CSVs if sizes match\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i, safe in enumerate(safe_list):\n",
    "                emb_dict[safe] = np.array(emb[i], dtype=np.float32)\n",
    "        else:\n",
    "            print(\"Warning: embedding count and CSV count mismatch; manual mapping required.\")\n",
    "    return emb_dict\n",
    "\n",
    "# load\n",
    "scalers = load_scalers_from_csv(DATA_DIR)\n",
    "datasets = load_datasets(DATA_DIR, WINDOW)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "\n",
    "print(\"Loaded assets:\", len(datasets))\n",
    "print(\"Loaded scalers:\", len(scalers))\n",
    "print(\"Loaded embeddings:\", len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3c8afdf-7f7e-4194-9e87-351e25985bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO from models\\multiasset\\ppo_multiasset.zip\n",
      "VecNormalize load failed: VecNormalize.load() missing 1 required positional argument: 'venv'\n"
     ]
    }
   ],
   "source": [
    "# Cell 5\n",
    "# Load PPO model\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Model not found: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded PPO from\", MODEL_FILE)\n",
    "\n",
    "# Try to load VecNormalize (optional)\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE)\n",
    "        print(\"Loaded VecNormalize from\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"VecNormalize load failed:\", e)\n",
    "        vecnorm = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0057ce-1c19-4530-882c-759946b1c26c",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# Cell 5 ‚Äî fetch_and_build_obs for live trading (fixed)\n",
    "# ================================================================\n",
    "def fetch_and_build_obs(safe_name, window, scalers, embeddings, safe_list):\n",
    "    \"\"\"\n",
    "    safe_name: e.g., 'EURUSD', 'Jump_100_Index'\n",
    "    Returns:\n",
    "        obs: np.array (window, n_features + embedding + extras)\n",
    "        vol: float, estimated volatility\n",
    "        last_price: float\n",
    "    \"\"\"\n",
    "    # Map safe_name -> broker symbol\n",
    "    raw_symbol = get_mt5_symbol(safe_name)\n",
    "\n",
    "    # Fetch latest bars\n",
    "    df = fetch_data_for_symbol(safe_name, window)\n",
    "    if df is None:\n",
    "        return None, None, None\n",
    "\n",
    "    # Use pct-change features\n",
    "    df_pct = df[['open','high','low','close','volume']].pct_change().dropna()\n",
    "    if len(df_pct) < window:\n",
    "        print(f\"‚ùå Not enough pct rows for {safe_name}\")\n",
    "        return None, None, None\n",
    "    df_pct = df_pct.tail(window)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = scalers[safe_name]\n",
    "    mean = scaler['mean'][['o_pc','h_pc','l_pc','c_pc','v_pc']]\n",
    "    std  = scaler['std'][['o_pc','h_pc','l_pc','c_pc','v_pc']].replace(0,1.0)\n",
    "    features_scaled = (df_pct.values - mean.values) / std.values\n",
    "\n",
    "    # Embedding\n",
    "    embed = embeddings.get(safe_name, np.zeros((list(embeddings.values())[0].shape[0],), dtype=np.float32))\n",
    "    emb_rep = np.repeat(embed[np.newaxis,:], window, axis=0)\n",
    "\n",
    "    # Extra columns: balance_norm=1.0, asset_id normalized\n",
    "    balance_norm = np.ones((window,1), dtype=np.float32)\n",
    "    asset_id_val = safe_list.index(safe_name)/max(1,len(safe_list))\n",
    "    asset_id_col = np.full((window,1), asset_id_val, dtype=np.float32)\n",
    "\n",
    "    # Final observation\n",
    "    obs = np.hstack([features_scaled.astype(np.float32), emb_rep, balance_norm, asset_id_col])\n",
    "    last_price = float(df['Close_raw'].iloc[-1])\n",
    "    vol = float(df_pct['close'].std())\n",
    "\n",
    "    return obs, vol, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1ba81f1-eefb-4651-bf08-0196e19fc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def fetch_and_build_obs(raw_symbol, window, scalers, embeddings, datasets, safe_list):\n",
    "    \"\"\"\n",
    "    Build observation matching training:\n",
    "    obs shape = (window, n_features=5 + 1 + embed_dim)\n",
    "      -> 5: o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "      -> 1: volatility column (c_pc) per timestep (training used balance column previously; we use c_pc as volatility column)\n",
    "         NOTE: original training used 'balance' column as +1; earlier env used balance=1.0. To be exact, we will include 'balance' column instead of vol column so obs_dim matches training:\n",
    "         training obs was [OHLCV_pctnorm, balance, embedding]. So we'll include balance (1.0) and embedding; volatility used for SL only.\n",
    "    \"\"\"\n",
    "    safe = safe_from_raw(raw_symbol)\n",
    "    if safe not in datasets:\n",
    "        print(\"No dataset for\", safe)\n",
    "        return None, None, None\n",
    "    df = datasets[safe]\n",
    "    if len(df) < window:\n",
    "        print(\"Not enough data for\", safe)\n",
    "        return None, None, None\n",
    "\n",
    "    window_df = df.iloc[-window:]\n",
    "    # features (pct-changes)\n",
    "    try:\n",
    "        features = window_df[['o_pc','h_pc','l_pc','c_pc','v_pc']].values.astype(np.float32)  # shape (window,5)\n",
    "        last_price = float(window_df['Close_raw'].iloc[-1])\n",
    "    except KeyError as e:\n",
    "        print(\"Column missing:\", e)\n",
    "        return None, None, None\n",
    "\n",
    "    # scale using scalers dict (mean/std may be pandas Series)\n",
    "    if safe not in scalers:\n",
    "        print(\"No scaler for\", safe)\n",
    "        return None, None, None\n",
    "    s = scalers[safe]\n",
    "    # accept either pandas Series or numpy arrays\n",
    "    mean = np.array(s['mean'].values if hasattr(s['mean'], \"values\") else s['mean'], dtype=np.float32)\n",
    "    std  = np.array(s['std'].values if hasattr(s['std'], \"values\") else s['std'], dtype=np.float32)\n",
    "    std = np.where(std == 0, 1e-8, std)\n",
    "    features_scaled = (features - mean) / std\n",
    "\n",
    "    # embedding vector\n",
    "    if safe in embeddings:\n",
    "        emb_vec = np.array(embeddings[safe], dtype=np.float32)\n",
    "    else:\n",
    "        # fallback zero vector of expected embedding dim (use first embedding length if possible)\n",
    "        if len(embeddings)>0:\n",
    "            emb_dim = next(iter(embeddings.values())).shape[0]\n",
    "            emb_vec = np.zeros(emb_dim, dtype=np.float32)\n",
    "        else:\n",
    "            emb_vec = np.zeros(8, dtype=np.float32)  # fallback to 8\n",
    "\n",
    "    emb_block = np.repeat(emb_vec.reshape(1, -1), window, axis=0)  # (window, embed_dim)\n",
    "\n",
    "    # Balance column (training used balance; set to 1.0)\n",
    "    balance_col = np.full((window,1), 1.0, dtype=np.float32)\n",
    "\n",
    "    # Final obs: [features_scaled (5) , balance (1) , embedding (embed_dim)]\n",
    "    obs = np.concatenate([features_scaled, balance_col, emb_block], axis=1).astype(np.float32)  # (window, 5+1+embed_dim)\n",
    "\n",
    "    # VecNormalize optional normalization: apply if present (flatten-first)\n",
    "    if vecnorm is not None:\n",
    "        try:\n",
    "            # vecnorm.obs_rms.mean shape matches flattened obs; we need to flatten timestep dimension\n",
    "            flat = obs.reshape(1, -1)  # shape (1, window * feat)\n",
    "            mean_rms = getattr(vecnorm.obs_rms, \"mean\", None)\n",
    "            var_rms = getattr(vecnorm.obs_rms, \"var\", None)\n",
    "            if mean_rms is not None and var_rms is not None and len(mean_rms) == flat.shape[1]:\n",
    "                flat_norm = (flat - mean_rms) / np.sqrt(var_rms + 1e-8)\n",
    "                obs = flat_norm.reshape(obs.shape).astype(np.float32)\n",
    "            else:\n",
    "                # fallback: use vecnorm.normalize_obs if available (some versions)\n",
    "                try:\n",
    "                    obs = vecnorm.normalize_obs(obs, False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(\"VecNormalize apply failed:\", e)\n",
    "\n",
    "    # estimate vol for lot/SL computation (std of c_pc)\n",
    "    vol_est = float(window_df['c_pc'].std())\n",
    "\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dfa1ab19-70d4-418b-94bf-bd15e38ab33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def estimate_sl_pips_from_vol(raw_symbol, last_price, vol_est, min_pips=5, max_pips=200):\n",
    "    \"\"\"\n",
    "    Heuristic placeholder for 'learned' SL:\n",
    "    - vol_est is std of c_pc (pct-change) per bar\n",
    "    - approximate absolute move = vol_est * last_price\n",
    "    - convert to pips: pip = 0.0001 (or 0.01 for JPY)\n",
    "    - produce sl_pips clipped to [min_pips, max_pips]\n",
    "    \"\"\"\n",
    "    if \"JPY\" in raw_symbol.upper():\n",
    "        pip = 0.01\n",
    "    else:\n",
    "        pip = 0.0001\n",
    "    # absolute expected move:\n",
    "    abs_move = vol_est * last_price\n",
    "    if abs_move <= 0:\n",
    "        sl_pips = DEFAULT_SL_PIPS_FALLBACK\n",
    "    else:\n",
    "        sl_raw = abs_move / pip\n",
    "        # safety scale factor\n",
    "        sl_pips = float(np.clip(np.round(sl_raw * 1.5), min_pips, max_pips))\n",
    "    return int(sl_pips)\n",
    "\n",
    "def compute_lot_fixed(default_lot=DEFAULT_LOT):\n",
    "    return float(default_lot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0c8a84c-a94a-4a91-8015-931e667fd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def get_positions_for_symbol(symbol):\n",
    "    pos = mt5.positions_get(symbol=symbol)\n",
    "    return [] if pos is None else list(pos)\n",
    "\n",
    "def place_market_order(symbol, direction, lot, sl_price=None, tp_price=None):\n",
    "    \"\"\"\n",
    "    direction: \"BUY\" or \"SELL\"\n",
    "    returns dict-like result (simulated if DRY_RUN)\n",
    "    \"\"\"\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"‚ùå Cannot get tick for\", symbol)\n",
    "        return None\n",
    "    price = float(tick.ask if direction==\"BUY\" else tick.bid)\n",
    "    if DRY_RUN:\n",
    "        # simulated response\n",
    "        return {\"retcode\": 10009, \"price\": price, \"comment\":\"DRY_RUN\", \"direction\":direction}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if direction==\"BUY\" else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl_price) if sl_price is not None else 0.0,\n",
    "        \"tp\": float(tp_price) if tp_price is not None else 0.0,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"ppo_multiasset_live\",\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(req)\n",
    "    return res\n",
    "\n",
    "def close_position_by_ticket(ticket):\n",
    "    pos_list = mt5.positions_get(ticket=ticket)\n",
    "    if not pos_list:\n",
    "        return None\n",
    "    p = pos_list[0]\n",
    "    symbol = p.symbol\n",
    "    if getattr(p, \"type\", None) == 0:  # BUY => close with SELL\n",
    "        order_type = mt5.ORDER_TYPE_SELL\n",
    "        price = mt5.symbol_info_tick(symbol).bid\n",
    "    else:\n",
    "        order_type = mt5.ORDER_TYPE_BUY\n",
    "        price = mt5.symbol_info_tick(symbol).ask\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\":10009, \"comment\":\"DRY_RUN_CLOSE\", \"ticket\": ticket}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(p.volume),\n",
    "        \"type\": order_type,\n",
    "        \"position\": int(ticket),\n",
    "        \"price\": price,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"auto_close\",\n",
    "    }\n",
    "    return mt5.order_send(req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c7430a4-d973-4627-bf0e-443e4d44947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets):\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    acct = mt5.account_info()\n",
    "    balance = float(acct.balance) if acct else 10000.0\n",
    "\n",
    "    # iterate assets\n",
    "    for safe in safe_list:\n",
    "        raw = raw_from_safe(safe)   # MT5 symbol name guess\n",
    "        print(\"\\n---\", raw, \"(\", safe, \") ---\")\n",
    "        obs, vol, last_price = fetch_and_build_obs(raw, WINDOW, scalers, embeddings, datasets, safe_list)\n",
    "        if obs is None:\n",
    "            print(\"Skip\", raw)\n",
    "            continue\n",
    "\n",
    "        # model expects batch dim\n",
    "        try:\n",
    "            action, _ = model.predict(obs[np.newaxis, ...], deterministic=True)\n",
    "            a = int(action[0]) if isinstance(action, (list,tuple,np.ndarray)) else int(action)\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "            continue\n",
    "\n",
    "        print(\"Signal:\", a, \" (0=HOLD,1=BUY,2=SELL)\")\n",
    "\n",
    "        # get current positions for symbol\n",
    "        positions = get_positions_for_symbol(raw)\n",
    "        print(\"Existing positions:\", len(positions))\n",
    "\n",
    "        # Close opposing positions if reversal\n",
    "        if a == 1:\n",
    "            # BUY signal -> close SELL positions (type==1)\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 1:\n",
    "                    print(\"Closing opposing SELL ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "        elif a == 2:\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 0:\n",
    "                    print(\"Closing opposing BUY ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "\n",
    "        # refresh positions\n",
    "        positions = get_positions_for_symbol(raw)\n",
    "        if len(positions) >= MAX_POS_PER_SYMBOL:\n",
    "            print(\"Max positions reached for\", raw)\n",
    "        else:\n",
    "            if a == 0:\n",
    "                print(\"HOLD ‚Äî no trade\")\n",
    "            else:\n",
    "                direction = \"BUY\" if a==1 else \"SELL\"\n",
    "                lot = compute_lot_fixed(DEFAULT_LOT)\n",
    "                # estimate SL pips from volatility, then compute TP = 3 * SL\n",
    "                sl_pips = estimate_sl_pips_from_vol(raw, last_price, vol)\n",
    "                tp_pips = int(sl_pips * TP_MULT)\n",
    "                # convert pips to price levels\n",
    "                if \"JPY\" in raw.upper():\n",
    "                    pip = 0.01\n",
    "                else:\n",
    "                    pip = 0.0001\n",
    "                if direction == \"BUY\":\n",
    "                    sl_price = last_price - sl_pips * pip\n",
    "                    tp_price = last_price + tp_pips * pip\n",
    "                else:\n",
    "                    sl_price = last_price + sl_pips * pip\n",
    "                    tp_price = last_price - tp_pips * pip\n",
    "\n",
    "                res = place_market_order(raw, direction, lot, sl_price, tp_price)\n",
    "                # normalize response\n",
    "                if isinstance(res, dict):\n",
    "                    retcode = res.get(\"retcode\")\n",
    "                    comment = res.get(\"comment\")\n",
    "                    exec_price = res.get(\"price\", last_price)\n",
    "                else:\n",
    "                    retcode = getattr(res, \"retcode\", None)\n",
    "                    comment = getattr(res, \"comment\", \"\")\n",
    "                    exec_price = last_price\n",
    "\n",
    "                entry = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"safe\": safe,\n",
    "                    \"symbol\": raw,\n",
    "                    \"action\": direction,\n",
    "                    \"lot\": lot,\n",
    "                    \"exec_price\": exec_price,\n",
    "                    \"sl_price\": sl_price,\n",
    "                    \"tp_price\": tp_price,\n",
    "                    \"sl_pips\": sl_pips,\n",
    "                    \"tp_pips\": tp_pips,\n",
    "                    \"retcode\": retcode,\n",
    "                    \"comment\": comment,\n",
    "                    \"dry_run\": DRY_RUN\n",
    "                }\n",
    "                pd.DataFrame([entry]).to_csv(LOG_FILE, mode=\"a\", index=False, header=header)\n",
    "                header = False\n",
    "                print(\"Placed\", direction, \"lot\", lot, \"retcode\", retcode)\n",
    "\n",
    "        # Trailing: update SL for existing positions if profitable (simple rule)\n",
    "        for p in get_positions_for_symbol(raw):\n",
    "            # compute trailing SL target in price; use trailing pips = max(5, sl_pips//2) as example\n",
    "            try:\n",
    "                trail_pips = max(5, int(estimate_sl_pips_from_vol(raw, last_price, vol) // 2))\n",
    "            except Exception:\n",
    "                trail_pips = 5\n",
    "            if \"JPY\" in raw.upper():\n",
    "                pip = 0.01\n",
    "            else:\n",
    "                pip = 0.0001\n",
    "            if getattr(p,\"type\",None) == 0:  # BUY\n",
    "                # new SL = current price - trail_pips\n",
    "                cur = mt5.symbol_info_tick(raw).bid\n",
    "                new_sl = float(cur - trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "            else:  # SELL\n",
    "                cur = mt5.symbol_info_tick(raw).ask\n",
    "                new_sl = float(cur + trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "\n",
    "    print(\"\\nSingle pass complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156225a-ebe3-4995-b38d-f08a5f6b5be2",
   "metadata": {},
   "source": [
    "# ================================================================\n",
    "# Cell 8 ‚Äî run_once_predict_and_manage (fixed)\n",
    "# ================================================================\n",
    "def run_once_predict_and_manage(model, safe_list, scalers, embeddings):\n",
    "    \"\"\"\n",
    "    Loop over all assets, predict, place orders, auto-close on reversal,\n",
    "    enforce MAX_POS_PER_SYMBOL, apply trailing SL.\n",
    "    \"\"\"\n",
    "    # Account info\n",
    "    acct = mt5.account_info()\n",
    "    balance = float(acct.balance) if acct else 10000.0\n",
    "\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "\n",
    "    for safe_name in safe_list:\n",
    "        raw_symbol = get_mt5_symbol(safe_name)\n",
    "        print(f\"\\n--- {raw_symbol} ({safe_name}) ---\")\n",
    "\n",
    "        obs, vol, last_price = fetch_and_build_obs(safe_name, WINDOW, scalers, embeddings, safe_list)\n",
    "        if obs is None:\n",
    "            print(f\"Skip {safe_name}\")\n",
    "            continue\n",
    "\n",
    "        # Predict\n",
    "        try:\n",
    "            action, _ = model.predict(obs[np.newaxis,...], deterministic=True)\n",
    "            a = int(action[0]) if isinstance(action,(list,tuple,np.ndarray)) else int(action)\n",
    "            print(\"Signal:\", a, \"(0=HOLD,1=BUY,2=SELL)\")\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "            continue\n",
    "\n",
    "        # Get current positions\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        print(\"Existing positions:\", len(positions))\n",
    "\n",
    "        # Auto-close opposing positions\n",
    "        if a == 1:\n",
    "            for p in positions:\n",
    "                if getattr(p, \"type\", None) == 1:  # SELL\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "        elif a == 2:\n",
    "            for p in positions:\n",
    "                if getattr(p, \"type\", None) == 0:  # BUY\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "\n",
    "        # Re-fetch positions & enforce max\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        if len(positions) >= MAX_POS_PER_SYMBOL:\n",
    "            print(f\"Max positions reached for {raw_symbol} ({len(positions)})\")\n",
    "        else:\n",
    "            if a == 0:\n",
    "                print(\"HOLD\")\n",
    "            else:\n",
    "                direction = \"BUY\" if a==1 else \"SELL\"\n",
    "                lot = compute_lot_from_balance(balance, vol, last_price)\n",
    "                sl, tp = compute_sl_tp_by_pips(raw_symbol, last_price, direction, DEFAULT_SL_PIPS, DEFAULT_TP_PIPS)\n",
    "                res = place_market_order(raw_symbol, direction, lot, sl, tp)\n",
    "                # Log trade\n",
    "                if isinstance(res, dict):\n",
    "                    retcode = res.get(\"retcode\")\n",
    "                    comment = res.get(\"comment\")\n",
    "                    price_executed = res.get(\"price\", last_price)\n",
    "                else:\n",
    "                    retcode = getattr(res,\"retcode\",None)\n",
    "                    comment = getattr(res,\"comment\",\"\")\n",
    "                    price_executed = last_price\n",
    "                entry = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"safe\": safe_name,\n",
    "                    \"symbol\": raw_symbol,\n",
    "                    \"action\": direction,\n",
    "                    \"lot\": lot,\n",
    "                    \"exec_price\": price_executed,\n",
    "                    \"sl\": sl,\n",
    "                    \"tp\": tp,\n",
    "                    \"retcode\": retcode,\n",
    "                    \"comment\": comment,\n",
    "                    \"dry_run\": DRY_RUN\n",
    "                }\n",
    "                pd.DataFrame([entry]).to_csv(LOG_FILE, mode=\"a\", index=False, header=header)\n",
    "                header = False\n",
    "                print(f\"Placed {direction} lot {lot} retcode {retcode}\")\n",
    "\n",
    "        # Trailing SL\n",
    "        for p in get_positions_for_symbol(raw_symbol):\n",
    "            new_sl = trailing_sl_level(raw_symbol, p, TRAIL_PIPS)\n",
    "            if new_sl:\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would modify SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\n",
    "                        \"action\": mt5.TRADE_ACTION_SLTP,\n",
    "                        \"symbol\": raw_symbol,\n",
    "                        \"position\": int(p.ticket),\n",
    "                        \"sl\": new_sl,\n",
    "                        \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0\n",
    "                    }\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\", None))\n",
    "\n",
    "    print(\"\\nSingle pass complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "935c76d6-5cca-4990-aa44-6d976340597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets to process: ['EURUSD', 'Jump_100_Index', 'Jump_10_Index', 'Jump_25_Index', 'Jump_50_Index', 'Jump_75_Index', 'Volatility_100_1s_Index', 'Volatility_100_Index', 'Volatility_10_1s_Index', 'Volatility_10_Index', 'Volatility_25_1s_Index', 'Volatility_25_Index', 'Volatility_50_1s_Index', 'Volatility_50_Index', 'Volatility_75_1s_Index', 'Volatility_75_Index']\n",
      "\n",
      "--- EURUSD ( EURUSD ) ---\n",
      "Signal: 2  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for EURUSD\n",
      "Placed SELL lot 0.1 retcode None\n",
      "\n",
      "--- Jump 100 Index ( Jump_100_Index ) ---\n",
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Jump 100 Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Jump 10 Index ( Jump_10_Index ) ---\n",
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Jump 10 Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Jump 25 Index ( Jump_25_Index ) ---\n",
      "Signal: 0  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "HOLD ‚Äî no trade\n",
      "\n",
      "--- Jump 50 Index ( Jump_50_Index ) ---\n",
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Jump 50 Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Jump 75 Index ( Jump_75_Index ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Jump 75 Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 100 1s Index ( Volatility_100_1s_Index ) ---\n",
      "Signal: 2  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 100 1s Index\n",
      "Placed SELL lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 100 Index ( Volatility_100_Index ) ---\n",
      "Signal: 2  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 100 Index\n",
      "Placed SELL lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 10 1s Index ( Volatility_10_1s_Index ) ---\n",
      "Signal: 2  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 10 1s Index\n",
      "Placed SELL lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 10 Index ( Volatility_10_Index ) ---\n",
      "Signal: 2  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 10 Index\n",
      "Placed SELL lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 25 1s Index ( Volatility_25_1s_Index ) ---\n",
      "Signal: 0  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "HOLD ‚Äî no trade\n",
      "\n",
      "--- Volatility 25 Index ( Volatility_25_Index ) ---\n",
      "Signal: 0  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "HOLD ‚Äî no trade\n",
      "\n",
      "--- Volatility 50 1s Index ( Volatility_50_1s_Index ) ---\n",
      "Signal: 0  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "HOLD ‚Äî no trade\n",
      "\n",
      "--- Volatility 50 Index ( Volatility_50_Index ) ---\n",
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 50 Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 75 1s Index ( Volatility_75_1s_Index ) ---\n",
      "Signal: 1  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "‚ùå Cannot get tick for Volatility 75 1s Index\n",
      "Placed BUY lot 0.1 retcode None\n",
      "\n",
      "--- Volatility 75 Index ( Volatility_75_Index ) ---\n",
      "Signal: 0  (0=HOLD,1=BUY,2=SELL)\n",
      "Existing positions: 0\n",
      "HOLD ‚Äî no trade\n",
      "\n",
      "Single pass complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Slick\\AppData\\Local\\Temp\\ipykernel_11712\\2806280530.py:80: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "# Cell 10\n",
    "safe_list = sorted(datasets.keys())\n",
    "print(\"Assets to process:\", safe_list)\n",
    "run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7bed311-5520-44a5-9915-43022fff4005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick simulated backtest (may take a while)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# run quick sim\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning quick simulated backtest (may take a while)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m df_trades = simulate_trades_on_history(datasets, model, assets=safe_list, window=WINDOW, horizon=\u001b[32m10\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSimulated trades:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_trades))\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_trades.empty:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msimulate_trades_on_history\u001b[39m\u001b[34m(datasets, model, assets, window, horizon)\u001b[39m\n\u001b[32m     18\u001b[39m obs = np.concatenate([feat_scaled, balance_col, emb_rep], axis=\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     action, _ = model.predict(obs[np.newaxis,...], deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m     action = \u001b[38;5;28mint\u001b[39m(action[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action,(\u001b[38;5;28mlist\u001b[39m,\u001b[38;5;28mtuple\u001b[39m,np.ndarray)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(action)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:557\u001b[39m, in \u001b[36mBaseAlgorithm.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    538\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    539\u001b[39m     observation: Union[np.ndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m     deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    543\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np.ndarray, ...]]]:\n\u001b[32m    544\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m \u001b[33;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.policy.predict(observation, state, episode_start, deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[39m, in \u001b[36mBasePolicy.predict\u001b[39m\u001b[34m(self, observation, state, episode_start, deterministic)\u001b[39m\n\u001b[32m    365\u001b[39m obs_tensor, vectorized_env = \u001b[38;5;28mself\u001b[39m.obs_to_tensor(observation)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     actions = \u001b[38;5;28mself\u001b[39m._predict(obs_tensor, deterministic=deterministic)\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[32m    370\u001b[39m actions = actions.cpu().numpy().reshape((-\u001b[32m1\u001b[39m, *\u001b[38;5;28mself\u001b[39m.action_space.shape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[39m, in \u001b[36mActorCriticPolicy._predict\u001b[39m\u001b[34m(self, observation, deterministic)\u001b[39m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> th.Tensor:\n\u001b[32m    710\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    711\u001b[39m \u001b[33;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[32m    712\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    715\u001b[39m \u001b[33;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[32m    716\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_distribution(observation).get_actions(deterministic=deterministic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:751\u001b[39m, in \u001b[36mActorCriticPolicy.get_distribution\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03mGet the current policy distribution given the observations.\u001b[39;00m\n\u001b[32m    746\u001b[39m \n\u001b[32m    747\u001b[39m \u001b[33;03m:param obs:\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[33;03m:return: the action distribution.\u001b[39;00m\n\u001b[32m    749\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    750\u001b[39m features = \u001b[38;5;28msuper\u001b[39m().extract_features(obs, \u001b[38;5;28mself\u001b[39m.pi_features_extractor)\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m latent_pi = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_actor(features)\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_action_dist_from_latent(latent_pi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:260\u001b[39m, in \u001b[36mMlpExtractor.forward_actor\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th.Tensor) -> th.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.policy_net(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\users\\slick\\Desktop\\ml\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 11 (optional)\n",
    "def simulate_trades_on_history(datasets, model, assets=None, window=WINDOW, horizon=10):\n",
    "    trades = []\n",
    "    assets = assets or list(datasets.keys())\n",
    "    for safe in assets:\n",
    "        df = datasets[safe].reset_index(drop=True)\n",
    "        emb = embeddings.get(safe, np.zeros((0,)))\n",
    "        for i in range(window, len(df)-horizon):\n",
    "            window_df = df.iloc[i-window:i]\n",
    "            feat = window_df[['o_pc','h_pc','l_pc','c_pc','v_pc']].values.astype(np.float32)\n",
    "            # scale manually as above\n",
    "            mean = scalers[safe]['mean'].values\n",
    "            std = scalers[safe]['std'].values\n",
    "            std = np.where(std==0,1e-8,std)\n",
    "            feat_scaled = (feat - mean) / std\n",
    "            emb_rep = np.tile(emb.reshape(1,-1),(window,1)) if emb.size>0 else np.zeros((window,0))\n",
    "            balance_col = np.full((window,1),1.0,dtype=np.float32)\n",
    "            obs = np.concatenate([feat_scaled, balance_col, emb_rep], axis=1)\n",
    "            try:\n",
    "                action, _ = model.predict(obs[np.newaxis,...], deterministic=True)\n",
    "                action = int(action[0]) if isinstance(action,(list,tuple,np.ndarray)) else int(action)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if action == 0: continue\n",
    "            entry_price = float(df['Close_raw'].iat[i-1])\n",
    "            exit_price = float(df['Close_raw'].iat[i+horizon-1])\n",
    "            pos = 1 if action==1 else -1\n",
    "            pnl = (exit_price - entry_price)/entry_price * pos\n",
    "            trades.append({\"safe\": safe, \"entry_i\": i, \"horizon\": horizon, \"action\":action, \"pnl\":pnl})\n",
    "    return pd.DataFrame(trades)\n",
    "\n",
    "# run quick sim\n",
    "print(\"Running quick simulated backtest (may take a while)...\")\n",
    "df_trades = simulate_trades_on_history(datasets, model, assets=safe_list, window=WINDOW, horizon=10)\n",
    "print(\"Simulated trades:\", len(df_trades))\n",
    "if not df_trades.empty:\n",
    "    print(\"Total PnL:\", df_trades['pnl'].sum(), \"Win rate:\", (df_trades['pnl']>0).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199affb-ac5f-4d70-981e-31231fdca96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "if os.path.exists(LOG_FILE):\n",
    "    df_log = pd.read_csv(LOG_FILE)\n",
    "    display(df_log.tail(20))\n",
    "else:\n",
    "    print(\"No live logs yet at\", LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe67fd-a259-40f6-acc6-d0173f8c4b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
