{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea968b1-204d-4b1b-a2ff-cf791dc86398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ Cell 1 â€” Imports\n",
    "# ================================================================\n",
    "import os, glob, time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e2cf19-c437-4848-b4e1-cdcd442ffd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ Cell 1 â€” Evaluation settings\n",
    "# ================================================================\n",
    "DRY_RUN = False             # Set True for no real trades\n",
    "WINDOW = 50                 # same as training\n",
    "# === PATHS (from your config) ===\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "METRICS_FILE = os.path.join(MODEL_DIR, \"eval_metrics.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6a7815-e44b-43c3-8b0a-6245a37f7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Timeframe mapping\n",
    "# --------------------------------------\n",
    "TIMEFRAME = \"M15\"\n",
    "TF_MAP = {\n",
    "    \"M1\": mt5.TIMEFRAME_M1,\n",
    "    \"M5\": mt5.TIMEFRAME_M5,\n",
    "    \"M15\": mt5.TIMEFRAME_M15,\n",
    "    \"M30\": mt5.TIMEFRAME_M30,\n",
    "    \"H1\": mt5.TIMEFRAME_H1,\n",
    "    \"H4\": mt5.TIMEFRAME_H4,\n",
    "    \"D1\": mt5.TIMEFRAME_D1\n",
    "}\n",
    "tf = TF_MAP[TIMEFRAME.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b4378f-8d0c-43c1-b949-2be12be65698",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LOT = 0.1           # as requested\n",
    "MAX_POS_PER_SYMBOL = 2\n",
    "\n",
    "# SL/TP settings: SL computed from volatility (function below); TP = 3 * SL\n",
    "DEFAULT_SL_PIPS_FALLBACK = 20\n",
    "TP_MULT = 3\n",
    "\n",
    "WINDOW = 50\n",
    "BUFFER = 60\n",
    "COUNT = WINDOW + BUFFER   # extra bars for safety\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"live_trade_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d07d08e-9c43-47d7-8bdd-e5db064c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "#def raw_from_safe(safe: str) -> str:\n",
    " #   return safe.replace(\"_\", \" \")\n",
    "\n",
    "def safe_from_raw(raw: str) -> str:\n",
    "    return make_safe_name(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7b7c48-d209-49c2-a642-f53f7d9928c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# âš™ï¸ Cell 4 - Symbols Selection\n",
    "# ================================================================\n",
    "SYMBOLS = [\n",
    "    # --- Volatility Indices (Standard) ---\n",
    "    \"Volatility 10 Index\",\n",
    "    \"Volatility 25 Index\",\n",
    "    \"Volatility 50 Index\",\n",
    "    \"Volatility 75 Index\",\n",
    "    \"Volatility 100 Index\",\n",
    "\n",
    "    # --- Volatility 1s Indices ---\n",
    "    \"Volatility 10 (1s) Index\",\n",
    "    \"Volatility 25 (1s) Index\",\n",
    "    \"Volatility 50 (1s) Index\",\n",
    "    \"Volatility 75 (1s) Index\",\n",
    "    \"Volatility 100 (1s) Index\",\n",
    "\n",
    "    # --- Volatility 10s Indices ---\n",
    "    \"Volatility 10 (10s) Index\",\n",
    "    \"Volatility 25 (10s) Index\",\n",
    "    \"Volatility 50 (10s) Index\",\n",
    "    \"Volatility 75 (10s) Index\",\n",
    "    \"Volatility 100 (10s) Index\",\n",
    "\n",
    "    # --- Jump Indices ---\n",
    "    \"Jump 10 Index\",\n",
    "    \"Jump 25 Index\",\n",
    "    \"Jump 50 Index\",\n",
    "    \"Jump 75 Index\",\n",
    "    \"Jump 100 Index\",\n",
    "\n",
    "    # --- Step Indices ---\n",
    "    \"Step Index 25\",\n",
    "    \"Step Index 50\",\n",
    "    \"Step Index 75\",\n",
    "    \"Step Index 100\",\n",
    "\n",
    "    # --- Forex Reference ---\n",
    "    \"EURUSD\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe6e769-a7fd-4bda-b176-6b24f6bcf440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¡ Fetching live data for: Volatility 10 Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 10 Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 25 Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 25 Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 50 Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 50 Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 75 Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 75 Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 100 Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 100 Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 10 (1s) Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 10 (1s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 25 (1s) Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 25 (1s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 50 (1s) Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 50 (1s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 75 (1s) Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 75 (1s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 100 (1s) Index\n",
      "âœ”ï¸ Loaded 110 bars for Volatility 100 (1s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 10 (10s) Index\n",
      "âŒ Symbol not found in MT5: Volatility 10 (10s) Index\n",
      "âš ï¸ Failed to load data for Volatility 10 (10s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 25 (10s) Index\n",
      "âŒ Symbol not found in MT5: Volatility 25 (10s) Index\n",
      "âš ï¸ Failed to load data for Volatility 25 (10s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 50 (10s) Index\n",
      "âŒ Symbol not found in MT5: Volatility 50 (10s) Index\n",
      "âš ï¸ Failed to load data for Volatility 50 (10s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 75 (10s) Index\n",
      "âŒ Symbol not found in MT5: Volatility 75 (10s) Index\n",
      "âš ï¸ Failed to load data for Volatility 75 (10s) Index\n",
      "ğŸ“¡ Fetching live data for: Volatility 100 (10s) Index\n",
      "âŒ Symbol not found in MT5: Volatility 100 (10s) Index\n",
      "âš ï¸ Failed to load data for Volatility 100 (10s) Index\n",
      "ğŸ“¡ Fetching live data for: Jump 10 Index\n",
      "âœ”ï¸ Loaded 110 bars for Jump 10 Index\n",
      "ğŸ“¡ Fetching live data for: Jump 25 Index\n",
      "âœ”ï¸ Loaded 110 bars for Jump 25 Index\n",
      "ğŸ“¡ Fetching live data for: Jump 50 Index\n",
      "âœ”ï¸ Loaded 110 bars for Jump 50 Index\n",
      "ğŸ“¡ Fetching live data for: Jump 75 Index\n",
      "âœ”ï¸ Loaded 110 bars for Jump 75 Index\n",
      "ğŸ“¡ Fetching live data for: Jump 100 Index\n",
      "âœ”ï¸ Loaded 110 bars for Jump 100 Index\n",
      "ğŸ“¡ Fetching live data for: Step Index 25\n",
      "âŒ Symbol not found in MT5: Step Index 25\n",
      "âš ï¸ Failed to load data for Step Index 25\n",
      "ğŸ“¡ Fetching live data for: Step Index 50\n",
      "âŒ Symbol not found in MT5: Step Index 50\n",
      "âš ï¸ Failed to load data for Step Index 50\n",
      "ğŸ“¡ Fetching live data for: Step Index 75\n",
      "âŒ Symbol not found in MT5: Step Index 75\n",
      "âš ï¸ Failed to load data for Step Index 75\n",
      "ğŸ“¡ Fetching live data for: Step Index 100\n",
      "âŒ Symbol not found in MT5: Step Index 100\n",
      "âš ï¸ Failed to load data for Step Index 100\n",
      "ğŸ“¡ Fetching live data for: EURUSD\n",
      "âœ”ï¸ Loaded 110 bars for EURUSD\n",
      "\n",
      "âœ… Completed fetching live data for 16 symbols.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# Fetch bars for each symbol individually\n",
    "# --------------------------------------\n",
    "def fetch_symbol_bars(symbol):\n",
    "    \"\"\"Fetch bars safely for one symbol.\"\"\"\n",
    "    # Ensure MT5 knows this symbol\n",
    "\n",
    "    mt5.initialize()\n",
    "    \n",
    "    info = mt5.symbol_info(symbol)\n",
    "    if info is None:\n",
    "        print(f\"âŒ Symbol not found in MT5: {symbol}\")\n",
    "        return None\n",
    "\n",
    "    if not info.visible:\n",
    "        mt5.symbol_select(symbol, True)\n",
    "\n",
    "    bars = mt5.copy_rates_from_pos(symbol, tf, 0, COUNT)\n",
    "\n",
    "    if bars is None or len(bars) < WINDOW + 2:\n",
    "        print(f\"âŒ Not enough bars for {symbol} (got {0 if bars is None else len(bars)})\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    mt5.shutdown()\n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch all symbols into a dictionary\n",
    "# --------------------------------------\n",
    "symbol_data = {}\n",
    "\n",
    "for sym in SYMBOLS:\n",
    "    print(f\"ğŸ“¡ Fetching live data for: {sym}\")\n",
    "    df = fetch_symbol_bars(sym)\n",
    "    if df is not None:\n",
    "        symbol_data[sym] = df\n",
    "        print(f\"âœ”ï¸ Loaded {len(df)} bars for {sym}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Failed to load data for {sym}\")\n",
    "\n",
    "print(f\"\\nâœ… Completed fetching live data for {len(symbol_data)} symbols.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48d7654d-ec48-473c-abe9-c05ee747bb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Loaded PPO model\n",
      "âš  VecNormalize could not load â€” continuing without it.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâš  VecNormalize could not load â€” continuing without it.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Load embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m embeddings = np.load(EMBED_FILE, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m).item()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ” Loaded embeddings:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(embeddings.keys()))\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Load asset index map\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ Cell 2 â€” Load PPO, Embeddings, Asset Map\n",
    "# ================================================================\n",
    "\n",
    "# Load PPO\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"âœ” Loaded PPO model\")\n",
    "\n",
    "# Try to load VecNormalize safely\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv=None)\n",
    "        print(\"âœ” Loaded VecNormalize\")\n",
    "    except:\n",
    "        print(\"âš  VecNormalize could not load â€” continuing without it.\")\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = np.load(EMBED_FILE, allow_pickle=True).item()\n",
    "print(\"âœ” Loaded embeddings:\", list(embeddings.keys()))\n",
    "\n",
    "# Load asset index map\n",
    "asset_map = pd.read_csv(ASSET_MAP_FILE)\n",
    "asset_to_idx = dict(zip(asset_map[\"asset\"], asset_map[\"index\"]))\n",
    "print(\"âœ” Loaded asset_to_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "985f726b-ddff-4d7e-93c2-d2ef702bbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded assets: 16\n",
      "Loaded scalers: 16\n",
      "Loaded embeddings: 17\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "# Load per-asset scalers saved as CSVs: {safe}_scaler.csv with columns mean/std\n",
    "def load_scalers_from_csv(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        # ensure ordering of mean/std columns matches our feature order: o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # ensure expected columns exist; otherwise try conversion as earlier notebooks did\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            # attempt auto-convert if original OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing expected columns and cannot convert\")\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found at\", embed_file)\n",
    "        return {}\n",
    "    emb = np.load(embed_file, allow_pickle=True)\n",
    "    # emb likely is ndarray shape (n_assets, embed_dim)\n",
    "    # Map using asset_to_idx.csv if present\n",
    "    emb_dict = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "        # am is mapping safe->idx\n",
    "        for safe, idx in am.items():\n",
    "            idx = int(idx)\n",
    "            if idx < emb.shape[0]:\n",
    "                emb_dict[safe] = np.array(emb[idx], dtype=np.float32)\n",
    "    else:\n",
    "        # fallback: map by order of normalized CSVs if sizes match\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i, safe in enumerate(safe_list):\n",
    "                emb_dict[safe] = np.array(emb[i], dtype=np.float32)\n",
    "        else:\n",
    "            print(\"Warning: embedding count and CSV count mismatch; manual mapping required.\")\n",
    "    return emb_dict\n",
    "\n",
    "# load\n",
    "scalers = load_scalers_from_csv(DATA_DIR)\n",
    "datasets = load_datasets(DATA_DIR, WINDOW)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "\n",
    "print(\"Loaded assets:\", len(datasets))\n",
    "print(\"Loaded scalers:\", len(scalers))\n",
    "print(\"Loaded embeddings:\", len(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e15e7b2b-fe4b-4051-a230-b6247b5e2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VecNormalize from models\\multiasset\\vec_normalize.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell: Safe VecNormalize loader using an inline dummy env (no external envs module required)\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# parameters that must match training\n",
    "WINDOW = 50                   # your window used in training\n",
    "N_PRICE_FEATURES = 5          # o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "EMBED_DIM = next(iter(embeddings.values())).shape[0] if len(embeddings)>0 else 8\n",
    "OBS_DIM = N_PRICE_FEATURES + 1 + EMBED_DIM   # matches: 5 + 1 + embed_dim\n",
    "\n",
    "class _DummyTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Minimal dummy env with the correct obs/action spaces for VecNormalize.load().\n",
    "    Used only to provide a 'venv' for VecNormalize.load(path, venv).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # observation: shape=(WINDOW, OBS_DIM)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(WINDOW, OBS_DIM), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(3)  # hold / buy / sell\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # return an observation of correct shape and empty info dict\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # return obs, reward, done, truncated, info in gymnasium style\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        reward = 0.0\n",
    "        done = True   # immediate done is fine for dummy\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "# Create vectorized dummy env with 1 env (can be >1)\n",
    "venv = DummyVecEnv([lambda: _DummyTradingEnv()])\n",
    "\n",
    "# Try loading VecNormalize with the dummy venv\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv)\n",
    "        # ensure it's in inference mode\n",
    "        vecnorm.training = False\n",
    "        vecnorm.norm_reward = False\n",
    "        print(\"Loaded VecNormalize from\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: VecNormalize.load failed:\", e)\n",
    "        vecnorm = None\n",
    "else:\n",
    "    print(\"No VecNormalize file found at\", VEC_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c28092-0391-4197-b3c2-c1830dd2cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ›  Cell 3 â€” Symbol name normalizer\n",
    "# ================================================================\n",
    "\n",
    "def safe_from_raw(symbol):\n",
    "    \"\"\"Match MT5 symbol name to training-safe name.\"\"\"\n",
    "    return symbol.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622a203d-5a10-4c61-8dbd-5bd624cedb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ“Š Cell 5 â€” Build Obs\n",
    "# ================================================================\n",
    "\n",
    "n_features = 5\n",
    "embed_dim = len(list(embeddings.values())[0])\n",
    "obs_dim = n_features + embed_dim\n",
    "\n",
    "def build_obs(df, symbol):\n",
    "    \"\"\"Convert MT5 DataFrame â†’ RL observation.\"\"\"\n",
    "    if len(df) < WINDOW:\n",
    "        return None\n",
    "    \n",
    "    window_df = df.iloc[-WINDOW:]\n",
    "\n",
    "    features = window_df[[\"open\", \"high\", \"low\", \"close\", \"tick_volume\"]].pct_change().fillna(0).values\n",
    "\n",
    "    embed = embeddings[symbol]\n",
    "    embed_rep = np.repeat(embed[np.newaxis, :], WINDOW, axis=0)\n",
    "\n",
    "    obs = np.hstack([features, embed_rep]).astype(np.float32)\n",
    "\n",
    "    if vecnorm:\n",
    "        obs = vecnorm.normalize_obs(obs)\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba269b55-d162-4523-8525-1a7104bb882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ’¹ Cell 6 â€” MT5 Safe Trading Wrapper\n",
    "# ================================================================\n",
    "\n",
    "def get_positions(symbol):\n",
    "    pos = mt5.positions_get(symbol=symbol)\n",
    "    return [] if pos is None else list(pos)\n",
    "\n",
    "def place_order(symbol, direction, lot, sl=None, tp=None):\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"âŒ No tick for:\", symbol)\n",
    "        return None\n",
    "\n",
    "    price = tick.ask if direction == \"BUY\" else tick.bid\n",
    "    order_type = mt5.ORDER_TYPE_BUY if direction == \"BUY\" else mt5.ORDER_TYPE_SELL\n",
    "\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\": 10009, \"comment\": \"DRY_RUN\"}\n",
    "\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": order_type,\n",
    "        \"price\": price,\n",
    "        \"sl\": sl or 0,\n",
    "        \"tp\": tp or 0,\n",
    "        \"magic\": 234000,\n",
    "        \"deviation\": 20,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK\n",
    "    }\n",
    "    return mt5.order_send(req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ba7db26-69ce-47f9-9891-ee979d07e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ¤– Cell 7 â€” Prediction + Trading Logic\n",
    "# ================================================================\n",
    "\n",
    "def run_once_predict_and_manage():\n",
    "    mt5.initialize()\n",
    "\n",
    "    for symbol, df in symbol_data.items():\n",
    "        print(f\"\\n--- {symbol} ---\")\n",
    "\n",
    "        obs = build_obs(df, symbol)\n",
    "        if obs is None:\n",
    "            print(\"âš  Not enough data\")\n",
    "            continue\n",
    "\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "        signal = int(action)\n",
    "        print(\"Signal:\", signal)\n",
    "\n",
    "        open_positions = get_positions(symbol)\n",
    "        print(\"Open positions:\", len(open_positions))\n",
    "\n",
    "        if signal == 0:\n",
    "            print(\"HOLD\")\n",
    "            continue\n",
    "\n",
    "        direction = \"BUY\" if signal == 1 else \"SELL\"\n",
    "\n",
    "        sl_pips = 0.1\n",
    "        tp_pips = sl_pips * 3\n",
    "\n",
    "        tick = mt5.symbol_info_tick(symbol)\n",
    "        price = tick.ask if direction == \"BUY\" else tick.bid\n",
    "\n",
    "        sl = price - sl_pips if direction == \"BUY\" else price + sl_pips\n",
    "        tp = price + tp_pips if direction == \"BUY\" else price - tp_pips\n",
    "\n",
    "        res = place_order(symbol, direction, 0.1, sl, tp)\n",
    "        print(\"Order result:\", res)\n",
    "\n",
    "    mt5.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b97339-7de1-4bb0-a111-b8ed3bf039c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Volatility 10 Index ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Volatility 10 Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# â–¶ï¸ Cell 8 â€” Execute\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m run_once_predict_and_manage()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mrun_once_predict_and_manage\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m symbol, df \u001b[38;5;129;01min\u001b[39;00m symbol_data.items():\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     obs = build_obs(df, symbol)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâš  Not enough data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mbuild_obs\u001b[39m\u001b[34m(df, symbol)\u001b[39m\n\u001b[32m     14\u001b[39m window_df = df.iloc[-WINDOW:]\n\u001b[32m     16\u001b[39m features = window_df[[\u001b[33m\"\u001b[39m\u001b[33mopen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtick_volume\u001b[39m\u001b[33m\"\u001b[39m]].pct_change().fillna(\u001b[32m0\u001b[39m).values\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m embed = embeddings[symbol]\n\u001b[32m     19\u001b[39m embed_rep = np.repeat(embed[np.newaxis, :], WINDOW, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     21\u001b[39m obs = np.hstack([features, embed_rep]).astype(np.float32)\n",
      "\u001b[31mKeyError\u001b[39m: 'Volatility 10 Index'"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# â–¶ï¸ Cell 8 â€” Execute\n",
    "# ================================================================\n",
    "run_once_predict_and_manage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868ebc5-6a0e-4c27-baa8-83d9d488c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ğŸ“¦ Cell 1 â€” Imports\n",
    "# ================================================================\n",
    "import os, glob, time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ“¦ Cell 1 â€” Evaluation settings\n",
    "# ================================================================\n",
    "DRY_RUN = False             # Set True for no real trades\n",
    "WINDOW = 50                 # same as training\n",
    "# === PATHS (from your config) ===\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "METRICS_FILE = os.path.join(MODEL_DIR, \"eval_metrics.json\")\n",
    "\n",
    "# --------------------------------------\n",
    "# Timeframe mapping\n",
    "# --------------------------------------\n",
    "TIMEFRAME = \"M15\"\n",
    "TF_MAP = {\n",
    "    \"M1\": mt5.TIMEFRAME_M1,\n",
    "    \"M5\": mt5.TIMEFRAME_M5,\n",
    "    \"M15\": mt5.TIMEFRAME_M15,\n",
    "    \"M30\": mt5.TIMEFRAME_M30,\n",
    "    \"H1\": mt5.TIMEFRAME_H1,\n",
    "    \"H4\": mt5.TIMEFRAME_H4,\n",
    "    \"D1\": mt5.TIMEFRAME_D1\n",
    "}\n",
    "tf = TF_MAP[TIMEFRAME.upper()]\n",
    "\n",
    "DEFAULT_LOT = 0.1           # as requested\n",
    "MAX_POS_PER_SYMBOL = 2\n",
    "\n",
    "# SL/TP settings: SL computed from volatility (function below); TP = 3 * SL\n",
    "DEFAULT_SL_PIPS_FALLBACK = 20\n",
    "TP_MULT = 3\n",
    "\n",
    "WINDOW = 50\n",
    "BUFFER = 60\n",
    "COUNT = WINDOW + BUFFER   # extra bars for safety\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"live_trade_logs.csv\")\n",
    "\n",
    "# Cell 3\n",
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "#def raw_from_safe(safe: str) -> str:\n",
    " #   return safe.replace(\"_\", \" \")\n",
    "\n",
    "def safe_from_raw(raw: str) -> str:\n",
    "    return make_safe_name(raw)\n",
    "\n",
    "# ================================================================\n",
    "# âš™ï¸ Cell 4 - Symbols Selection\n",
    "# ================================================================\n",
    "SYMBOLS = [\n",
    "    # --- Volatility Indices (Standard) ---\n",
    "    \"Volatility 10 Index\",\n",
    "    \"Volatility 25 Index\",\n",
    "    \"Volatility 50 Index\",\n",
    "    \"Volatility 75 Index\",\n",
    "    \"Volatility 100 Index\",\n",
    "\n",
    "    # --- Volatility 1s Indices ---\n",
    "    \"Volatility 10 (1s) Index\",\n",
    "    \"Volatility 25 (1s) Index\",\n",
    "    \"Volatility 50 (1s) Index\",\n",
    "    \"Volatility 75 (1s) Index\",\n",
    "    \"Volatility 100 (1s) Index\",\n",
    "\n",
    "    # --- Volatility 10s Indices ---\n",
    "    \"Volatility 10 (10s) Index\",\n",
    "    \"Volatility 25 (10s) Index\",\n",
    "    \"Volatility 50 (10s) Index\",\n",
    "    \"Volatility 75 (10s) Index\",\n",
    "    \"Volatility 100 (10s) Index\",\n",
    "\n",
    "    # --- Jump Indices ---\n",
    "    \"Jump 10 Index\",\n",
    "    \"Jump 25 Index\",\n",
    "    \"Jump 50 Index\",\n",
    "    \"Jump 75 Index\",\n",
    "    \"Jump 100 Index\",\n",
    "\n",
    "    # --- Step Indices ---\n",
    "    \"Step Index 25\",\n",
    "    \"Step Index 50\",\n",
    "    \"Step Index 75\",\n",
    "    \"Step Index 100\",\n",
    "\n",
    "    # --- Forex Reference ---\n",
    "    \"EURUSD\"\n",
    "]\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch bars for each symbol individually\n",
    "# --------------------------------------\n",
    "def fetch_symbol_bars(symbol):\n",
    "    \"\"\"Fetch bars safely for one symbol.\"\"\"\n",
    "    # Ensure MT5 knows this symbol\n",
    "\n",
    "    mt5.initialize()\n",
    "    \n",
    "    info = mt5.symbol_info(symbol)\n",
    "    if info is None:\n",
    "        print(f\"âŒ Symbol not found in MT5: {symbol}\")\n",
    "        return None\n",
    "\n",
    "    if not info.visible:\n",
    "        mt5.symbol_select(symbol, True)\n",
    "\n",
    "    bars = mt5.copy_rates_from_pos(symbol, tf, 0, COUNT)\n",
    "\n",
    "    if bars is None or len(bars) < WINDOW + 2:\n",
    "        print(f\"âŒ Not enough bars for {symbol} (got {0 if bars is None else len(bars)})\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    mt5.shutdown()\n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# Fetch all symbols into a dictionary\n",
    "# --------------------------------------\n",
    "symbol_data = {}\n",
    "\n",
    "for sym in SYMBOLS:\n",
    "    print(f\"ğŸ“¡ Fetching live data for: {sym}\")\n",
    "    df = fetch_symbol_bars(sym)\n",
    "    if df is not None:\n",
    "        symbol_data[sym] = df\n",
    "        print(f\"âœ”ï¸ Loaded {len(df)} bars for {sym}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Failed to load data for {sym}\")\n",
    "\n",
    "print(f\"\\nâœ… Completed fetching live data for {len(symbol_data)} symbols.\")\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ“¦ Cell 2 â€” Load PPO, Embeddings, Asset Map\n",
    "# ================================================================\n",
    "\n",
    "# Load PPO\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"âœ” Loaded PPO model\")\n",
    "\n",
    "# Try to load VecNormalize safely\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv=None)\n",
    "        print(\"âœ” Loaded VecNormalize\")\n",
    "    except:\n",
    "        print(\"âš  VecNormalize could not load â€” continuing without it.\")\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = np.load(EMBED_FILE, allow_pickle=True).item()\n",
    "print(\"âœ” Loaded embeddings:\", list(embeddings.keys()))\n",
    "\n",
    "# Load asset index map\n",
    "asset_map = pd.read_csv(ASSET_MAP_FILE)\n",
    "asset_to_idx = dict(zip(asset_map[\"asset\"], asset_map[\"index\"]))\n",
    "print(\"âœ” Loaded asset_to_idx\")\n",
    "\n",
    "# Cell 4\n",
    "# Load per-asset scalers saved as CSVs: {safe}_scaler.csv with columns mean/std\n",
    "def load_scalers_from_csv(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        # ensure ordering of mean/std columns matches our feature order: o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # ensure expected columns exist; otherwise try conversion as earlier notebooks did\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            # attempt auto-convert if original OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing expected columns and cannot convert\")\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found at\", embed_file)\n",
    "        return {}\n",
    "    emb = np.load(embed_file, allow_pickle=True)\n",
    "    # emb likely is ndarray shape (n_assets, embed_dim)\n",
    "    # Map using asset_to_idx.csv if present\n",
    "    emb_dict = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "        # am is mapping safe->idx\n",
    "        for safe, idx in am.items():\n",
    "            idx = int(idx)\n",
    "            if idx < emb.shape[0]:\n",
    "                emb_dict[safe] = np.array(emb[idx], dtype=np.float32)\n",
    "    else:\n",
    "        # fallback: map by order of normalized CSVs if sizes match\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i, safe in enumerate(safe_list):\n",
    "                emb_dict[safe] = np.array(emb[i], dtype=np.float32)\n",
    "        else:\n",
    "            print(\"Warning: embedding count and CSV count mismatch; manual mapping required.\")\n",
    "    return emb_dict\n",
    "\n",
    "# load\n",
    "scalers = load_scalers_from_csv(DATA_DIR)\n",
    "datasets = load_datasets(DATA_DIR, WINDOW)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "\n",
    "print(\"Loaded assets:\", len(datasets))\n",
    "print(\"Loaded scalers:\", len(scalers))\n",
    "print(\"Loaded embeddings:\", len(embeddings))\n",
    "\n",
    "# Cell: Safe VecNormalize loader using an inline dummy env (no external envs module required)\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "# parameters that must match training\n",
    "WINDOW = 50                   # your window used in training\n",
    "N_PRICE_FEATURES = 5          # o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "EMBED_DIM = next(iter(embeddings.values())).shape[0] if len(embeddings)>0 else 8\n",
    "OBS_DIM = N_PRICE_FEATURES + 1 + EMBED_DIM   # matches: 5 + 1 + embed_dim\n",
    "\n",
    "class _DummyTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Minimal dummy env with the correct obs/action spaces for VecNormalize.load().\n",
    "    Used only to provide a 'venv' for VecNormalize.load(path, venv).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # observation: shape=(WINDOW, OBS_DIM)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(WINDOW, OBS_DIM), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(3)  # hold / buy / sell\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # return an observation of correct shape and empty info dict\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # return obs, reward, done, truncated, info in gymnasium style\n",
    "        obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        reward = 0.0\n",
    "        done = True   # immediate done is fine for dummy\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "# Create vectorized dummy env with 1 env (can be >1)\n",
    "venv = DummyVecEnv([lambda: _DummyTradingEnv()])\n",
    "\n",
    "# Try loading VecNormalize with the dummy venv\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv)\n",
    "        # ensure it's in inference mode\n",
    "        vecnorm.training = False\n",
    "        vecnorm.norm_reward = False\n",
    "        print(\"Loaded VecNormalize from\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: VecNormalize.load failed:\", e)\n",
    "        vecnorm = None\n",
    "else:\n",
    "    print(\"No VecNormalize file found at\", VEC_FILE)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ›  Cell 3 â€” Symbol name normalizer\n",
    "# ================================================================\n",
    "\n",
    "def safe_from_raw(symbol):\n",
    "    \"\"\"Match MT5 symbol name to training-safe name.\"\"\"\n",
    "    return symbol.strip()\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ“Š Cell 5 â€” Build Obs\n",
    "# ================================================================\n",
    "\n",
    "n_features = 5\n",
    "embed_dim = len(list(embeddings.values())[0])\n",
    "obs_dim = n_features + embed_dim\n",
    "\n",
    "def build_obs(df, symbol):\n",
    "    \"\"\"Convert MT5 DataFrame â†’ RL observation.\"\"\"\n",
    "    if len(df) < WINDOW:\n",
    "        return None\n",
    "    \n",
    "    window_df = df.iloc[-WINDOW:]\n",
    "\n",
    "    features = window_df[[\"open\", \"high\", \"low\", \"close\", \"tick_volume\"]].pct_change().fillna(0).values\n",
    "\n",
    "    embed = embeddings[symbol]\n",
    "    embed_rep = np.repeat(embed[np.newaxis, :], WINDOW, axis=0)\n",
    "\n",
    "    obs = np.hstack([features, embed_rep]).astype(np.float32)\n",
    "\n",
    "    if vecnorm:\n",
    "        obs = vecnorm.normalize_obs(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ’¹ Cell 6 â€” MT5 Safe Trading Wrapper\n",
    "# ================================================================\n",
    "\n",
    "def get_positions(symbol):\n",
    "    pos = mt5.positions_get(symbol=symbol)\n",
    "    return [] if pos is None else list(pos)\n",
    "\n",
    "def place_order(symbol, direction, lot, sl=None, tp=None):\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"âŒ No tick for:\", symbol)\n",
    "        return None\n",
    "\n",
    "    price = tick.ask if direction == \"BUY\" else tick.bid\n",
    "    order_type = mt5.ORDER_TYPE_BUY if direction == \"BUY\" else mt5.ORDER_TYPE_SELL\n",
    "\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\": 10009, \"comment\": \"DRY_RUN\"}\n",
    "\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": order_type,\n",
    "        \"price\": price,\n",
    "        \"sl\": sl or 0,\n",
    "        \"tp\": tp or 0,\n",
    "        \"magic\": 234000,\n",
    "        \"deviation\": 20,\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK\n",
    "    }\n",
    "    return mt5.order_send(req)\n",
    "\n",
    "# ================================================================\n",
    "# ğŸ¤– Cell 7 â€” Prediction + Trading Logic\n",
    "# ================================================================\n",
    "\n",
    "def run_once_predict_and_manage():\n",
    "    mt5.initialize()\n",
    "\n",
    "    for symbol, df in symbol_data.items():\n",
    "        print(f\"\\n--- {symbol} ---\")\n",
    "\n",
    "        obs = build_obs(df, symbol)\n",
    "        if obs is None:\n",
    "            print(\"âš  Not enough data\")\n",
    "            continue\n",
    "\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "        signal = int(action)\n",
    "        print(\"Signal:\", signal)\n",
    "\n",
    "        open_positions = get_positions(symbol)\n",
    "        print(\"Open positions:\", len(open_positions))\n",
    "\n",
    "        if signal == 0:\n",
    "            print(\"HOLD\")\n",
    "            continue\n",
    "\n",
    "        direction = \"BUY\" if signal == 1 else \"SELL\"\n",
    "\n",
    "        sl_pips = 0.1\n",
    "        tp_pips = sl_pips * 3\n",
    "\n",
    "        tick = mt5.symbol_info_tick(symbol)\n",
    "        price = tick.ask if direction == \"BUY\" else tick.bid\n",
    "\n",
    "        sl = price - sl_pips if direction == \"BUY\" else price + sl_pips\n",
    "        tp = price + tp_pips if direction == \"BUY\" else price - tp_pips\n",
    "\n",
    "        res = place_order(symbol, direction, 0.1, sl, tp)\n",
    "        print(\"Order result:\", res)\n",
    "\n",
    "    mt5.shutdown()\n",
    "\n",
    "# ================================================================\n",
    "# â–¶ï¸ Cell 8 â€” Execute\n",
    "# ================================================================\n",
    "run_once_predict_and_manage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
