{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715153ff-1c31-4113-bf77-3f4936ccc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: uncomment to install missing libs in the notebook environment\n",
    "import sys\n",
    "# !{sys.executable} -m pip install MetaTrader5 stable-baselines3 gymnasium numpy pandas matplotlib joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a52618e-1d66-4e03-9663-d5cbfe45acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, time, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "\n",
    "# Paths (based on your earlier config)\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "METRICS_FILE = os.path.join(MODEL_DIR, \"eval_metrics.json\")\n",
    "\n",
    "# Runtime config\n",
    "WINDOW = 50\n",
    "TF_MT5 = mt5.TIMEFRAME_M1\n",
    "DRY_RUN = True               # KEEP True while testing\n",
    "DEFAULT_LOT = 0.1\n",
    "MAX_POS_PER_SYMBOL = 2\n",
    "\n",
    "# SL/TP: SL derived from volatility estimate; TP = TP_MULT * SL\n",
    "DEFAULT_SL_PIPS_FALLBACK = 20\n",
    "TP_MULT = 3\n",
    "\n",
    "# Trailing SL pips (will use half of SL estimation as applied)\n",
    "TRAIL_PIPS = 5\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"live_trade_logs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205ce50-6de3-4981-bb24-ed75bda5fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ⚙️ Cell 4 - Symbols Selection\n",
    "# ================================================================\n",
    "SYMBOLS = [\n",
    "    # --- Volatility Indices (Standard) ---\n",
    "    \"Volatility 10 Index\",\n",
    "    \"Volatility 25 Index\",\n",
    "    \"Volatility 50 Index\",\n",
    "    \"Volatility 75 Index\",\n",
    "    \"Volatility 100 Index\",\n",
    "\n",
    "    # --- Volatility 1s Indices ---\n",
    "    \"Volatility 10 (1s) Index\",\n",
    "    \"Volatility 25 (1s) Index\",\n",
    "    \"Volatility 50 (1s) Index\",\n",
    "    \"Volatility 75 (1s) Index\",\n",
    "    \"Volatility 100 (1s) Index\",\n",
    "\n",
    "    # --- Volatility 10s Indices ---\n",
    "    \"Volatility 10 (10s) Index\",\n",
    "    \"Volatility 25 (10s) Index\",\n",
    "    \"Volatility 50 (10s) Index\",\n",
    "    \"Volatility 75 (10s) Index\",\n",
    "    \"Volatility 100 (10s) Index\",\n",
    "\n",
    "    # --- Jump Indices ---\n",
    "    \"Jump 10 Index\",\n",
    "    \"Jump 25 Index\",\n",
    "    \"Jump 50 Index\",\n",
    "    \"Jump 75 Index\",\n",
    "    \"Jump 100 Index\",\n",
    "\n",
    "    # --- Step Indices ---\n",
    "    \"Step Index 25\",\n",
    "    \"Step Index 50\",\n",
    "    \"Step Index 75\",\n",
    "    \"Step Index 100\",\n",
    "\n",
    "    # --- Forex Reference ---\n",
    "    \"EURUSD\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9748a5-fd1b-44c3-b73d-5e90dbec9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "def raw_from_safe(safe: str) -> str:\n",
    "    return safe.replace(\"_\", \" \")\n",
    "\n",
    "def safe_from_raw(raw: str) -> str:\n",
    "    return make_safe_name(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d782c0-bb7c-4404-bec6-2b1ad470b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic mapping - start with likely mappings; you can expand/update this dict\n",
    "MANUAL_SYMBOL_MAP = {\n",
    "    \"EURUSD\": \"EURUSD\",\n",
    "    \"Jump_100_Index\": \"JUMP100\",\n",
    "    \"Jump_10_Index\": \"JUMP10\",\n",
    "    \"Jump_25_Index\": \"JUMP25\",\n",
    "    \"Jump_50_Index\": \"JUMP50\",\n",
    "    \"Jump_75_Index\": \"JUMP75\",\n",
    "    \"Volatility_100_1s_Index\": \"VOL100_1S\",\n",
    "    \"Volatility_100_Index\": \"VOL100\",\n",
    "    \"Volatility_10_1s_Index\": \"VOL10_1S\",\n",
    "    \"Volatility_10_Index\": \"VOL10\",\n",
    "    \"Volatility_25_1s_Index\": \"VOL25_1S\",\n",
    "    \"Volatility_25_Index\": \"VOL25\",\n",
    "    \"Volatility_50_1s_Index\": \"VOL50_1S\",\n",
    "    \"Volatility_50_Index\": \"VOL50\",\n",
    "    \"Volatility_75_1s_Index\": \"VOL75_1S\",\n",
    "    \"Volatility_75_Index\": \"VOL75\",\n",
    "    # Add more if needed\n",
    "}\n",
    "\n",
    "def get_mt5_symbol(safe_name: str) -> str:\n",
    "    \"\"\"Return broker symbol name for a safe_name using MANUAL_SYMBOL_MAP or fallback to spaces.\"\"\"\n",
    "    return MANUAL_SYMBOL_MAP.get(safe_name, raw_from_safe(safe_name))\n",
    "\n",
    "def fetch_data_for_symbol(safe_name: str, window: int = WINDOW):\n",
    "    \"\"\"\n",
    "    Fetch recent bars from MT5 for the broker symbol mapped from safe_name.\n",
    "    Returns DataFrame with columns open,high,low,close,volume,Close_raw\n",
    "    \"\"\"\n",
    "    raw_symbol = get_mt5_symbol(safe_name)\n",
    "    info = mt5.symbol_info(raw_symbol)\n",
    "    if info is None:\n",
    "        print(f\"❌ Symbol not found in MT5: {raw_symbol}\")\n",
    "        return None\n",
    "    # ensure visible\n",
    "    if not info.visible:\n",
    "        try:\n",
    "            mt5.symbol_select(raw_symbol, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    count = window + 60  # a buffer\n",
    "    bars = mt5.copy_rates_from_pos(raw_symbol, TF_MT5, 0, count)\n",
    "    if bars is None or len(bars) < window + 2:\n",
    "        print(f\"❌ Not enough bars for {raw_symbol} (got {0 if bars is None else len(bars)})\")\n",
    "        return None\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    if 'tick_volume' in df.columns:\n",
    "        df = df.rename(columns={'tick_volume': 'volume'})\n",
    "    df = df[['open','high','low','close','volume']].copy()\n",
    "    df['Close_raw'] = df['close']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09e58e-4b8a-4392-bf98-48636e608dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers: read per-asset CSVs created earlier (mean/std)\n",
    "def load_scalers_from_csv(data_dir=DATA_DIR):\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                print(\"Skipping\", p, \"- unexpected format\")\n",
    "                continue\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n",
    "\n",
    "def load_embeddings(embed_file=EMBED_FILE, data_dir=DATA_DIR):\n",
    "    if not os.path.exists(embed_file):\n",
    "        print(\"No embeddings file found:\", embed_file)\n",
    "        return {}\n",
    "    emb = np.load(embed_file, allow_pickle=True)\n",
    "    emb_dict = {}\n",
    "    if os.path.exists(ASSET_MAP_FILE):\n",
    "        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "        for safe, idx in am.items():\n",
    "            idx = int(idx)\n",
    "            if idx < emb.shape[0]:\n",
    "                emb_dict[safe] = np.array(emb[idx], dtype=np.float32)\n",
    "    else:\n",
    "        csvs = sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\")))\n",
    "        safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "        if len(safe_list) == emb.shape[0]:\n",
    "            for i, safe in enumerate(safe_list):\n",
    "                emb_dict[safe] = np.array(emb[i], dtype=np.float32)\n",
    "        else:\n",
    "            print(\"Warning: embedding count and CSV count mismatch\")\n",
    "    return emb_dict\n",
    "\n",
    "# Load them\n",
    "scalers = load_scalers_from_csv(DATA_DIR)\n",
    "datasets = load_datasets(DATA_DIR, WINDOW)\n",
    "embeddings = load_embeddings(EMBED_FILE, DATA_DIR)\n",
    "\n",
    "print(\"Loaded datasets:\", len(datasets), \"scalers:\", len(scalers), \"embeddings:\", len(embeddings))\n",
    "safe_list = sorted(datasets.keys())\n",
    "print(\"Assets:\", safe_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63d23d6-353d-4242-a19e-8e2efd91c6df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load PPO and VecNormalize\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(MODEL_FILE):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel file missing: \u001b[39m\u001b[33m\"\u001b[39m + MODEL_FILE)\n\u001b[32m      4\u001b[39m model = PPO.load(MODEL_FILE)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Load PPO and VecNormalize\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Model file missing: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded PPO model:\", MODEL_FILE)\n",
    "\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        vecnorm = VecNormalize.load(VEC_FILE)\n",
    "        print(\"Loaded VecNormalize:\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: VecNormalize load failed:\", e)\n",
    "        vecnorm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15854475-aea9-48ce-9611-16147c5b78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_build_obs(safe_name, window, scalers, embeddings, datasets, safe_list):\n",
    "    \"\"\"\n",
    "    Build observation consistent with training:\n",
    "    obs shape = (window, 5 + 1 + embed_dim)\n",
    "    where 5 = o_pc,h_pc,l_pc,c_pc,v_pc ; 1 = balance (fixed 1.0) ; embed_dim from embeddings.\n",
    "    \"\"\"\n",
    "    # prefer live bars, but fallback to preloaded dataset if fetch fails\n",
    "    df_live = fetch_data_for_symbol(safe_name, window)\n",
    "    if df_live is None:\n",
    "        # fallback to prepared dataset\n",
    "        if safe_name not in datasets:\n",
    "            print(\"No data available for\", safe_name)\n",
    "            return None, None, None\n",
    "        df = datasets[safe_name].iloc[-(window+1):].copy()\n",
    "        # reconstruct OHLCV in price-space from Close_raw if needed is not possible; but we expect dataset to contain pct-change already\n",
    "        # We'll use stored pct columns if present:\n",
    "        df_pct = df[['o_pc','h_pc','l_pc','c_pc','v_pc']].copy()\n",
    "        last_price = float(df['Close_raw'].iloc[-1])\n",
    "    else:\n",
    "        # compute pct-change from live OHLCV\n",
    "        df = df_live\n",
    "        df_pct = df[['open','high','low','close','volume']].pct_change().dropna()\n",
    "        if len(df_pct) < window:\n",
    "            print(\"Not enough pct rows for\", safe_name)\n",
    "            return None, None, None\n",
    "        df_pct = df_pct.tail(window)\n",
    "        last_price = float(df['Close_raw'].iloc[-1])\n",
    "\n",
    "    # when using infile dataset, df_pct above already length==window\n",
    "    if df_pct.shape[0] < window:\n",
    "        print(\"Window mismatch for\", safe_name)\n",
    "        return None, None, None\n",
    "\n",
    "    # get scaler\n",
    "    if safe_name not in scalers:\n",
    "        print(\"No scaler for\", safe_name)\n",
    "        return None, None, None\n",
    "    s = scalers[safe_name]\n",
    "    mean = np.array(s['mean'].values if hasattr(s['mean'], 'values') else s['mean'], dtype=np.float32)\n",
    "    std  = np.array(s['std'].values if hasattr(s['std'], 'values') else s['std'], dtype=np.float32)\n",
    "    std = np.where(std == 0, 1e-8, std)\n",
    "\n",
    "    # Ensure df_pct column order matches mean/std indices: scalers were saved with index names o_pc,h_pc,l_pc,c_pc,v_pc\n",
    "    cols = ['o_pc','h_pc','l_pc','c_pc','v_pc']\n",
    "    # If df_pct currently has original OHLCV names, convert order\n",
    "    if list(df_pct.columns) != cols:\n",
    "        # attempt to rename if current names are open,high,...\n",
    "        if all(c in df_pct.columns for c in ['open','high','low','close','volume']):\n",
    "            df_pct = df_pct[['open','high','low','close','volume']]\n",
    "        else:\n",
    "            # assume df_pct already has o_pc,h_pc...\n",
    "            pass\n",
    "\n",
    "    # get features array of shape (window,5)\n",
    "    features = df_pct[cols].values.astype(np.float32) if all(c in df_pct.columns for c in cols) else df_pct.values.astype(np.float32)\n",
    "    features_scaled = (features - mean) / std\n",
    "\n",
    "    # embedding\n",
    "    if safe_name in embeddings:\n",
    "        emb_vec = np.array(embeddings[safe_name], dtype=np.float32)\n",
    "    else:\n",
    "        emb_dim = next(iter(embeddings.values())).shape[0] if len(embeddings)>0 else 8\n",
    "        emb_vec = np.zeros(emb_dim, dtype=np.float32)\n",
    "    emb_block = np.repeat(emb_vec.reshape(1,-1), window, axis=0)\n",
    "\n",
    "    # balance column (used in training) - keep as ones\n",
    "    balance_col = np.ones((window,1), dtype=np.float32)\n",
    "\n",
    "    obs = np.concatenate([features_scaled, balance_col, emb_block], axis=1).astype(np.float32)\n",
    "\n",
    "    # If VecNormalize exists and matches expected flattened size, attempt to apply (vecnorm expects flattened obs in older SB3 versions)\n",
    "    if vecnorm is not None:\n",
    "        try:\n",
    "            flat = obs.reshape(1, -1)\n",
    "            mean_rms = getattr(vecnorm.obs_rms, \"mean\", None)\n",
    "            var_rms = getattr(vecnorm.obs_rms, \"var\", None)\n",
    "            if mean_rms is not None and var_rms is not None and len(mean_rms) == flat.shape[1]:\n",
    "                flat_norm = (flat - mean_rms) / np.sqrt(var_rms + 1e-8)\n",
    "                obs = flat_norm.reshape(obs.shape).astype(np.float32)\n",
    "            else:\n",
    "                # some SB3 versions provide normalize_obs\n",
    "                try:\n",
    "                    obs = vecnorm.normalize_obs(obs, False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(\"VecNormalize apply error:\", e)\n",
    "\n",
    "    # volatility estimate for SL/lot sizing\n",
    "    vol_est = float(df_pct['c_pc'].std()) if 'c_pc' in df_pct.columns else float(np.std(features[:,3]))\n",
    "\n",
    "    return obs, vol_est, last_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9af439-aee8-47a0-aaab-952c00d4c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip_value(symbol):\n",
    "    return 0.01 if \"JPY\" in symbol.upper() else 0.0001\n",
    "\n",
    "def estimate_sl_pips_from_vol(raw_symbol, last_price, vol_est, min_pips=5, max_pips=200):\n",
    "    if \"JPY\" in raw_symbol.upper():\n",
    "        pip = 0.01\n",
    "    else:\n",
    "        pip = 0.0001\n",
    "    abs_move = vol_est * last_price\n",
    "    if abs_move <= 0:\n",
    "        sl_pips = DEFAULT_SL_PIPS_FALLBACK\n",
    "    else:\n",
    "        sl_raw = abs_move / pip\n",
    "        sl_pips = float(np.clip(np.round(sl_raw * 1.5), min_pips, max_pips))\n",
    "    return int(sl_pips)\n",
    "\n",
    "def compute_lot_from_balance(balance, vol, price, risk_pct=0.005, min_lot=0.01, max_lot=1.0):\n",
    "    risk_amount = balance * risk_pct\n",
    "    vol = max(vol, 1e-8)\n",
    "    price_scale = 1000.0\n",
    "    lot = risk_amount / (vol * price_scale)\n",
    "    return float(max(min_lot, min(max_lot, round(lot, 2))))\n",
    "\n",
    "def compute_sl_tp_by_pips(symbol, price, direction, sl_pips, tp_pips):\n",
    "    pip = pip_value(symbol)\n",
    "    if direction == \"BUY\":\n",
    "        sl = price - sl_pips * pip\n",
    "        tp = price + tp_pips * pip\n",
    "    else:\n",
    "        sl = price + sl_pips * pip\n",
    "        tp = price - tp_pips * pip\n",
    "    return float(sl), float(tp)\n",
    "\n",
    "def get_positions_for_symbol(symbol):\n",
    "    pos = mt5.positions_get(symbol=symbol)\n",
    "    return [] if pos is None else list(pos)\n",
    "\n",
    "def place_market_order(symbol, direction, lot, sl_price=None, tp_price=None):\n",
    "    tick = mt5.symbol_info_tick(symbol)\n",
    "    if tick is None:\n",
    "        print(\"❌ Cannot get tick for\", symbol)\n",
    "        return {\"retcode\": None, \"comment\":\"NO_TICK\"}\n",
    "    price = float(tick.ask if direction==\"BUY\" else tick.bid)\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\": 10009, \"price\": price, \"comment\":\"DRY_RUN\", \"direction\":direction}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(lot),\n",
    "        \"type\": mt5.ORDER_TYPE_BUY if direction==\"BUY\" else mt5.ORDER_TYPE_SELL,\n",
    "        \"price\": price,\n",
    "        \"sl\": float(sl_price) if sl_price is not None else 0.0,\n",
    "        \"tp\": float(tp_price) if tp_price is not None else 0.0,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"ppo_multiasset_live\",\n",
    "        \"type_filling\": mt5.ORDER_FILLING_FOK,\n",
    "    }\n",
    "    res = mt5.order_send(req)\n",
    "    return res\n",
    "\n",
    "def close_position_by_ticket(ticket):\n",
    "    pos_list = mt5.positions_get(ticket=ticket)\n",
    "    if not pos_list:\n",
    "        return None\n",
    "    p = pos_list[0]\n",
    "    symbol = p.symbol\n",
    "    if getattr(p, \"type\", None) == 0:  # BUY => close with SELL\n",
    "        order_type = mt5.ORDER_TYPE_SELL\n",
    "        price = mt5.symbol_info_tick(symbol).bid\n",
    "    else:\n",
    "        order_type = mt5.ORDER_TYPE_BUY\n",
    "        price = mt5.symbol_info_tick(symbol).ask\n",
    "    if DRY_RUN:\n",
    "        return {\"retcode\":10009, \"comment\":\"DRY_RUN_CLOSE\", \"ticket\": ticket}\n",
    "    req = {\n",
    "        \"action\": mt5.TRADE_ACTION_DEAL,\n",
    "        \"symbol\": symbol,\n",
    "        \"volume\": float(p.volume),\n",
    "        \"type\": order_type,\n",
    "        \"position\": int(ticket),\n",
    "        \"price\": price,\n",
    "        \"deviation\": 20,\n",
    "        \"magic\": 234000,\n",
    "        \"comment\": \"auto_close\",\n",
    "    }\n",
    "    return mt5.order_send(req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0349e9da-d73f-46dd-a433-a78bd866dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets):\n",
    "    header = not os.path.exists(LOG_FILE)\n",
    "    acct = mt5.account_info()\n",
    "    balance = float(acct.balance) if acct else 10000.0\n",
    "\n",
    "    for safe_name in safe_list:\n",
    "        raw_symbol = get_mt5_symbol(safe_name)\n",
    "        print(f\"\\n--- {raw_symbol} ({safe_name}) ---\")\n",
    "        obs, vol, last_price = fetch_and_build_obs(safe_name, WINDOW, scalers, embeddings, datasets, safe_list)\n",
    "        if obs is None:\n",
    "            print(\"Skip\", safe_name)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            action, _ = model.predict(obs[np.newaxis,...], deterministic=True)\n",
    "            a = int(action[0]) if isinstance(action,(list,tuple,np.ndarray)) else int(action)\n",
    "            print(\"Signal:\", a)\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "            continue\n",
    "\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        print(\"Existing positions:\", len(positions))\n",
    "\n",
    "        # auto-close opposing\n",
    "        if a == 1:\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 1:\n",
    "                    print(\"Closing opposing SELL ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "        elif a == 2:\n",
    "            for p in positions:\n",
    "                if getattr(p,\"type\",None) == 0:\n",
    "                    print(\"Closing opposing BUY ticket\", p.ticket)\n",
    "                    close_position_by_ticket(p.ticket)\n",
    "\n",
    "        # refresh positions\n",
    "        positions = get_positions_for_symbol(raw_symbol)\n",
    "        if len(positions) >= MAX_POS_PER_SYMBOL:\n",
    "            print(\"Max positions reached, skipping open\")\n",
    "        else:\n",
    "            if a == 0:\n",
    "                print(\"HOLD\")\n",
    "            else:\n",
    "                direction = \"BUY\" if a==1 else \"SELL\"\n",
    "                lot = compute_lot_from_balance(balance, vol, last_price)\n",
    "                sl_pips = estimate_sl_pips_from_vol(raw_symbol, last_price, vol)\n",
    "                tp_pips = int(sl_pips * TP_MULT)\n",
    "                sl_price, tp_price = compute_sl_tp_by_pips(raw_symbol, last_price, direction, sl_pips, tp_pips)\n",
    "                res = place_market_order(raw_symbol, direction, lot, sl_price, tp_price)\n",
    "\n",
    "                if isinstance(res, dict):\n",
    "                    retcode = res.get(\"retcode\")\n",
    "                    comment = res.get(\"comment\")\n",
    "                    exec_price = res.get(\"price\", last_price)\n",
    "                else:\n",
    "                    retcode = getattr(res,\"retcode\", None)\n",
    "                    comment = getattr(res,\"comment\", \"\")\n",
    "                    exec_price = last_price\n",
    "\n",
    "                entry = {\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"safe\": safe_name,\n",
    "                    \"symbol\": raw_symbol,\n",
    "                    \"action\": direction,\n",
    "                    \"lot\": lot,\n",
    "                    \"exec_price\": exec_price,\n",
    "                    \"sl_price\": sl_price,\n",
    "                    \"tp_price\": tp_price,\n",
    "                    \"sl_pips\": sl_pips,\n",
    "                    \"tp_pips\": tp_pips,\n",
    "                    \"retcode\": retcode,\n",
    "                    \"comment\": comment,\n",
    "                    \"dry_run\": DRY_RUN\n",
    "                }\n",
    "                pd.DataFrame([entry]).to_csv(LOG_FILE, mode=\"a\", index=False, header=header)\n",
    "                header = False\n",
    "                print(\"Placed\", direction, \"lot\", lot, \"retcode\", retcode)\n",
    "\n",
    "        # trailing\n",
    "        for p in get_positions_for_symbol(raw_symbol):\n",
    "            # recompute vol-based trail\n",
    "            trail_pips = max(5, int(estimate_sl_pips_from_vol(raw_symbol, last_price, vol) // 2))\n",
    "            pip = pip_value(raw_symbol)\n",
    "            if getattr(p,\"type\",None) == 0:  # BUY\n",
    "                cur = mt5.symbol_info_tick(raw_symbol).bid\n",
    "                new_sl = float(cur - trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw_symbol, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "            else:  # SELL\n",
    "                cur = mt5.symbol_info_tick(raw_symbol).ask\n",
    "                new_sl = float(cur + trail_pips * pip)\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY] Would set trailing SL for ticket {p.ticket} -> {new_sl}\")\n",
    "                else:\n",
    "                    req = {\"action\": mt5.TRADE_ACTION_SLTP, \"symbol\": raw_symbol, \"position\": int(p.ticket), \"sl\": new_sl, \"tp\": float(p.tp) if getattr(p,\"tp\",None) else 0.0}\n",
    "                    r = mt5.order_send(req)\n",
    "                    print(\"Modify SL result:\", getattr(r,\"retcode\",None))\n",
    "\n",
    "    print(\"\\nSingle pass complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de81fed5-24d5-45d5-b35a-0a55aa53973e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_datasets\u001b[39m(data_dir=DATA_DIR, window=WINDOW):\n\u001b[32m      2\u001b[39m     datasets = {}\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(data_dir, \u001b[33m\"\u001b[39m\u001b[33m*_normalized.csv\u001b[39m\u001b[33m\"\u001b[39m))):\n",
      "\u001b[31mNameError\u001b[39m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "def load_datasets(data_dir=DATA_DIR, window=WINDOW):\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        # ensure expected columns exist; otherwise try conversion as earlier notebooks did\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            df = df[expected].dropna()\n",
    "        else:\n",
    "            # attempt auto-convert if original OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                df = tmp.dropna()\n",
    "            else:\n",
    "                raise ValueError(f\"{p} missing expected columns and cannot convert\")\n",
    "        if len(df) > window:\n",
    "            datasets[safe] = df\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c56baa-07d1-4b6c-b5c6-40544c7b70f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m datasets = load_datasets(data_dir=DATA_DIR, window=WINDOW)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(data_dir=DATA_DIR, window=WINDOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41147064-4a75-48e5-b7bd-c76a47d82e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m safe_list = \u001b[38;5;28msorted\u001b[39m(datasets.keys())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssets:\u001b[39m\u001b[33m\"\u001b[39m, safe_list)\n\u001b[32m      3\u001b[39m run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "safe_list = sorted(datasets.keys())\n",
    "print(\"Assets:\", safe_list)\n",
    "run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54fea9-5259-4165-b77b-d6f64778418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use with caution. Set DRY_RUN=False only after testing thoroughly.\n",
    "# try:\n",
    "#     while True:\n",
    "#         run_once_predict_and_manage(model, safe_list, scalers, embeddings, datasets)\n",
    "#         time.sleep(60)  # wait one minute (for M1)\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopped by user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ec0b5-9f5e-4672-8543-9ec78e0c5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(LOG_FILE):\n",
    "    df = pd.read_csv(LOG_FILE)\n",
    "    display(df.tail(30))\n",
    "else:\n",
    "    print(\"No logs yet at\", LOG_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
