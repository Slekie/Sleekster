{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02843706-884d-48be-bcf1-80bb8662a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MetaTrader5 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (5.0.5388)\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from stable-baselines3) (2.9.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\slick.aihs\\miniconda3\\lib\\site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - (optional) install runtime deps (uncomment if needed)\n",
    "%pip install MetaTrader5 stable-baselines3 gymnasium numpy pandas matplotlib joblib\n",
    "#!{sys.executable} -m pip install MetaTrader5 stable-baselines3 gymnasium numpy pandas matplotlib joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da085793-72c8-4650-9a44-a15024d965ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Imports\n",
    "import os, glob, time, traceback, json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MetaTrader5 as mt5\n",
    "import gymnasium as gym\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d148e9-c21c-4c74-bc33-69b75f10f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Config (edit paths & settings here)\n",
    "DRY_RUN = True            # SAFE DEFAULT: True => no real trades (set False only after testing)\n",
    "WINDOW = 50               # must match training window\n",
    "BUFFER = 60\n",
    "COUNT = WINDOW + BUFFER\n",
    "\n",
    "TIMEFRAME = \"M15\"         # \"M1\",\"M5\",\"M15\",\"M30\",\"H1\",\"H4\",\"D1\"\n",
    "TF_MAP = {\n",
    "    \"M1\": mt5.TIMEFRAME_M1, \"M5\": mt5.TIMEFRAME_M5, \"M15\": mt5.TIMEFRAME_M15,\n",
    "    \"M30\": mt5.TIMEFRAME_M30, \"H1\": mt5.TIMEFRAME_H1, \"H4\": mt5.TIMEFRAME_H4,\n",
    "    \"D1\": mt5.TIMEFRAME_D1\n",
    "}\n",
    "TF_MT5 = TF_MAP.get(TIMEFRAME.upper(), mt5.TIMEFRAME_M15)\n",
    "\n",
    "DATA_DIR = os.path.join(\"data\", \"multiasset\")\n",
    "MODEL_DIR = os.path.join(\"models\", \"multiasset\")\n",
    "EMBED_FILE = os.path.join(MODEL_DIR, \"asset_embeddings.npy\")\n",
    "ASSET_MAP_FILE = os.path.join(DATA_DIR, \"asset_to_idx.csv\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"ppo_multiasset.zip\")\n",
    "VEC_FILE = os.path.join(MODEL_DIR, \"vec_normalize.pkl\")\n",
    "\n",
    "LOG_DIR = os.path.join(MODEL_DIR, \"live_logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"live_trade_logs.csv\")\n",
    "\n",
    "DEFAULT_LOT = 0.1\n",
    "MAX_POS_PER_SYMBOL = 2\n",
    "TP_MULT = 3\n",
    "DEFAULT_SL_PIPS_FALLBACK = 20\n",
    "TRAIL_PIPS_DEFAULT = 5\n",
    "\n",
    "# Symbols list you trained on / want to trade (use broker exact names if needed)\n",
    "SYMBOLS = [\n",
    "    \"Volatility 10 Index\",\"Volatility 25 Index\",\"Volatility 50 Index\",\"Volatility 75 Index\",\"Volatility 100 Index\",\n",
    "    \"Volatility 10 (1s) Index\",\"Volatility 25 (1s) Index\",\"Volatility 50 (1s) Index\",\"Volatility 75 (1s) Index\",\"Volatility 100 (1s) Index\",\n",
    "    \"Volatility 10 (10s) Index\",\"Volatility 25 (10s) Index\",\"Volatility 50 (10s) Index\",\"Volatility 75 (10s) Index\",\"Volatility 100 (10s) Index\",\n",
    "    \"Jump 10 Index\",\"Jump 25 Index\",\"Jump 50 Index\",\"Jump 75 Index\",\"Jump 100 Index\",\n",
    "    \"Step Index 25\",\"Step Index 50\",\"Step Index 75\",\"Step Index 100\",\"EURUSD\"\n",
    "]\n",
    "\n",
    "# Map safe names to broker MT5 names if broker uses different naming.\n",
    "MANUAL_SYMBOL_MAP: Dict[str, str] = {\n",
    "    # e.g. \"Volatility_75_Index\": \"Deriv-Vol75\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29dfd2c9-f5c8-4a3b-b112-11d6b4238d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - MT5 connect / helper utilities\n",
    "def mt5_connect() -> bool:\n",
    "    try:\n",
    "        if not mt5.initialize():\n",
    "            # try again\n",
    "            ok = mt5.initialize()\n",
    "            if not ok:\n",
    "                print(\"MT5 initialize failed:\", mt5.last_error())\n",
    "                return False\n",
    "        print(\"MT5 connected, version:\", mt5.version())\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"MT5 connection error:\", e)\n",
    "        return False\n",
    "\n",
    "def mt5_shutdown():\n",
    "    try:\n",
    "        mt5.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def make_safe_name(sym: str) -> str:\n",
    "    return sym.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"_\")\n",
    "\n",
    "def get_mt5_symbol_from_safe(safe_name: str) -> str:\n",
    "    # default inverse transformation\n",
    "    return MANUAL_SYMBOL_MAP.get(safe_name, safe_name.replace(\"_\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9ca82e-f81f-4819-8375-f95f8d095b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPO model: models\\multiasset\\ppo_multiasset.zip\n",
      "Embeddings loaded: 0 embed_dim: 8\n",
      "VecNormalize loaded: models\\multiasset\\vec_normalize.pkl\n",
      "Scalers: 0 Datasets: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Load model and preprocessors\n",
    "# MODEL_FILE, VEC_FILE, EMBED_FILE, DATA_DIR should be configured in Cell 3\n",
    "\n",
    "# Load PPO model\n",
    "if not os.path.exists(MODEL_FILE):\n",
    "    raise FileNotFoundError(\"Model file missing: \" + MODEL_FILE)\n",
    "model = PPO.load(MODEL_FILE)\n",
    "print(\"Loaded PPO model:\", MODEL_FILE)\n",
    "\n",
    "# Load embeddings (handle dict or ndarray)\n",
    "embeddings: Dict[str, np.ndarray] = {}\n",
    "EMBED_DIM = 8\n",
    "if os.path.exists(EMBED_FILE):\n",
    "    try:\n",
    "        emb_raw = np.load(EMBED_FILE, allow_pickle=True)\n",
    "        if isinstance(emb_raw, np.ndarray) and emb_raw.dtype == object:\n",
    "            # possibly saved dict\n",
    "            try:\n",
    "                obj = emb_raw.item()\n",
    "                if isinstance(obj, dict):\n",
    "                    embeddings = {k: np.array(v, dtype=np.float32) for k, v in obj.items()}\n",
    "            except Exception:\n",
    "                pass\n",
    "        if len(embeddings) == 0:\n",
    "            arr = np.array(emb_raw)\n",
    "            if arr.ndim == 2:\n",
    "                # try mapping via asset_to_idx\n",
    "                if os.path.exists(ASSET_MAP_FILE):\n",
    "                    try:\n",
    "                        am = pd.read_csv(ASSET_MAP_FILE, index_col=0, header=None).iloc[:,0].to_dict()\n",
    "                        for safe, idx in am.items():\n",
    "                            idx = int(idx)\n",
    "                            if idx < arr.shape[0]:\n",
    "                                embeddings[safe] = np.array(arr[idx], dtype=np.float32)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if len(embeddings) == 0:\n",
    "                    # fallback map by CSV order\n",
    "                    csvs = sorted(glob.glob(os.path.join(DATA_DIR, \"*_normalized.csv\")))\n",
    "                    safe_list = [os.path.basename(p).replace(\"_normalized.csv\",\"\") for p in csvs]\n",
    "                    if len(safe_list) == arr.shape[0]:\n",
    "                        for i, s in enumerate(safe_list):\n",
    "                            embeddings[s] = np.array(arr[i], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load embeddings:\", e)\n",
    "if len(embeddings) > 0:\n",
    "    EMBED_DIM = next(iter(embeddings.values())).shape[0]\n",
    "print(\"Embeddings loaded:\", len(embeddings), \"embed_dim:\", EMBED_DIM)\n",
    "\n",
    "# Safe load VecNormalize using dummy env\n",
    "vecnorm = None\n",
    "if os.path.exists(VEC_FILE):\n",
    "    try:\n",
    "        class _DummyEnv(gym.Env):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(WINDOW, 5 + 1 + EMBED_DIM), dtype=np.float32)\n",
    "                self.action_space = gym.spaces.Discrete(3)\n",
    "            def reset(self, seed=None, options=None):\n",
    "                return np.zeros(self.observation_space.shape, dtype=np.float32), {}\n",
    "            def step(self, action):\n",
    "                return np.zeros(self.observation_space.shape, dtype=np.float32), 0.0, True, False, {}\n",
    "        venv = DummyVecEnv([lambda: _DummyEnv()])\n",
    "        vecnorm = VecNormalize.load(VEC_FILE, venv)\n",
    "        vecnorm.training = False\n",
    "        vecnorm.norm_reward = False\n",
    "        print(\"VecNormalize loaded:\", VEC_FILE)\n",
    "    except Exception as e:\n",
    "        print(\"VecNormalize load failed (continuing without it):\", e)\n",
    "        vecnorm = None\n",
    "else:\n",
    "    print(\"No VecNormalize file found; continuing without it.\")\n",
    "\n",
    "# Load scalers from CSV files (saved during data prep)\n",
    "def load_scalers(data_dir=DATA_DIR) -> Dict[str, Dict[str, pd.Series]]:\n",
    "    scalers = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_scaler.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_scaler.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        # store 'mean' and 'std' Series\n",
    "        scalers[safe] = {\"mean\": df[\"mean\"], \"std\": df[\"std\"]}\n",
    "    return scalers\n",
    "\n",
    "# Load prepared normalized datasets (for fallback & indexing)\n",
    "def load_prepared_datasets(data_dir=DATA_DIR, window=WINDOW) -> Dict[str, pd.DataFrame]:\n",
    "    datasets = {}\n",
    "    for p in sorted(glob.glob(os.path.join(data_dir, \"*_normalized.csv\"))):\n",
    "        safe = os.path.basename(p).replace(\"_normalized.csv\",\"\")\n",
    "        df = pd.read_csv(p, index_col=0, parse_dates=True)\n",
    "        expected = ['o_pc','h_pc','l_pc','c_pc','v_pc','Close_raw']\n",
    "        if all(c in df.columns for c in expected):\n",
    "            tmp = df[expected].dropna()\n",
    "            if len(tmp) > window:\n",
    "                datasets[safe] = tmp\n",
    "        else:\n",
    "            # try to convert if raw OHLCV present\n",
    "            if all(c in df.columns for c in ['open','high','low','close','volume']):\n",
    "                tmp = pd.DataFrame(index=df.index)\n",
    "                tmp['o_pc'] = df['open'].pct_change()\n",
    "                tmp['h_pc'] = df['high'].pct_change()\n",
    "                tmp['l_pc'] = df['low'].pct_change()\n",
    "                tmp['c_pc'] = df['close'].pct_change()\n",
    "                tmp['v_pc'] = df['volume'].pct_change()\n",
    "                tmp['Close_raw'] = df['close']\n",
    "                tmp = tmp.dropna()\n",
    "                if len(tmp) > window:\n",
    "                    datasets[safe] = tmp\n",
    "    return datasets\n",
    "\n",
    "scalers = load_scalers()\n",
    "datasets = load_prepared_datasets()\n",
    "print(\"Scalers:\", len(scalers), \"Datasets:\", len(datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5133cb-cf6c-4907-9c8a-b47350f79eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Live fetch helper\n",
    "def _fetch_single_symbol_bars(mt5_symbol: str, timeframe: int = TF_MT5, count: int = COUNT) -> Optional[pd.DataFrame]:\n",
    "    info = mt5.symbol_info(mt5_symbol)\n",
    "    if info is None:\n",
    "        return None\n",
    "    if not info.visible:\n",
    "        try:\n",
    "            mt5.symbol_select(mt5_symbol, True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    end_time = datetime.now()\n",
    "    bars = mt5.copy_rates_from(mt5_symbol, timeframe, end_time, count)\n",
    "    if bars is None or len(bars) < WINDOW + 2:\n",
    "        return None\n",
    "    df = pd.DataFrame(bars)\n",
    "    df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df = df.set_index('time')\n",
    "    if 'tick_volume' in df.columns:\n",
    "        df = df.rename(columns={'tick_volume':'volume'})\n",
    "    df = df[['open','high','low','close','volume']].copy()\n",
    "    df['Close_raw'] = df['close']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de24c9-a91a-4847-9bbc-d4267b9d154b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
